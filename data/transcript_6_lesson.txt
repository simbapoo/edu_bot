 Добрый день. Здрасте всем. Я слышал? Да. Здрасте. Я здесь, значит, задержался, мне будут проблевать смок. Давайте начнем? Давайте начнем. Сейчас я просто проверю, все, иностранал. Можно, пожалуйста, спросить у вас, все ли видно на экране? Да, видно. Давайте еще раз. Давайте представим еще раз. Меня зовут Рипарс, в целом 37 участников. Сколько примерно у вас на потоке? Есть смысл дождаться кого-то еще? Номера 65, 60 или 5. Очень мало, да? Сейчас поделим. Давайте сейчас до 40 подождем. Прошлый раз сколько было людей примерно? 40, 40, чем-то обычно. Мы открываем. Мы подошли к новому блоку, к логическому мэлу. Сегодня я отдал, я в течение буду следующего. И до этого блока, следующего блока буду вашим тренером, преподавателем за передвижение. В целом, да, спасибо, что нашли силу кучиться. Все мы работаем, все мы понимаем, что сложно. Поздравляю, уже один шаг вперед. Давайте начнем. Я быстренько начну с водного. В общем, я Рипарс. В целом, если помните, я тебе рассказывал. Если есть вопросы, то ко мне лично можете обращаться и в конце лекции, и в чате, открыть разговором. Сегодняшняя лекция будет классическим мэлем. Ну и конкретно, то мы коснемся чуть-чуть. И в целом покроем такое понятие. Ну и учение под присмотром. Рипарс, пока не на филе, я извини. Хотел такой момент уточнить. В субботу у нас во сколько будет занятие? Вы сейчас можете сказать или вы потом уже скажете? Так, я в целом знаю, вроде бы в 10 утра. Вроде бы в 10. Но кажется, до субботы мы еще как-нибудь с вами встретимся. В пятницу? Да, вроде в пятницу уже урок есть. 11 число. Мне кажется, нас в пентилейк среда суббота. Да, вы путаете, наверное, за то сами с курсом. Вот его, окей. Смотри, может быть, да. Ладно. Ну, я не знаю. В принципе, могу отписать позже или напишите, наверное, к ребятам, координатами курса, они точнее. Камал Хан, прошлый раз говорил, что, возможно, у вас там может быть в 7.30 урок или в 7 часов. Если я не ошибаюсь... Окей, хорошо, прости. Вы уже отписались, что будет в 10 утра. А, я не заметил. Тогда прошу прощения. Все, извините. Хорошо. С вашего подробия начну. В целом, смотрите, Камал Хан говорил, что у меня связался, что мы в тот раз на дата-инжиниринге блоки не до конца дошли к тему теста на нормальность и анализ корреляционной анализы. Поэтому мы быстренько бегло пробежимся по нему, перед тем, как найти кунуцию ML. Поэтому давайте начнем. В целом, смотрите, как у нас первое занятие открывающие блоки, я планировал ввести слайды на английском языке, ввиду того, что в некоторой термологии плохо переводится на русский язык. Поэтому если что, в конце урока по ощущениям, не в течение недели можете дать обратную связь, как вам удовольствовать удобно. В целом, давайте поговорим сегодня про вообще, что такое тест на нормальность, что это такое, для чего это надо и почему мы об этом-то особо кипишем. То есть я это сразу указал. Почему все круто вокруг этого гавосового расследления, нормального расследления. На самом деле здесь три вопроса я обучусь всегда. Где мы можем увидеть гавосовое расследление? Почему вообще машинное обучение, и ML в целом статистически какие-то моделирования, оно assume, то есть предполагает какой-то assumption, ставит на нему, что это такое, что это такое, что это такое. В целом, это три вопроса. Можете в общем себя спросить, почему весь мир крутится вокруг гавосового расследления. В целом, можете почитать эту книгу, и почитать, что это просто такой, что мы можем увидеть гавосовое расследление. Я думаю, что в нашем контексте машинного обучения это по некоторым причинам в следующем. Теория центрального предела, то есть центрального лимитеара. Есть такое понятие, может, в математике есть, кто рядом. Не могут принципом это это со своих уроков. Если очень жарко сказать, то сконсолидировать, это, например, дефиницию этой теории, в том, что все стремится к нам на расследение. Если мы будем собирать очень большое количество сэмплов, вероимнованных переменных, складывать массы всех львов на планете Земля за всю историю, или массу слонов. То есть, различные вещи в природе, которые мы можем оцифровать, они все стремятся к этому центрального пределу. То есть, они все стремятся по нормальному расследованию. Если вы сложите всех львов в Саванне или в Африке, по массе львов, и попытаетесь это разгруппировать, то общая динамика сплотностей всех встречаемых львов будет вырисовываться в колокол. Это такая загадка, наверное, у Вселенной почему-то так работает. Это первый очень веселый элемент. Все в нашем мире, оно tends to be normally distributed. Далее, очень важный момент, оценка параметров. Приемлемы в МЛ, мы оценку параметров, это имеется в виду, когда мы какие-то параметры высчитываем статистически через машинное обучение. Вся эта оценка параметров, она assumption под собой. Внутри гипотез МЛЕ, максимум likelihood estimation, максимальное подобоподобие. Это assumption в суть в корне, в сети всего МЛЕ. Можете почитать по него отдельно. Очень важный концепт. Он тоже под собой базируется полностью на том, что данные нормально распредлены. Я объясню подробнее, что это значит. Также, нормально распределение имеет такие математические свойства внутри себя, которые очень хорошо по-математически, они между собой бьются. Это очень странно, но в целом, там очень много фишек, трюков, при которых математика за этим голосом распределения очень лаконичная, элегантная. Далее feature scaling, скалирование признаков. Если говорить по-приромному, это computational performance and convergence. Это значит, что мы все учитываем не на руках, не на бумажке, мы еще считаем через компьютеры. Калкуляторы огромные, которые имеют дискретный взгляд на себя. У них есть какой-то лимит в памяти, у них есть какие-то лимиты в вычислениях. Они не могут все покрыть внутри своей памяти, в своих регистрах и тому подобное. Это тоже не бесконечные вещи. Поэтому большинство моделирований, не только промышленное обучение, а всякие разные сложные моделирования физических систем, проектирование телекоммуникационных сетей, проектирование различных 3D-визуализаций сложных систем, химических, физических и так подобное, они все в основу берут какое-то эффективное скалирование даты информации. Соответственно, когда данные нормально распределены, то есть скалированы в нормальном распределении, то они имеют очень лучшую performance в вычислении. То есть, словно говоря, вы увидите, что в дальнейшем алгоритмы машин обучения, это не только, они гораздо лучше учитываются, если имеют нормальное распределение для себя. А допустим, если они не нормально распределены, то они немного не так хорошо считаются компьютерами и также не так хорошо convergent словом, сходимость, это переводится на русский язык. То есть алгоритмы не сходятся. Мы еще поговорим, что такое сходимость. Так же про error modeling, то есть моделирование ошибок, кто-то ассумеет нормальное распределение для себя. То есть, допустим, кто может поработал с физикой, различными системами контроля, теории контроля или электрическими системами, схемами, то многие assumption, которые они не обучают, они не обучают. То есть, допустим, кто может поработал с физикой, различными системами контроля, теории контроля или электрическими системами, схемами, то многие assumption, положение в моделировании происходит на основе того, что мы используем какой-то шум в нашей системе для того, чтобы симметрирует реальный мир, а в основе этого шума мы используем нормальное распределение. То есть, словно говоря, мы берем гауссовый шум, идеальный гауссовый шум для того, чтобы сделать максимальное мимикрирование реального мира. Потому что, в целом, шум, на самом деле, если вы даже замерите шум в вашей квартире, имея шум не такой, как шум звука, а шум радиоактивный шум или типовой шум, то в целом, если вы его грамотно оцифровать, то он будет стремиться к нормальным распределениям. Проверьте это так. Даже на вашем телефоне типовой шум, который проявляется в чипе процессора, он также будет иметь паттерны колокола по каким-то переменам. Ну и также последний обследок, это probabilistic models. То есть, есть вероятности моделей, такие как гауссовые процессы, байсовые нейронные сети, вариационные автоинкодеры, variational autoincoders, probabilistic PCA, потом hidden Markov models, GMMA, потом GMMA, это кажется, гауссовый mixture models. Это различные probabilistic models для диффузии, может быть, вы работали диффузиями моделями ранее, для генерации датчик-жидалей, там midjourney они работают. Внутри них, под капотом, все строится на основе того, что модели и вообще скрытое состояние обучения в параметрах, они нормально распределены вначале. То есть, стартовый point для обучения моделей также идет как тум в виде нормального расстреления. Это не просто так берется, потому что у них есть много массы статистических подоблек внутри себя, которые хорошо срабатывают между собой. Вообще, это на самом деле очень сложно раскрыть в одном предложении, что это почему-то так, но в целом берите за истину, я постараюсь максимум, чтобы вас убедить в этом. Может, вопросы какие-то, я дальше иду. Да, ребята, наверное, будет лучше, если тело построим. Давайте не молчать, да? Я помогу вам, вы поможете мне это сделать, давайте будем активны максимально. Давайте не стесняться, мы здесь все обидные одной обися, поэтому давайте будем активны, если какие-то вопросы, да, нет, давайте поактивнее. Будем держать дефекцию. Пока вопросы не задавали. Пока вопросов нет. Окей, спасибо. Так, ну давайте поговорим теперь про тесты на нормальность. Вообще, их очень долго, можно погуглить. Я выбил наиболее два, которые Камахан передал, это Shapiro Will Test, также Kalmogorov Smirnov Test. Вообще, ну как я говорил ранее, это тест для того, чтобы понять данные расстрелены нормально или нет, да, для этого эти тесты нужны. Это очень интересные тесты, можете почитать в Википедии, ссылки я оставил. В целом, если как-то сжато, там вообще кроется, на самом деле, вообще эти темы, на самом деле, можно, на каждый из них, потратить на раскрытие около двух дней, трех дней, то есть точно не два, три лекции, потому что это огромное знание. Но это не тот формат по курсу, поэтому мы не будем погружаться прямо в кишки этих моделей и вообще формулы этих тестов. В целом, я так скажу, что Shapiro Will Test, очень классный тест, я его в основном часто использую, и вот KSTest тоже. Shapiro он больше, ну они оба нацелены на то, чтобы проверить две кипотезы, то есть обычно, когда мы тестирование делаем, то же самое бы тест, но здесь две кипотезы всегда. Может кто слышал, да? Это, допустим, h0 и h1, h0 это как бы гипотеза того, что данные распределены нормально, а h1 это соответственно контргипотеза того, что данные распределены ненормально. Соответственно, по завершению этого теста мы либо опровергаем и подтверждаем одну из гипотез, соответственно, только на p-value, p-значение. В целом, если говорить про Shapiro Will Test, то он, учитывая W-статистику, не знаю, такого нет определения, и вот она по сути и сравнивается с критическим значением. В целом, я покажу это в практичном занятии сейчас попозже, что это такое. В целом, там просто идет линейное сравнение, обычно, если выше, ниже, если выше, там есть, ну сейчас посмотрим, точно не помню. То же самое с формулой мы сейчас попозже посмотрим, то же самое и с про Колмогоров смирных тестов. По сути, это вообще шикарный тест для того, чтобы почти все компании, которые работают с обетестированием, с какими-то гипотезами, проверками, используют этот Колмогоров смирных тест. В любом случае, там, где я работал, это все используется. То есть, это позволяет нам, этот тест позволяет нам не только понять, да, на распределении нормальный или нет, а также понять, насколько та или иная выборка групп статистически отличается друг от друга. Если говорить про Колмогоров, то там есть, по сути, такое простейшее понятие, как подсчет высчитывания максимальной дистанции от empirical density function от ее CDF. То есть, если вы слышали, есть CDF, есть PDF, есть EDF, да, функции. Известяне такое? Да, статистически. Ну, по сути, это вот про эту историю идет. В самом деле, не знаю, на сколько мы поговорится будем в этом. Я думаю, возьмите это на домашнее задание, почитайте подробнее. Ну, в практике посмотрим сегодня. Вот. В целом, давайте дальше. Сейчас вернемся еще. А также попросил Камалхана объяснить корреляционный анализ, что это такое. В целом, по сути, это тоже огромная тема, ее на самом деле нельзя так просто в одном слайде закрыть. Но в виду того, что курс сжатый, мы будем делать. В целом, что это значит? Этот анализ выявляет следующее понятие, как выяснение на скоп. Какая корреляция имеет взаимосвязь между собой? То есть, насколько они сильны между собой и никакого их направления? То есть, корреляция бывает от трех типов, ну, условно. Она ранджирована от минус одного до одного. И в целом, если она негативная, то она значит негативная корреляция. Если одна зависимая переменная угаличивается в манитуде, то сравнимая с ней переменная должна уменьшаться. У них негативная корреляция. Если корреляция равна нулю, то это отсутствие корреляции. То есть, они вообще друг другом никак не корреляются. То есть, изменения в одной перемене, в одном признаке никак не отвлекаются ни в каком из изменений, ни в другом. Ещё третья, это когда позитивная, то есть, когда у нас она больше нуля, значит, если растёт первая переменная, то вторая тоже растёт вместе с ней. В целом, справа форма, ну да, самым популярным в линейном анализе опять это piercing correlation coefficient. Вот, по сути, справа формула его же. Если посмотреть хитрым образом, то здесь видно, что это по сути отношение expected value, мат ожиданий, перемена X, перемена Y. Ну, если covariance матрица верить на по сути стандарт отгонения двух samples. Ну, это по сути формула, она раскрытая просто формулой. А проспор это по сути среднее значение, эти капуляции X. То же самое снизу. И по сути, вот такое, если не разобраться, ну, тут видно, что это, на самом деле, просто очень простейшее математическое перемножение. Здесь никаких таких сложных формул нет. И по сути, этот коэффициент помогает нам понять, насколько та или иная переменная с другой переменной корридируется. Вот тоже обосолив практике. Суфа, вопрос какие-то есть? Я хотел перейти у Лена. Пока все понятно. Попрос, что про то делится тесты на нормализацию. Еще раз. Вы их обещать не будете? Хотел узнать про эти тесты, которые два. Нам, получается, им надо самим дочитать вот эту информацию про них. Вообще, Дак Камахан мне объяснил. Он предполагал, что я должен вам показать этот... Как его? Ну забрень какие-нибудь. Показать именно просто практический участник. Порта? Сорри, там кого-то слышно, я его заглушу. Так. Да, кажется, вы слышите. Так, сейчас... Давайте я сейчас зашарю первый мой с код. Скажите, пожалуйста, видите вы с код мой текущий? Да. В целом, я поделил такой ноутбук, я его попозже скину. Реподиторий наш общий. Посмотрим, что случится. Окей. В целом, да, я повторю еще раз. По сути, два теста возьмите как домашку на себя. Там просто посмотреть формулы две, это не так сложно. Посмотреть, как считается статистика там. На самом деле, ничего сложного. Вот это как домашнее будет. Я хотел посмотреть, что я говорю, теперь на демонстрации сам. В целом, так, виды хорошо, прилежать не надо. Так лучше, да, наверное? Так получше. Да, так получше. Хорошо, да, там наш принтер. В целом, все просто. Хотел бы еще спросить, ввиду того, что это первая PML, насколько у нас есть в группе студентов понимание настройки среди виртуалей? Все умеют это делать? Я умею. Окей. В целом, да, стоит отметить, что вам придется очень часто кодировать. Поэтому, да, сейчас. Еще раз повторить, я, кажется, прослужил. Виртуальную среду настраивать? Ага, то есть, нужно будет подготовить среду, потому что, практически, знаете, очень много будет в этом курсе. Поэтому, чтобы питон работал все отлажно, библиотеки все работали правильно. Это как бы мы осимим, что вы это умеете делать. Поэтому, я, наверное, в рамках курса не буду показывать, как питон настроить, создать это все от вас. Поэтому, смотрите, у меня есть среда виртуальная, есть питон и питер ноутбук, есть питон 3.12. В этом курсе вам потребуется просто numpy, scipy, matplotlib и скален. Это не факт. Вот, давайте посмотрим. Здесь просто импортирую библиотеки. Далее я создаю вот такой хитрый метод, функцию для того, чтобы сделать, по сути, тест. Что, по сути, вся хитрость делается не мной, а этим модулем от scipy. То есть, если у них пакет stats, вот, по сути, если зайдем сюда, то... О, я понял, что вы волнуете. Окей, у нас... А здесь можно спросить еще раз? Этот тест сьемит на H0, что есть нормальное распределение и если мы reject, то мы не будем делать это. Получается, еще пора есть еще. Да, я как раз ответ на вопрос сейчас буду следовать после этого объяснения. По сути, наш код, я просто импортирую с модуля scipy stats, под модуль, который есть уже шопированный метод. И вот, мы уже видим, что у нас есть вот такой код, который, по сути, у нас есть вот такой код, который, по сути, у нас есть вот такой код, под модуль, который есть уже шопированный метод. Не знаю, почему он сейчас не отображается, просто было еще одно из помогающих. Там и детали методов, как говорили. Вот, ну... Да ладно, ладно, видимо, похоже, окей, не хочет он сейчас работать. А в целом, я провайду ему дату, далее он возвращает мне статистик, которую я говорил в лекции только что про doublex-статистик, и также возвращает мне pvalid. Я сейчас верну, объясню на что по ровне. И далее, да, вот здесь просто принты выводятся, и идет момент вот того, что мы и говорили. Если pvalid этого теста, он будет больше, чем наша альфа, альфа, это, по сути, берется как приз набрать его 0.05 для очутительности вообще всех академических тестов на нормальность. То есть, любое тестирование, там, любое тестирование, любое статистическое тестирование, любое именно датасетов, когда тестируем популяции, мы берем в основном 0.05. Ну, я, наверное, не отвечу, почему именно 0.05, это общая призная практика, ввиду того, что она, в большинстве случаев, она работает хорошо, как это число в значении. Протеснно, если pvalid этого теста больше, чем 0.05, наш порог, то мы говорим, что это Gauss's probability, или мы подтверждаем нулевой гипотезу, то есть h0, гипотез 0, то есть fail to reject. То есть мы не можем дать ее ответственности. Если наоборот, то получается, мы отбрасываем нулевой гипотезу, то есть подтверждаем первую гипотезу h1, и говорим, что sample и Gauss's probability. Здесь просто плотим и... На самом деле, я что-то не понял. Типа это же нелогично? А h0, брать что распределение нормальное, оно же вообще, ну практически там, ну типа, потенциально может быть не очень нормальным, да, то есть не прям хорошо таким bell curve таким быть, а каким-нибудь просто между юниформой bell curve. То есть вот так вот тесты сфюмить все равно, что типа, по дефолту у нас h0 это распределение нормальное, а еще раз вот h1 это ненормальное. Да. Ну не знаю, ну просто нулевая гипотеза спущает в этом тесте. Я вопроса не понял сейчас вообще, если я понял вчера. Я не понимаю, я не понимаю, почему в этом тесте, ну чисто логику, типа нулевая гипотеза, она сфюмит, что все по дефолту нормальное. Типа я думал, мы будем доказывать через тест, что что-то нормальное, оно, а мы пытаемся reject, ну что оно нормальное. Правильно же, понимаю логику? Если я правильно вас понял, то как будто бы да, как бы что мы делаем, мы по сути выстраиваем две гипотезы. Нулевая гипотеза это то, что наш сэмпл данных, он нормальный. Да. Вторая гипотеза это то, что он ненормальный. Ненормальный, да, и я говорю, что странно, что нулевая гипотеза, она типа заверяет, что все по дефолту нормальное, а нам нужно доказать, что ненормальное. Ну то есть сложно же доказать, по сути, особенно с пяти порционной альфой. То есть она чаще будет говорить, то... Непозможно я отвечу на этот вопрос. О, давай. Смотрите, получается, оно традиционно так вышло, потому что изначально это было схоже с судебной системой. Условно, есть присутствие невиновности о том, что человек невиновен. И вам нужно доказать, что он виновен. И, собственно, оно отсюда и пошло. То есть вы первые... То, что вам нужно доказать, вы ставите ваш один. То, что вы не можете, как бы сказать, то, что... То, что вам нужно отвергнуть, оно стоит ваш ноль. Это просто условность такая, и всегда так было. Нет, я понимаю, просто... Я думал, что в данном случае в презумпции невиновность как раз будет о том, что у нас данные распределены ненормально, а нам нужно доказать, что нормально. Но если наоборот, просто я удивился, что так происходит. И решил переспросить просто, нет ли ошибки. А так все окей. В целом, я понял, да, я не сильно до конца понял, что именно смущает. Но окей, я постараюсь сейчас в учении курса, в учении этого урока еще подумать на фоне. Вернуться к тому вопросу. В целом, как Агжан сказал, что в целом, в самом деле... Это стандартный поход того, как мы делаем это. То есть, если посмотреть любой другой тест, когда там, сочистский тест, T-тест, student test, они все работают в схожем режиме. Ну, как бы, от обратного дойдут. Окей. В целом, я хотел бы еще раз зацепить внимание здесь, здесь, на этом участке кода. В целом, ну окей, random seed мы нитеризировали, чтобы данные были детерминированы, условно говоря, пол-рандом. Далее, создаю два распределения, да, причем, ну, одно нормального типа, другого ненормального типа. Ну, экспоненциально, да, в общем. Вот, ну, с размером сюда выборки там, тысячу, тысячу, или это, цепот. Ну, и по сути, заплот. И далее, загоняю вот мою функцию навоспеченную, загоняю первый тип данных, то есть нормальный данных. Потому что, ну, я вижу, что я их выдавал через все это random от NumPy, через нормальную функцию. И, ну, если NumPy правильно работает, то она даже создает нормальный датасет. Вот. И по сути, если мы запускаем его, мы видим, да, вот, на сути такой, как бы, такой, да, в общем, мы видим, да, вот, по сути такой, такой неплохой вывод интересный. Первый вывод это на пидару нормандерской линии, второй вывод это на ненормандерской линии, на экспоненциальной. И видим, что вот, аж первого тест, результат, вот статистик, да, был статистик, который я только что показывал, он выводит следующим образом. Вот, если почитать, там, формула была очень такая интересная, пятрики, да, это домашнее здание, возьмите. В общем, этот метод за нас все посчитал, выполнил какое число. Вот, также он еще проводит с собой пивали. Если мы говорим, да, если пивали, ну, мы, по-моему, писали здесь, если пивали, по той же причине, больше, чем наш порог, то есть альфа, то мы говорим, что это нормандерская линия. Вот. Таким образом, мы также тестируем и экспоненциальное расходление, да, то есть высчитываем, и видим, что в данном случае пивали вообще очень маленькие, то есть 2.5, здесь мы 3-й, тем самым, получается, мы видим, что, ну, мы подтверждаем, что гипотезу любой может отвернуть, и это совершенно ненормально, что не так. Вот, также помимо этого, да, помимо просто сухих чисел, еще есть такое понятие, как QQ-плот, ну, я не знаю, как его на русском озвучивать, но в целом это плот, когда мы раскидываем наше значение по квантилиям, да. Квантилия это, ну, все знают, что это квантилия. Это условно такие, ну, подгруппы, сгруппированные участки наших данных, ну, там бывают квартири, да, это когда 4, да, у нас квартальные квантили. Есть децили, да, по 10, ну, и так далее. В данном случае вот у нас теоретически квантили, пока не спрашивается, как он считается, здесь есть такая хитроумная функция, вот здесь, вот, да, prop-plot, вот она, по сути, считает. Что она делает? Давайте фокусируемся на верхней границе этого графика, первый, второй, это вот график, это, по сути, вот, про нормальное, а нижние два это про экспедиенциальное распределение. И мы видим, да, что по сути каждая сильная точка это наш sample death данных. И по сути мы видим, что по выводу вот этого qqplot-а он раскидал по квантилям все наши значения, то есть по fcy это наше значение наших samples в антитуде. И по сути, если мы, ну, здесь выстраивается прям ровная линия, вот этот тренд, вот эта красная линия, насколько я с ней ошибаюсь, этот qqplot он выстраивает в зависимости от по сути best fit line, как бы, в контексте вот этих квантилей. И мы видим, что отгонение, да, от этой красной листи, ну, почти минимальное, да, то есть она, по сути, полностью все точки обокомплектованных квантилей, да, этого плота, они все почти строго идут в разрез этой best fit line. Вот. Что еще из этого можно увидеть, да, помимо этого? На самом деле, если вы, если как бы похитрее сделать, да, допустим, давайте, а, ну, окей, это позже. Также, чтобы это посмотреть, да, давайте просто в хитрическом виде, более традиционном виде, рассмотрим это в Instagram на виде, да, то есть когда мы построим те же самые точки, ну, уже по количеству их occurrence, да, по количеству их, ну, совпадения, вот, встречаемости. И мы видим, да, что, ввиду того, что у нас, ну, данные были распредлены на, ну, на среднем в нуле, то есть мы видим, что в целом Instagram она повторяет почти идеально колокол, то есть она имеет среди на нуле, ну, и какое-то, да, с 20-х с половиней влево-вправо. Вот. Это значит то, что у нас концентрация всех точек, которые мы сгенерировали в этой функции, да, random norm, они больше всего концентрируются в нуле. И вот, если мы, допустим, ну, если как-то раскрасить поинтереснее вот эти точки, да, в этом графике qqplot, вот где-то в этом регионе, да, где у нас 0 здесь, здесь будет гораздо больше точек, просто их не совсем много, их всем хорошо видно, но они как бы будут концентрированы больше здесь находиться, в этом регионе, чем здесь или тем, где здесь. Вот. Также я хотел бы еще, знаете, попробовать, смотрите, вот вы все обратили внимание, да, что 0,9986, да, на вот эту статистику, да, 0,9986. Если мы, скажем, увеличим, да, еще данные на большем виде, то есть, да, опять-таки, как я и ранее говорил, то есть мы сейчас протестируем централимитёрию, да, то есть чем больше данных, тем больше будет такой яркий, яркий выраженный паттерн нормального расположения, да. Давайте увеличим в 10 раз, смотрим, да, что тут стоит. Ну, видим, да, что вот эта гистограмма, она стала еще более, ну, выглаженнее, да, стала. Ну, окей, я могу еще сам где-нибудь бинцы подкрутить чуть-чуть, чтобы было чуть-чуть, ну, гладче, да, не 30, а 50. Ну, допустим, если мы дальше пойдем, там, увеличим еще, то мы видим, да, что наш вот этот статистик, он стремится к однероге. Соответственно, пивали тоже растет, то есть мы показываем, что чем больше в нашей выборке, да, имеется данных, да, которые нормально распредлены, то есть мы загоняем больше, условно говоря, уверенности, да, в это распределение, то есть у нее имеется больше, как сказать, сэмплов, то мы, соответственно, стремимся к бесконечности, и соответственно мы, ну, сходим и, знаешь, видит, видит, что если я буду дальше продолжать такой паттерн, у меня статистика будет стремиться к однероге, и пивали тоже как-то стремится. Ну, соответственно, и гистограмма тоже, да, она становится еще гладче, потому что, ну, мы полностью подтверждаем то, что мы делаем. Вот, то тем же самым я хотел бы перейти к экспоненциальному распределению, вот, по сути, здесь тоже, здесь видно, да, прям обратная совершенная реакция того, что все точки, они, если они разделены под пантерем, да, то мы видим, что опять-таки, ну, не знаю, как вам видно, но у меня хорошо видно, что я проявлю, что здесь вот этот цвет на нуле значений, ну, здесь как будто плотность играет очень больше, ну, и того, что распределение самоэкспоненциально, но такое природу считано, что на нуле, да, вот здесь количество этих точек, да, встречаем их в этом датасеть гораздо больше будет количеств. Ну, и мы видим, да, что здесь только тысячу, тысячу семьсот точек, почти вагитуды нуля. Вот, и так далее она, ну, затухает, да, ватка затухает. Вот, соответственно, если мы также сделаем то же самое и здесь, величим точки, то есть на значения будут еще больше стремиться к этим значениям, и тем самым, ну, если посмотреть сюда, то пивали датей становится еще меньше. Вот, то есть она становится гораздо меньше, чем было раньше, да, при перемене количества сэмплов, и тем самым она гораздо меньше становится, чем 0.05 наш порог для статистической значимости. Ну, и тем самым, да, мы можем посудить, что мы можем рэжекнуть, да, тот или иной, то или иной гипотез. В целом, ну, вот это по сути тест, Shapiro Milk Test. Он очень хорошо реализован через IP, можете тоже поиграться после этой лиц. В целом, давайте перейдем дальше к Могорову Смирнову. В целом, здесь то же самое, я попытался сделать то же самое, что и выше, то есть, похожим образом сгенерироваться два типа распределения, один нормальный, один другой экспедиционный, с таким же почти иперпараметрами. Ну, и в целом такая же логика, да, если у нас пивали после вывода к STS от Modular SIPI больше, чем наш альфа, то есть 0.05, опять-таки мы подтверждаем, что это гауссовое распределение, если нет, то это не гауссово. Сейчас. Так, ну, давайте допустим. Опять-таки, да, сверху разберем, разберем нормальное распределение, истина нормальное распределение, и потом экспедиционное. Как мы видим, здесь я выдел гистограмм данных почти всем же самым образом, но еще, помимо этого, для более прониктательности, я добавил еще и прерывную функцию, лотку функцию, PDF гауссовое. Видимо, она подстреляет ее, и в целом здесь тоже самое, что и также аналоги мы сделали, там построили QQPlot, мы тоже видим, что в целом третий сохраняется. Это для визуализации, а вот, что в принципе вывел сам KS-тест, он выплыл так же статистику свою и выплыл так же P-value. Видим, что P-value тоже достаточно большой, если мы также раскинем количество данных, укрепим confidence в тестах, видим, что он тоже стремится гораздо больше значения. То есть, мы фейлим QRezhack H0. Ну и действительно, чем больше итерация сэмпла, мы видим, тема гораздо лучше становится гистограмма, но видим, что она прям подействовала. Ну это прям круто. То же самое, давайте посмотрим и про экспоненциальные распределения, и в сторону видим QQPlot, также видим очень скошенные влево распределения. И в целом мы видим, что KS-тест также предлагает очень маленькое P-value, тем самым Rejective H0. Rejective H0 выплывется, ввиду того, что она просто меньше 0.5. Вот. В целом, это все от протеста, мы их очень быстро пробежали, прошли, поэтому рекомендую вам почитать про них детально, какая математика за ними сидит. В целом предоставлю этот ноутбук, посмотрите дальше, что это такое. А можно пока мы от этого децета экспоненциально уйдем? Чуть-чуть вперед, если забежать. Ага. У нас, ну типа мы хотим построить модель, и у нас есть VH, которая явно экспоненциальное распределение. Вот. И как ее, то есть, как раз будет модель якобы хуже перформить, с учет того, что мы осимим, что все линер, ой, что все нормал. Вот. И как с этим справляться? Вопрос. Вообще, в самом деле, хороший ответ на этот вопрос, вот, в следующий разом, it depends, да? Все зависит от кейса. На самом деле, никогда оно вообще, как сказать, не бывает какого-то единой панацией на все текущие проблемы, которые вы видите. Все нужно всегда, то есть, к тому, что все зависит от того, с чем мы работаем. Ну, в целом, если есть какие-то бэйс-практики, которые хотя бы минимизируют этот эффект экспоненциальный, то в целом есть такое понятие, как log-normal transformation. Слышали, не слышали, да? То есть, когда мы преобразовываем наши данные через логарифом, тем самым мы нивелируем этот сильный long tail, да, форматом. То есть, когда у нас условно DRA. Давай, окей, сейчас. Не, я понял. Давай, я вот просто нивели, да? Когда у нас какой-то есть long tail, да, вот такой, когда вот это маневрение, они там стремятся не до десяти, а там до тысячи, да? Тем самым у нас какой-то нереально кривой сдвиг в сторону влево, тем самым можем отдать вот эти хорошие, которые я знаю, это log-normировать. То есть, применяют логарифом, а естественно, натурально либо по двойкой, либо по десяткой. В принципе, это нравится. Он хорошо сквашивает, то есть, сожмет паттерн функции тем самым, что она будет более-менее похожа на нормальную. Ну, опять-таки, все case-by-case. Ну, general-practice такая. Ну, просто какими-то математические преобразованиями пытаться не нивелировать этот эффект. Я бы с этого начал. Да, понял. Ага, спасибо. Окей. Вот. Окей. Давайте быстренько по корридорационному анализу пойдем. Да, вот, в принципе, я понимаю, все это пошло, офигеть. Мы даже не начали нашу лекцию. Окей. В целом, да, как я говорил ранее, корридорационный анализ. Здесь я сидел, подумал, почитал, где-то укорал. Я построил, короче, здесь две перемены. Количество сна и продуктивность. Где количество сна? Это от... Ну, количество сна это получается 7, среднее. С отменением 1, и таки 100 сэклов. Ну, это сухо выспите до среди. И есть продуктивность. Мера, она такая линейная функция, нехитрая. Тоже есть какой-то рандом небольшой. И тарак это линейная комбинация с каким-то коэффициентом. То есть функция заходит сюда и выходит. Такая линейная зависимость, очевидно, здесь есть. Но это мы не зная, это мы подготовили сами. Давайте предположим, что мы это не знаем. Дальше датафрейм подгружаю, как просто засунуть туда, чтобы для удобства. И вот по сути, здесь тоже от stats, от scipy. Есть хороший такой метод, Pearsonart. То есть качественная корридация Pearsonart. Загоняем туда наши часы сна и продуктивность. И он вот этого щита, вот эту страшную форуму, которую я показывал здесь. Ну, неважно, на действии показывал я. И возвращает тоже сам коэффициент еще к Иваню. Давайте запустим, чтобы ты пока примет, что это такое вообще. Ну, в общем, партия. А здесь я выбил, да, то есть не из вещам. Я выбил коэффициент корридации Pearson, ипи значение, которое выбил мне этот от stats, scipy. Вот, по значению. Они мне внизу. Соответственно, мы уже не знаем, что это такое. Но мы уже знаем, что это такое. И вот здесь мы уже знаем, что это такое. Вот, по значению. Они мне внизу. Соответственно, он выбил 0,45. То есть помните, да, он от минус одного до одного рассчитан. То есть, соответственно, у нас почти 0,5 значения. Значит, у нас имеется позитивная корридация, да, в данном случае. Ну, мы это понимаем. И мы видим, да, что на графике, которая самая-то проясняется. То есть мы понимаем, да, ну, это по сути наши точки. Наши числа на x, на оси y это наша корридительность. По той функции, этой линейной, которую я преобразовал. И мы видим, да, какой-то тренд. Ну, условно говоря, вправо, вверх чуть-чуть, да, каким-то слопом. Вот, ну, красная линия, по сути, это отображение тоже такого тренда, среднего, да, то есть, ну, baseline этих данных. И мы видим, да, что у них есть зависимость линейная. То есть при росте по x, также растет, ну, конечно, не в одинаковом порядке, растет оси y также. Ну, а значит медленно растет, да, мы видим, что слоп не такой сильный. То есть наклон здесь 0,5, это даже не один. То есть он даже медленно растет, чем y равна x, да. Вот, то есть условно говоря, если будет очень много часов сна, то у нас также пропорциональная линия на небудрость. Ну, если продолжим, да, вправо дальше, если будет график, точек, то там будет какая-то сагнация. Вот, интересно, мы видим, да, что если это корреляция, взаимоотношение между двумя переменами, да, ну, условно говоря, в данном случае можно сказать, что это две фичи, мы видим какую-то корреляцию. Вот, соответственно, здесь тоже есть такое очень интересное место, это heatmap, да, кажется, от cborn. Да, вот, cns, cborn, heatmap. Он на вход, по сути, получает... Ну, я здесь еще считал, корреляционную матрицу высчитывал, и он по сути грамотно очень так вырисовывает мне карты с каждым, да, то есть число сна на себя, число сна на правительность. Ну, соответственно, если на самом себе это корреляция полная, потому что, ну, это логично, на самом себе ты растешь так, как сам растешь. Вот, и мы видим, что также надо правительность, число сна на 45, и так далее. Ну, очень как бы интересный график, удобный, чтобы понимать, насколько глубина по типовой карте видно, да, силу той корреляции. Вот, ну, здесь средняя просто, там, по роке игрался, сказал, если 0,3 слабая, то есть имеется средняя, если выше, ну, это сильная. Да, ну, опять-таки, да, здесь тоже певали использовались, то есть певали говорит нам, как бы, насколько эта корреляция фактически значима, да. То есть бывает такое, что корреляция есть, позитивная, да, условно говоря, она есть, она растет, х растет, в бане одновременно, но она может быть выцена шумом или случайностью. Соответственно, как бы, певали нам помогает понять, что это не какой-то шум, а это какая-то реальная, действительно, очутительность, которая срезаемая, и которая позволяет нам сказать, что это не было вызвано шумом или просто рано-домки. Вот, ну, так это, в принципе, делается. Очень простой пример. Еще раз, да? Проще. Я тут по-туркийски написал, вот, часы обучения, да, результаты тестирования. То есть мы понимаем, да, что если посчитать, ну, ну, мы сами понимаем, что здесь закономерность понятна. Чем больше учишься, тем лучше тесты результаты. Поэтому мы видим, да, что при запросе, то да, певали у нас очень маленькие, а корреляция вообще, ну, почти один, да, то есть полная корреляция присутствует. И сила, да, эта корреляция, она очень сильная. Ну, в целом, это было больше показать, как это в Python можно реализовать легко. Вот, вообще, рекомендую, да, потом почитать все эти кейс-тесты, да, как они в SciPy реализованы. Очень интересно, ну, сделано внутри. Окей. Так, ну, давайте тогда перейдем к нашему основу уроку. Почему мы собрались? Уже час прошел. Так, вы все здесь, да? Я буду пропонировать. Да-да. Да, все тут. Да, все тут. Окей. Спасибо. В целом, давайте поговорим про MSAMML. Я вывел такие маянс тонны, да, пехи, пехи, там, не знаю, пеха, так как же переводится. Вообще, таких, так я вижу, да, на мой взгляд, ситуации, которые у MSML произошли за эти годы. В целом, это, в 50-е годы это, да, вышли там очень мощные ученые математики. Это Turing, Alan Turing. Он предложил, предложил, предложил свой тест Turing, да, чтобы почитать Turing тест. Очень интересная штука. Вот, туда-туда, то есть, тогда пошли задатки, да, что такое вообще, а что такое, что такое? Машина, может ли она думать. А далее, в честь седьмом году, если не ошибаюсь, там, у Казымблак был, чувак, да, он, кажется, был вообще не ревиолог, но там человек медицины даже. А, простите, вы презентацию показываете? Да. То, что у вас открыто. Сорри, сорри, блин. Да, хочу сказать. Я этот. Противо показывал, да, стоп. Хорошо. Надеюсь, видно сейчас? Да, видно. Да, 57-й год это Rosenblatt. Это такой, то ли американский ученый. Он предложил первый дизайн Perceptron, вы считаете, что как Perceptron. Perceptron, по сути, это первая нейронная сеть. Вот MLP, multi-layer Perceptron, это про этот тему. То есть, по сути, мы понимаем, что тему мы делаем, мы делаем, мы делаем, мы делаем, мы делаем, мы делаем. MLP, multi-layer Perceptron, это про этот тему. То есть, по сути, мы понимаем, что самая теоретический материал, фундамент, к тому, с чем мы сейчас работаем, он в теории, когда у Росущества просто не было технической реализации и подхода вообще, как это что делать. Вот. Также 60-е, 80-е годы была такая зима, да, и зима, когда началась тагнация в процессах, там shortage of funding, ну и прогресс немного упал на нее. Вот. Потом было обратно такое второе дыхание, 80-е, 90-е годы, когда вот уже такие практические вещи, о которых мы начали говорить, начали появляться. Это SVM, да, Support Factor Machine, Metaphoric Vector, где это ученые, да, из Советского Союза участвовал, Вапник, Владимир, кажется. В общем, все шарливается. Ну, живой все еще. Это, по сути, человек, который SVM придумал в одном, это считается фундаментальным алгоритмом. Ну, SVM, по моему мнению, это, ну, он был как бы киллером всех алгоритмов до революционированных сетей. Вот. На SVM держалось все раньше. Вот. Это прям нереально сильная штука. Тоже ранее, если оцепать ранние Face Recognition модели до нерестивых решений, они были на SVM лучше в своем виде. Вот. Тревесное решение. И другие статистические модели, там, Regression всякие. Случайные лица, гранитные бусинги, это в то время пошли. То есть хайп тогда уже начинался соединить, потому что люди начали понимать, что в данных строится какой-то бизнес-вали. Ну, не только бизнес-вали. Вот. Потом 2000-е годы, да, там интернет коридирует, да, здесь интернет появляется, компьютеры появляются, массово проникают в различные компании, большие, малые, средние. Валяется такой big data boom, да. Когда люди начали отцифровывать свои данные, начало понять, вот, вот, вот, строили храдки, да, появляться, когда очень много данных стало отцифровываться в цифровых соцсетях. Вот. Также пошел 10-й год, да, наверное, ну, тоже более-менее что-то я там, вы застали. Осознанно имеется в виду про революцию в глубинном обучении. И, да, про это все, да, про это все, да. И, да, такой же пропорциональный рост в hardware, да, и в FitchTech, то есть когда GPU, да, начали появляться массово, прям эффективные GPU. Люди начали понимать, что на GPU очень можно крутые вещи создавать. Вот. То есть еще начал boom появляться, там же и AlexNet появляться начало, да, в AVG разные сети, которые начали просто ушатывать вообще соревнования по капитальным зрениям, да. Ну, 90-й год это уже в наше время более, это уже генеративный AI, это век трансформеров, дефузионных моделей, ну, и все, что сейчас на hype ходит, в QGPT, Cloud и всякие, это да и т.п. вещи. Это уже такая революция нейроцитей, но уже под видах генеративный AI на основе архитектуры трансформера в большинством виде. Вот тут ссылки, вы читайте, на что я ссылаюсь. Ну, я ожидаю, что еще что-то будет, потому что это 24-й год на дворе, поэтому мне кажется, more yet to come. Окей. Вот, теперь давайте посмотрим, что такое вообще диффиниция машинного обучения, да. Я просто задал этот вопрос, Cloud 3.5 Sonnet, а там Robic. Что-то вот такой нехитрый умный диффиниция меня, ну, выкатила. Вот, можете почитать. И по сути, на самом деле, это у меня очень порад, я аж выделил, потому что машинное обучение, это то, когда мы говорим о машине, сделать это без каких-то явных инструкций. То есть, without being explicitly programmed. То есть, без каких-то моих таких жестких правил, бизнес-правил, захаркойных правил, мы пытаемся, чтобы она без них поняла сущные задачи. И по сути, это есть про машинное обучение. Мы пытаемся выявить какие-то скрытые паттерны, какие-то законобедности, поведение, какие-то вообще абстрактные функции, которые вообще в голове не могут усесться правильно в нашем понимании, которые выполняются и работают, ну и работают. По сути, это по сути, это про это. Вот, весь обалден это, опроксимация сложных функций. И все. Х на вход, Y на выход, X на вход, Y на выход и так далее. А в целом, по классике я разделил, наверное, Supervise, Rainform, Learning. То есть, кто слышал про Supervise, Learning? Слышали, да, уже, наверное? Да-да. Да. Окей, ну понимание, это когда у нас есть какая-то выборка от учителя, от людей, когда мы явно говорим это да, это нет, это да, это нет, это кошка, это собака, это там человек-мечтатель. Вот, и она пытается заканчиваться куда-то быстрее, между собой. Вот, также бывает задача посложнее, ну их посложнее, а более комплекснее. Это когда без учителя, Supervise, Learning. Это когда у нас нету явного, ну не то чтобы это не мы делаем, а скорее потому что сложно сделать, а мы не можем какие-то данные, словно, разметить руками, и получать возможно, поэтому нужно находить какие-то структуры в данных, или паттерны, да? И тем самым можем условно, условно какие-то обустроения там. То есть, это я считаю вообще очень крутым вещью. Вот, ну и RL, это уже более advanced topic, уже про роботов, про Gen.AI, про HCI, про ген. Когда у нас нету такой явной, легкой, простой функции как Supervise, Learning или Unsupervised, когда у нас есть более сортная структура системы, и существует агент, среда, на картинке собачка это агент, среда это человек и палочка. А есть какой-то агент, в этой среде существует какая-то стеклянная, ну, как бы, стеклянная, Роботы ходят по плоскости, люди не прописывают функцию как ходить, робот сам пытается найти эту функцию, сложную многопараметрическую функцию того, как ходить, в его условиях в текущих. Среда это кроящий мир, actions это его роторы, механизмы вращения, осей, колес там, и rewards это, скорее всего, если он падает, не падает, это ну, или что-то такое. Ну, это advanced-вещи. Вот. Хотел бы также затронуть быстренько про тему Backrisp, Backrisp, это такое понимание, как cross-industry standard process of data mining. То есть, как вообще проходит процесс в идеальном мире, это когда из понимания бизнеса, далее получается понимание данных, вы должны понять данные из бизнеса, обработать их, подготовить, моделировать, подготовить, моделировать их в таком цикле. Далее вы должны обученный модель после обучения сделать эволюцию этой модели, предквалибировать, проверить насколько она реально работает, правильно или неправильно. Ну, если случится чего, decloak. То есть, выбор напротив, ну и так далее. Если нет, то бизнеса, там бизнеса потом будет сказать, давай допусти это и тогда это. Это вообще такая топология, ну, я просто думаю, это варьер-закляр. OK, давайте поговорим про супермастер, не подробнее. Есть классификация, есть эгрессия, да, слышали, да, где задачу основать. В целом, если про классификацию, это такая задача. Все понимают, что классификация, эгрессия или эскипать. Или остаться. Да, да. Да, да, понимаем. Ага, я не понимаю, может быть. А может быть, лучше остаться? Нет, давайте по-потронему объясните. Что я сказал? Хорошо. OK, спасибо. Теперь я понимаю, что надо как двигаться. Вот, смотрите, есть два типа задачи, да, классификация, эгрессия. Классификация – это у нас, когда есть набор данных, да, дискретный набор данных. То есть у них есть какое-то понятие категорий или лейблов, либо классов внутри себя. То есть, словно говоря, мужчина, женщина – это два класса, это два лейбла, это две категории людей. Вот. Как бы сказать, такие, знаешь, конечные результаты, они дискретные. Вот. А есть такие задачи, да, где нужно работать с такими более непрерывными значениями. Как, допустим, предсказание цены на квартиру в остании, да, не знаю, в Аллате. Когда у нас вор значения будет от 100 тысяч в тинге до 300 тысяч в тинге. А бывает и 135 тысяч точка 200 тинге, 135 точка 100 тинге и т.д. То есть значения очень непрерывные, да, они не конкретные, но как класс, их нельзя выделить, там, мужчина или женщина, например. Ну, на примере справа картинка, видите, у нас есть квалификация, да, есть конкретно какие-то объекты ярко-зеленого цвета точки и менее яркого. И мы пытаемся их разграничить между собой. Ну, она, конечно, немного кривая изображена, но в целом как бы разделить два класса. Типа эта сторона класса зеленого, а эта сторона менее зеленого, ярко-зеленого. Какой-то разделяющей линии, не обязательно линии, а, ну, не прямой линии, может быть, кривой линии. Вот. Справа регрессия, да. Нам в целом не нужно понимать, как между собой соотносятся между с каждой точкой, нам нужно просто предсказать следующие точки, их значения следующих точек. Впрочем, видите, мы нарисовали черную линию, да, справа вот здесь вот, вот тренд. Наилучший, кто-то предсказывает, да, общее поведение динамики этих точек. Ответственно, если мы прокладываем вправо, да, дальше значения, мы можем опираться на эту регрессиологическую линию, делать этот прогноз, какие точки еще могут быть. Вот. Ну, на примере, смотрите, если говорить конкретней, то пример модификации это леечистская регрессия, древо решение, ну, не факт, не всегда они эти регистрии тоже решаются, random forest, да, sp. Вот. Ну, они, чтобы ли там, там сплотиться. А дирегрессия это критерия регрессия, паридомиер решен, да, там, ну, нет, ревервешен, и т.д. И такое. Т.е., long story short, можно сказать, что квасификация это когда мы предсказываем какие-то категории классы, собака, кошка там, лёгов, а регрессия это когда мы предсказываем какие-то действительные числа. Под действительными числами я имею ввиду натуральные числа, да, натуральные числа, непрерывные, вот. нутральное число, то есть, если будет число непрерывное. Суфаса гуд? Дальше можно идти? Думаю, да. Давайте начнем с линейной грессии как безъем. Мне кажется, мы обязаны пройти пройдем. Малите, before we begin, да? Сначала мы начнем. Хотел бы произвести такую очень важную дотацию. В самом деле, каждая тема в этом курсе требует очень хорошей подготовки еще с теоретической стороны. Поэтому я настоятельно рекомендую не ограничиваться этим материалом, не ограничиваться лекцией, а также использовать эту лекцию только как референс. Постарайтесь, вы видели, в репетитории есть книги, на которые можно опираться во время обучения. То есть, не считайте лекции первым височкой информации. Это не так. Это просто легкий референс для легкого понимания погружения в саджект. Всегда идите в первые сочи в книги, какие-то авторитетные книги, я скину их в конце список. В слайдах обработанная в таком легком режиме. Поэтому давайте начнем. Что такое линия регрессии? Линия регрессии – это когда мы пытаемся выстроить линейную связь между признаками входными данными и каким-то целевой переменой. В данном случае, входные признаки – это наши фичи. То есть, наши признаки по возраст человека, его пол, наличие семейного положения, социальный статус, замуж, женат, и т.д. Это входные признаки нашей системы. Мы пытаемся трансформировать эти признаки из X-домена в новый Y-домен. Таким образом, чтобы наш предсказанный Y-домен был очень близок обучаемой выборке. На чем обучались. По сути, справа есть формула 1. У нас есть большой датасет X большая, с m сэмплами. Выдвижественным образом у нас есть X1 и его пара примера в таргете Y1. То есть, это может быть, не знаю, на примере X и X давайте это будет пол, размер ноги, вес, рост, признаки входные. А Y это будет предсказанный возраст. То есть, 35-37. И так далее, и так далее. И эти точки, запятые, это различные сэмплы в нашем датасете. Много будет таких там мужчин, женщин, людей разных в наш цельевой датасет. И по сути, что мы делаем, а вторая формула, самая важная формула в данном случае. Это мы строим какую-то линейную параметры обучаемой. Плюсуем его с линейной трансформацией W1X1. И еще погрешность шума, например, epsilon. Я вообще объясню почему это. То есть, так она просто витминт. Но опять-таки это для кейса, когда у нас одна переменная. То есть, только на вход приходит пол, либо вес его, либо рост. Да, это только одна. Вот если будет две, там будет еще одна такая. Ну, как вот здесь. Больше комбинации растет. То есть, оно говоря, вместо X будет пол, X2 это будет у нас его жерельная масса. И так далее. Вес, рост, это будет такой. То есть, будет больше членов этого уравнения. И по сути, если вы здесь нехитро, можно догадаться, что по сути, вот эта комбинация уравнений, это линейная комбинация. То есть, здесь сентиментальные алгебридические операции это умножение и сложение присутствующего. И по сути, это есть линейная операция. Поэтому она так называется линейная регрессия. Вот. На двойке это получается 1D фича. На тройке уравнение это получается, когда у нас N-ное количество фичей, признаков на вход. Сколько признаков, столько будет коэффициентов уравнения. Double. Вот. И четвертый вид, это получается, когда у нас есть полином. То есть, когда у нас есть более сложная структура данных. Она линейная, но она имеет сложные, более сложные, как сказать, она может управлять более сложные паттерны данных. Но она до сих пор является линейной. Линейной регрессией. Потому что комбинация здесь плюс и минус. Вот. В целом, это про формулы. Сейчас проем всем, как говорим еще, чтобы не обращать внимания на нее. Смотрите, как я говорил ранее, ввиду того, что мы пытаемся просто из X замена перейти в Y замен. Какой-то функции несложной. Мы должны понимать, как и куда идти. То есть, как мы должны сделать так, чтобы наши параметры, обучаемые параметры W. Надо было сказать, что W это обучаемые параметры модели. По сути, это есть мозги нашей модели. Коэффициенты W. Ну, амега. То есть, компьютер должен понимать, как делать так, чтобы W найти. По сути, здесь есть такая понятие, как Optic Function. Я по-разному называю это. Optic Function, Lost Function, Cost Function. Там есть много таких взаимозаменяемых слов. Но, в принципе, я не об этом вам не говорил. В данном случае, это MSE. То есть, у нас есть такая функция. MSE расшироется как... Различная ошибка. Да, да, да. Сами же все верно. Ну, mean square error, средняя культура. А ошибка – это play. То есть, по сути, справа. Пятое уравнение – это когда у нас есть... Когда мы суммируем разницу, потратов нашего предсказанного y. От целевого y. То есть, target variable и predicate variable. И деленное на n количество. На количество популяции. То есть, y' и ti – это, получается, наш target label. Истинное значение нашего сэмпла. А y и ti с head – получается наш предсказанный y. То есть, то, что наша модель пытается предсказать. И думает, что это ее выбор. Честно говоря, мы делаем разницу этих значений, возводим их в квадрат, суммируем все наши сэмплы и делим на количество этих сэмплов. И тем самым мы получаем и высчитываем, насколько в квадратной мере, в среднем, мы отклоняемся от нашего целевого y. Сколько будет сейт. Соответственно, если легко подогадаться, мы всегда хотим, чтобы mse было минимальное. Чтобы... Извините, можно вопрос? Да, можно. Mse, мы же его считаем на тест датасет, мы уже отделяем от training датасет также. Нет. Нет? На одном том же? И для вопроса другом. Ау? Извините. Я хотел спросить. Вы пропадаете. Ну, да. Сорри, кто говорит еще раз вопрос? Чей был? Мой вопрос. Туран, да? Да. Я кажется, да, пропадаю, у меня connection is unstable. В целом, это не так. То есть, mse, когда мы используем, это не факт, что мы обязаны на валидации или на тесте делать это. И для тренинга. Это делается и там, и там. Отвитал на вопрос? Да, да, да. Хорошо. Здесь пока все понятно? По формулам? Да, да, все понятно. Да. Окей. Не стесняйтесь, да, мы детально рассмотрим каждый вариант. Окей. Окей, объяснили про уравнение, объяснили про функцию потери, да? На все данные случаи. И давайте теперь скажем, а собственно, а как проходит принципное обучение? Ну окей, получили мы у вас функцию, да, поняли, насколько мы отводимся, насколько мы отвалились от целевой переменной, да, а что дальше делать? То есть, есть, по сути, следующий ответ. То есть, я написал, optimize with either gradient descent or qualiform solution. То есть, что это значит? Вообще, как я в начале говорил, что даже ледяная регрессия, да, это на самом деле очень фундаментальный и глубокий концепт. Ну, на самом деле его это... Вообще, новички многие считают, что это, простите, это первая модель, да, в которой они встречаются, поэтому они думают, что это очень простая модель. На самом деле, она далеко не простая модель, она очень фундаментальная в своем понимании, и она очень много в себе заключает информации, на которой строятся последующие более сложные модели. Честно говоря, я бы сказал, что в каком-то смысле, в каком-то смысле сложные глубинные сверточные сети, да, которые имеют несколько миллионов сотен параметров, куда легче и интуитивнее работать, чем математика за ледяной регрессией. Это вот, поверьте мне, это мои слова. Потому что MSE, видите, почему я это говорю? Потому что здесь много подходов оптимизации. Видите, она писала, что можно использовать градиент и спуск, да? Ну, вы слышали про градиент и спуск ранее? Да, да, да. Не-не. Извиняюсь, вы, получается, имеете в виду то, что с RNN-ками более сложно работать, чем с CNN-ками? Еще раз, еще раз, сори. Что еще раз? Это, получается, вы имеете в виду то, что с RNN-ками сложнее работать, чем с CNN-ками? Не-не, я не про RNN, я говорил про ледяной регрессии. Я не говорю, что сложнее работать, я всего, что концепт вообще, который внутри математически содержит ледяной регрессии, она на самом деле не такая легкая, как кажется. То есть, я просто здесь не стал перегружать слайды, на самом деле, если вы посчитаете, как вообще под капотом почему ледяная регрессия работает, почему накладательство, то существуют какие-то. Она будет куда сложнее, по моему пониманию, чем те же ровные сети сложные. Хотя, ну, старее, смотри. Почему это так? Потому что я продолжал дальше. Есть градиэтый спуск, да? Кто-то слышал про градиэтый спуск? Да, слышали. Ну, понимаете, да, что это просто метод оптимизации, численно метод оптимизации для того, чтобы найти какую-то optimal функцию сложнее. Но он работает здесь тоже, то есть, я также могу использовать градиэтый спуск для того, чтобы посчитать ледяную регрессию, ошибку и параметры. Но в данном случае, если так вот существует close-form-солидар, то есть решение закрыть форму. Ну, потому что функция не такая сложная, и мы можем какой-то без итеративного способа получения градиентов, мы можем найти сразу точку оптимума. Потому что функция не такая сложная, можно просто... Ну, это параболло, если вы на СЕ это посмотрите, это, по сути, параболло в своем виде. То есть он будет вот так вверх смотреть, и у него есть какой-то... С учетом того, что это выпуклая функция, или вогнутая функция, то там есть один локальный минимум. В соответствии, локальный минимум можно найти на приравнед градиэт или производную нулю, тем самым, и сразу получим точку оптимума на нуле. То есть это будет самый низ функции. Опять-таки, возвращаясь с первого слайда лекции, это все работает, потому что мы здесь assumим, предполагаем, максимум light-field estimation. Отдельно прошу каждого почитать, что такое максимум light-field estimation в контексте классического мышленно-обучения, дабы понять, почему это все работает. Я не в праве объяснять, потому что это займет очень больше времени, но в целом вы должны почитать, что такое MLE. Вкратце могу сказать, это принцип того, как вообще зародился ML, в плане, что мы... Ну, что это значит? Это значит, что мы пытаемся всегда сделать во время обучения, estimating наши параметры так, чтобы при каких-то... Ну, чтобы вероятность исхода событий какой-то из системы всегда была максимально продоподобна, то есть у меня был максимальный всплеск вот лоджета при каких-то входных параметрах, входных данных. Именно это концепция подразумевает то, что... то, что в начале говорил, что все крутится вокруг... нормально, к сожалению. Не знаю, если я корректно объяснил, но вообще почитайте про MLE. На самом деле очень важно, это, конечно, вряд ли будет в рамках этого курса, но, я думаю, для глубокого понимания сабжекта или диней регрессии в целом нужно понимать, что такое MLE. Ну, окей, путать не буду, наверное, дальше не буду. Ну и Scikit-learn это хорошая оптимизация моделей в коде. Окей, давайте дальше пойдем. Вопросы какие-то есть здесь по диней регрессии? Прием-прием? Вроде нет. Окей, давайте теперь про логическую регрессию. Логическая регрессия это, по сути, задача уже классификации, беда в классификации в частном виде. Ну не всегда, да, на триллере. То есть иногда можно сделать мультикласс классификации. Перевью. Насколько важно нам запоминать эти математические формулы? Честно сказать или не честно сказать? Нет, ну естественно, честно, конечно. Честно сказать, очень важно. Все? Реально все важно. Потому что, на самом деле, я скажу, ну это не всем такой ответ будет, корректный, это правильный, но, дискурс следующий. Что, скорее всего, вы эти формулы никогда не будете сами приписывать. Ну, то есть, на работе вашей там... Они же в библиотеках прописаны уже. Да, но это вот да, поэтому я и говорю, да. В целом, никогда там не будете прописывать сигволи с нуля. В принципе, не нужно, потому что есть куда побольше библиотек, которые уже оптимизированы, под это сделаны, и в первый раз будут работать лучше, чем будет это сделать. Но почему это важно, почему это проходит, и мы почувствуем, что это делают. Понимаете? Не зная этих понятий, да, этих фундаментов математических, алгаебраических, я не знаю, линии для алгаебра, да, и всей этой каши, вы на самом деле не сможете быть... Не сможете по-настоящему, прямо по-настоящему, по-настоящему повторю, понимать, что... как устроены эти алгоритмы. То есть, вы не сможете никогда хорошую модель построить, или улучшить текущую хорошую модель, зная, опираясь на знания только использованные пакеты финпорта. То есть, импортируют пакет из А в Б и все. И это факт. Поэтому единственное, что вам может помочь, чтобы не быть таким программистом, инженером в машинном обучении, это понимать эти азы. Поэтому ответ – да, это важно делать. Тогда вытекает следующий вопрос. В эмейлинге есть алгоритмы машинного обучения, типа там Андрева решение или Леннинг, и так далее, и так далее. Но я так понял, сколько изучал, я понял, что в основном применяются XGBoost, Catboost, LightGBM для работы, например, с этими гиперпараметрами, та же обтюны, условно. То есть, те же Андрева решения и случайный лес, они в основном уже не имеют никакого значения. Андрева решения и случайный лес, они в основном уже не применяются так широко. Применение идет только от XGBoost, LightGBM, условного. Нам в любом случае нужно понимание математических формул, старых алгоритмов эмейлинга, или уже не актуальные. И можно понять того же XGBoost. Хороший вопрос. Спасибо за этот вопрос. В целом, да, все верно, частично, что вы сказали. То есть, в принципе, гранитный бустинг на имплементации XGBoost, LightGBM, Catboost, он будет почти всегда победителем, если будет сравнивать его с генеей регрессии простой. Но тут уже есть просказывание, непонимание материала. Либо и гранитный бустинг устроится на основе генеей регрессии, либо того же Андрева решения. То есть, мы дальше затронем тему гранитного бустинга, и вообще бустинговых к тебе есть алгоритмов, или бэггинговых к тебе есть алгоритмов. Они все используют weak-neurner, то есть слабые алгоритмы, такие как генеей регрессии или древа решений, для построения внутри себя всей армии таких алгоритмов. И эта армия вся и есть гранитный бустинг. Я смог ответить на вопрос? Да, да, да. Опять вернулись туда же, видите? Вы смотрите, опять, да, import, exit boost, data pack. Но если вы не будете понимать, что внутри это decision tree такое же, это будет, скажем, странно очень. Да, я понимаю, я имею в виду, есть, например, алгоритмы, которые устаревшие, там, M-Bella алгоритмы, которые вообще не используются в практике, и есть там новейшие алгоритмы. То есть не легче ли нам просто понимать математические, статистические новые модели, чем старые, например, словно? Вот я вот к этому. Я понял. Знаете, ну, смотрите, вообще trend такое существует, что если бы эта гипотеза, гипотеза, то что вы сказали, она была бы верна, то сейчас бы все кегл чемпионы в мире, чемпионаты по разным соревнованиям, то выиграли бы нейросетками. Верно же, потому что они новые. Но если вы посмотрите на самом деле, как происходят дела сейчас на кегле, то вы увидите, что нейросетки хоть и выиграли какую-то часть позиций в CV, в NLP, в более сложных свинчах. Я не скажу, что они смогли перебить те самые градентные бусинки в каких-то задачах. Я к чему говорю, к тому, что есть до сих пор, и существуют всегда будут существовать какие-то задачи, каких-то доменов очень узких, специфичных, где более сложные современные алгоритмы не так хорошо работают, как старые алгоритмы. И по сути, то есть, ну, логическая регрессия, я не сказал бы, что это вы никогда не будете использовать. Я уверен, вы будете ее довольно часто использовать даже в практике. То есть это очень фундаментально грейдово. Старое самоление регрессии – это вполне себе неустаревший алгоритм. Вот. Уже бывают задачи, где можно градентную бусинку решить, а можно линейной грейси решить, и линейная грейсия будет гораздо лучше работать. Вот. Понимаете, помимо того, что очень показательная модель, да вывести точности, то есть, лос панический сделать, а всякие прессисты, далеко почти хорошие, вы должны еще думать, когда вы выйдете в проде, вы должны думать про то, как алгоритм еще в кино работает по скорости. Вот. Соответственно, более простейшие алгоритмы, они работают гораздо быстрее, чем те же сабусники сам. То есть вопрос куда глубже, на самом деле. Ага. Понятно. Все. Я теперь все понял. Спасибо. Вот. Ну давайте про линейную грейсию поговорим теперь. У нас сегодня очень мало времени, да? Окей. Смотрите, это про бинарную классификацию. Не всегда можно будет классификацию использовать. Это также опять-таки та же самая линейная комбинация, что и про линейную грейсию, да? Но еще присобачивается, да, поверху, вернее, в конце, не линейная функция активации, да? То есть вон non-linear activation function. Ну а usually sigmoid используется. Sigmoid это вот это вот восьмом уровнении. Это формула sigmoid. Вот. Объясню почему, да, позже объясню, почему она используется. То есть, по сути, если мы видим, да, то это тот же самый клон линейной грейсии, только в конце просто еще можно задержать, даже надо засунуть все это вот уравнение, да? Вот это вот это уравнение, внутри этой функции. И все. Вот. Ну и помимо этого еще, из-за того, что это задача уже классификации, да, а та была задача регрессии, да, мы должны изменить нашу функцию. Как вонение, как Сарежан сказал. То в данном случае это у нас уже пинарная экросентропия. Вот BCE, да? У нее другая формула, у нее вот такая формула. Ее тоже надо знать, к сожалению. Но она не такая сложная. Вот. Объясню позже, что такое кросентропия и энтропия. В целом, кто знаком еще с энтропией, кросентропией, в общем, других деменов? Я еще не знаком. Физики, может быть, есть, математики? Я знаком. Энтропия, по сути, уровень рандома. Да, поверхностно знакомы с этим. Ну, энтропия, да. Ну, да, уровень рандома очень-очень схож. Очень схожая, это, по сути, на самом деле, мера хаоса в системе. Насколько система неопределена. Вот, понимаете, при моделировании любой этой системы, ну, системы имеются задачей машинного обучения, есть какое-то подпространство данных, да, ваш subset data, он какой-то, да, он имеет какую-то физическую, физическую какую-то, заставляющую в себе информационную, заставляющую в ваш dataset. Ну, для одну мы можем сделать gnost 0. Вот для primera м제로etter 0 на squishy, для Honda нае конс厲害ный. Ну, Universe 0 мы надеемся. А если бы букс Din up теперь Об busy Вот господа мэна, detailed поэтому мы пытаемся не терпевшись с оппонентами. Это функция потери. Опять-таки, это то, что я говорил про кросс-антропию. Антропия, это то же самое, что кросс-антропия, просто это... Всего информация, это больше про количество информации в битах, чтобы нужно, чтобы передать информацию из А в Б. А кросс-антропия, это то же самое, только когда мы сравним два статических распределения. У нас есть референс, статическое распределение, истинное, ну почти истинное, куда мы стремимся, наш PDF какой-то. И есть другой PDF, который мы истимитим, мы строим его насчет модели. И мы пытаемся эти два распределения применить друг другу максимально, чтобы у них было меньше разницы в антропии. Но опять-таки, почитайте больше, это домашнее знание. Окей, дальше. Эта оптимизация идет задачей тоже через градиентный спуск. Здесь уже нет каких-то ковс-пан-солушен, потому что функция слегка сложнее. Ее так просто не оптимизируешь. И опять-таки, как я говорил ранее, здесь ассумпшн строгий идет, что это используется подход максимум лайк и хутостименшн. В принципе, максимальное правдоподобие. Ну и опять-таки, сайки твердые. Вот. Окей, мы закончили с лекциями. Теперь мы вот этот... так, все живые. Нормально все. Нормально. Ага. Смотрите, да, деслайд. Хотел бы очень проявить внимание, постарайтесь принять мои слова максимально серьезно вот на этом слайде. Я вот... Если вам пригодятся вообще в этом курсе, дай мне только, и попав в все ваши карьеры в дальнейший, я думаю, это... Если вы укрепитесь в том фундаменте этими знаниями, хорошо с этого получите пуск какой-то. Вот. Здесь пишем от 26 года, pattern edition, ваш шоперник. Это очень сложная книга. Не спорю, у меня парамоходы очень высокие. Туда сложно читать номенчикам, но старайтесь читать ее все равно. Попытайтесь, если вам не нравится, то я вам не буду. Туда сложно читать номенчикам, но старайтесь читать ее все равно. Попытайтесь понимать, что там вообще пишется. Также есть Фаранцов. Российский дес, яндацовский тренер, учитель, преподаватель, профессор. Очень хорошая тоже книга у него есть. Следующая. Также есть Зорич по Матану. И также есть более практические книги. Горелик и Корман по Питону и по алгоритмам. Вот, второй совет это, наверное, пытайтесь все непреметить from scratch. То есть из нуля. Многим этот подкушень не нравится, но я его практикую. Вообще всем рекомендую, потому что когда вы from scratch что-то делаете, гораздо лучше понимаете сабжект. Ну, имеется в виду там не портуться скейлер на линейную грестию, а создать свою собственную. Это будет куда круче, сам. И пытайтесь понимать do understand every detail of every part. Не ссылать на какой-то assumption, я там плюс-минус понимаю. Не нужно понимать это, понял, это не понял. И в общем я понял. Пытайтесь понять всю часть кода, всю часть уравнения, чтобы вы полностью понимали, что происходит. В целом окей, давайте перейдем на... на VS Code. А вы вот эту презентацию, или как только что показывали, вы скинете в общий доступ? Ну, чтобы мы... Да-да-да, конечно скину, конечно скину. Я про книги, которые вы советуете же. А, именно книги сами? Не-не, ну там же в конце в любом случае в этой презентации написано какие книги. Да-да-да-да-да, я книги не скину, но я скину, да, эти названия. Спасибо большое. Ага, пожалуйста. Окей, давайте движемся сюда. Позвольте всем не видно мой экран. Да, видно. Окей, давайте по-быстрому пойдем. Смотрите, импортируем билетеки. Как я говорил ранее, SkyLearn. Матлит, панда, сноумпай, это и т. п. Все просто, все понятно, ничего такого нет. И вот, в общем, мы все поняли. И вот, в общем, мы все поняли. И вот, в общем, мы все поняли. Все просто, все понятно, ничего такого нет. В принципе. Давайте теперь разберем пример линейной регрессии. С размерностью 1D на размерность признаков фичей. Ну, размеры признаков входных данных. То есть я тут указал уравнение наше. У нас есть какая-то линейная зависимость от y к x с коэффициентами p0, ну или омега 0. И омега 0, либо бета 1. Из-за того, что это 1D фича, то мы используем только b1x. Если было бы больше, то одной фичей было бы еще 1++. Бета 2x, допустим. Вот. Ну, в целом, это просто формальность опять-таки. Давайте создадим какую-то, есть такая функция, замечательная от SkyLearn MakeEgression. Это такая функция, которая создает сэмпл данных, которые можно регрессионно аппроксимировать. То есть я создаю свой sample.dataset x1, далее количество сэмплов к 1000, размер фичей 1D, ну или какой-то уровень шума 10. Я не знаю, в какой мере это, ну 10, какой-то уровень шума, чтобы не были данные слишком сететически идеальные. Ну и random state 42 для того, чтобы... Давайте теперь все перейдем. Все понимают здесь random state 42, что значит? Ну, кто понял, что говорить не надо, а кто не понял, говорите, я не понял. Кто понял, ничего не говорите. Я не понял. Не понял. Это сет рандомайзера, правильно? Да, да, да, да, да. Окей. Ну, я объясню это. Да, спасибо, что ребята сказали, кто сказал, не знаю, это все честно. Не стесняйтесь, пожалуйста, ладно? Потому что я должен сделать свою работу правильно, поэтому я обязан, чтобы все понимали. Ну, хотя, например, 90-е понимали, что я говорю. Окей? Хорошо. Договорились? Поэтому если реально что-то не понимаете, говорите, я не понимаю. Здесь ничего такого нет. Мы все учимся, все что узнаем новое. Окей. Смотрите, random state вообще это... Акчан сказал, это как бы, ну, такая штучка, зерно, да, я не знаю, еще random seed иногда называют. Это такое как бы число. Вернее, давайте так, забудьте, что я сказал. Смотрите, существует random число, да? На самом деле их не существует. Random число вообще не существует в теории. Ну, в теории не существует, вы на практике их никогда не видите. Допустим, даже когда вы какое-то там событие в мире видите, оно кажется, возможно, вам рандомом, на самом деле это нифига не рандом. Он близок к рандому, но на самом деле есть какой-то цепочку событий, которые появились к этому событию. То есть абсолютно рандома никогда не существует. То же самое и в жизни, и в компьютере. То есть когда, допустим, импортирует Python, NumPy, массив из чисел, 100 чисел рандомных, они, конечно, кажутся рандомными для меня, но на самом деле они вычисляются там тоже с какой-то закономерностью. То есть он берет какую-то там текущую дату, текущую там дату, секунду во времени, текущую там память моего файла, вот так что там перемножает, делает, делает и получается рандом. Но по сути это называется псевдорандом. Соответственно, видите, для чего рандом-сет теперь нужен? Для того, чтобы условно говоря, вот вы сейчас копируете, я сейчас проведу какую-то работу, в этом выйдет в конце, после эксперимента число 45. Допустим 45, что-то выйдет 45. А прикиньте, потом я вам даю этот датасет, сампл, ноутбук, вы избираете себе, и вы можете в этом датасете, и вы можете в этом датасете, и вы можете в этом датасете, вы избираете себе, проигрываете то же самое, и у вас входит 43, 44. Как бы в целом ничего такого, но в целом можно перетраться, почему же это-то чуть разные. Это случилось потому, что во время вашей естественной интеллизации рандомных переменных был аутстейд другой, ну там какой-то там, не 42, а 50 был. Соответственно, у вас вот этот рамный стейд, помогает создать, детерминировать рандомного процесса, то есть мы можем в вас создать рандомный процесс заново. Он будет рандомным, но опять-таки он будет условно рандомным. То есть, условно говоря, я около 42 рандомстейд и запущу эту xy, потом вы дома сделаете так же самое с родной 32, у вас такие же данные получатся. Они будут рандомные, но они будут такие же. Если я сделаю 44, то будет немного другая xy. Они тоже будут рандомны. То есть это для того, Это для того, чтобы мы частично сталкиваться и вы даже понимаете, почему же это что такое random state и random seed. Это для того, чтобы мы могли реплицировать чужие работы, чужие данные, чтобы ученые могли в разных точках мира садиться на них в этих цифрах. Потому что даже вот такие маленькие изменения, они могут быть колоссально высачиваться. По сути это проект. Давайте бежим дальше. Дальше, смотрите, здесь я спличу датасет. Все знакомы с понятием датасет-сплит? Что это значит, и для чего это делается? Да. То есть повторю еще раз. Что не знал? То есть обычно мы всегда во время машин обучения делим наши обучаемые данные на 2-3 подгруппы. Это обычно train, test и validation. Три выборки. Делается это обычно в такой патроне, что обучаемая забирает большую часть выборки, потому что она обучена на них, поэтому требуется больше информации. А тест и validation она делает для того, чтобы уже отложить данные и избежать какой-то предвзятости в результатах. Что модель не увидел эти данные, и результаты скоррега этой модели были более объективны и адекватны. Честными. В данном случае есть прекрасная скритер функция. Все в виде пользы train, test, split. Когда мы делаем наш x, y датасет, соответственно, на 2 типа. Train и test. С таким 0,2 шагом развеяния. То есть 20% от всего датасета, допустим, если у меня было 100 samples, то 80 samples будут в train и 20% в test. И опять-таки random set 42. То есть у меня попадут те же самые примеры, как и у вас попадут дома. В train или в test. Это понятно? Да. Отлично. Дальше давайте мы инициализируем пока что model regression. То есть пишем model равно line regression, мы импортировали эскадеронский класс, создали линейный regression. Вот. На вход ничего не получаем, дальше ее обучаем. Fit. Этот fit обучаем нашим train, xtrain, ytrain. Она обучилась моментально за сотни секунд. Давайте посмотрим. На 2 секунды все. И далее мы делаем prediction. Мы делаем prediction опять-таки уже не y, не на xtrain, а на xtest. Потому что мы хотим посмотреть, как модель показывает себя на данных, которых она не видела во время обучения. Ну и сохраняем эту перемену ypref. YterScorePref. Дальше проводим эволюацию модели путем мета средней ауказотичной ошибки. Если мы видим, что я импортировал отсюда. Испортируем matrix, import, min.square. Вот посчитал. На вход завел туда истинный ytest, истинные данные, которые как бы таргетами являются true values. И также закинул туда наши предикты нашей модели. То есть как модель считает на тот или иной sample xtesta, как она считает его ypref. Как он должен быть. ответственно мы загоняем туда и выплевываем наше МСЕ. Миску атерора. Дальше спотим и ну и это окей. Как мы видим, здесь миску атерора 107.9. То есть в среднем, как это депокировать? В среднем наш разброс вывода нашей модели с сравнением с истинным показанием модели, модели, а данных квадрате, если возвести, будет 107.9. Как бы то сега не всем понятно, и что, как это понять дальше. А чтобы понять это лучше, можем квадрат возвести, чтобы было оригинально, да, меры числения. То будет допустим 10 скобейками. То есть в среднем у нас абсолютно мы на 10 каких-то значений ошибаемся. Вот, от телевого нашего значения. Вот, ну здесь такая мера, потому что это был синтетический датасет. И вот давайте посмотрим, что по сути получилось. Я заплатил здесь каторплот. Синей точки, то наши тестовые данные, если не ошибаюсь, это тестовые данные. Видите, они в виду того, что они были созданы синтетически, они имеют какой-то паттерн длини. И видите, мы построили красную, красная линия, это есть наше длинение, это гейси. Это по сути вот эта красная линия, с математикой знакомой, это по сути вот это уравнение. Вы видите, это тестез. Можете, пожалуйста, там blue написано. Я сам красную линию регрессии, может, поняли, там, colo blue, delt и scatter, вот вопрос у меня с катором. Ага, ага. А что именно i, вот это, обозначает вот этой точке? Можете, пожалуйста, объяснить. А, это рассул, да, говорит, рассул? Да, да, да, это я. Ага, наслусь, смотрите, да, вот это, да, линия? Да, scatter. Смотрите, да, plt.scatter, это по сути, plt – это модуль Matplotlib, вот он, да, Matplotlib – это по сути, mathematical plotwing library. Это по сути библиотека для того, чтобы плот это, ну, это графики строить, в бетоне. Splot, то есть я просто и назвал plt. Я говорю i plt scatterplot, scatterplot – это получается, ну, я не знаю, как переводится на русском, но это значит как бы построить точки, точечно построить точки на двух осях. Извините, можете, пожалуйста, plt plot и scatter label actual и предикты поменять? Та поменять или нет? Вот actual предиктить. Что поменять не всем? Например, plt scatter и plot, например, поменять друг другу. Ah! Label поменять. У вас тогда название сверху слева поменять. Это просто label. Все хорошо, понял. Ну нормально, все хорошо. Все понял. Да. А, ну да, color – это просто цвет, вот видите scatter – это синий цвет. Все, я понял, спасибо. Полточка. Видите, на фото каре функции, чтобы 2D, это 2D проспрацер, у нас есть Y ось и X ось. Чтобы в 2D построить какой-то значимый график, видимость, нужно две точки опорные. Иначе вы ничего не построите. Вы не можете построить 2D рисунок с одной осью точек. Потому что у вас всегда значения были, здесь должно быть кое-какую причине, здесь должно быть кое-какую причине. Если посмотреть, давай я тебе покажу, может быть, я плохо показал. Xt, да? Давайте возьмем первый, первый, пятый точек. И whiteness. Вот смотрите. Значит, видите, по X, вот у меня ось X, есть точка 1.52. Она где-то вот здесь. Ну, я понял, в России обсессия объема, вот это выбирается. Да, да. И, соответственно, 29-я, она где-то здесь. А вот, касательно, можно ли этот график сделать на 3D, то есть сет добавить? Да, да, да, мы это сейчас сделаем. Да, да, да, мы это сейчас сделаем чуть позже. Я покажу два дофича. Как раз мы это сделаем. Просто красивее было бы. Угу, хорошо. И смотрите, по сути, мы видим, что мы построили линейную эгрессию, которая зафитилась параметрами, коэффициентами своими, так, чтобы предсказывать следующий паттерн. Допустим, теперь скажем, давайте, там, придет мне на вход какая-то новая точка, тестовое с деплоя, там X минус 1, она будет X равна минус 1. То есть мы загоряем ее сюда, смотрим ее перчей на красной линии, и это будет минус 20, да. И, скорее всего, если такая природная задача существует, это будет реально корректным данным. То есть мы, видите, мы этой линией предсказываем, новый параметр, который модель в целом не видела ранее. Вот. В этом, по сути, видите, то, что я в начале говорил, мы предсказываем, пытаемся сделать так, чтобы машина пыталась, без каких-то явно выученных правил, предсказывать новые явления, новые события. Я все понятно, да, Карцер? Надеюсь. Да, теперь прямо, да. Ага. Смотрите, теперь давайте пройдем прямо на интерпретацию, да. Смотрите, у моей обученной MullModel есть такой аргумент, не аргументы, а атрибуты. Coef и Intercept. По сути, это Coef, это есть наши w или beta, да. А Intercept это наш нулевой, да. W и beta, 0 или w. Ну Intercept это, знаете, это пересечение на оси y. То есть это наш, по математике знаете, это наш этот сдвиг. Вот. И, по сути, смотрите, модель выучилась, да, она что-то обучила. Давайте посмотрим, что, чему равны наши коэффициенты, чему равны наши мозги, наши модели. Они равны, смотрите, 16.71 и bias, да, это, ну, сдвиг, да, Intercept. Он равен минус 0.09. По сути, это все. Угу. Используя эти две, два этих переменных, мы можем построить эту линию, видите? Все понятно. И, смотрите, если кто математику помнит, да, 16, да, интерпретировать можно как наклон, да, наклон линии, и производную, да. Если мы посмотрим, то на самом деле, ну, кто может помнить, да, y равно кx в школе, кx плюс b. Да, да, да, да, к это есть weight, да, наш. Если посмотрим, да, х будет, да, вот, вот, протащим, это будет 16, примерно, потому что у меня k равно 16, 16.7. Мы видим, да, что, да, это где-то рядом 16, это вот здесь. В принципе, это и есть наша линия фокса. Разве k это не разница y поделить на ту же разницу x, то есть, если мы в два значения, то есть, любые две значения. Да, это так есть? Это же есть наклон? Да. То есть, тангенс этого угла, да, который он пресекает. Было бы интересно, еще и пара было сделана. Да, там дальше будет. По сути, да, это есть наклон, да. У меня вопрос, вот, до этого вы приводили пример, что, допустим, если x будет минус один, то значение y будет, скорее всего, минус 20, так как ранее этого значения не было. Допустим, в диплейменте у нас поступило такое значение. Поменяйте. То есть, вы не только значение, допустим, у нас через некоторое время поступило какое-то количество новых данных. Перестроится ли в этом случае вот эти коэффициенты smoke и interception с учетом того, что, то есть, будете пытаться модель найти новый шлок и новый intersection так, чтобы туда, ну, вот не было вот этих новых данных. Точнее, чтобы линия построилась таким образом, чтобы были вот побольше этих белых пробелов. Окей, куда ты понял? Спасибо за вопрос. Давайте проясню, правильно я понял его. И спрашивайте, когда в сценарии диплоя, когда мы, скажем, выводим модель уже в боевую среду, приходят какие-то новые данные и адаптируются ли модели к этим новым? Да, то есть, то, что она как у меня на интернет подвисает или у вас? Скорее всего, у меня. Да, у вас она подвисает. То есть, будет ли, то, что она будет подстраиваться, наверное, да, она же сама себя будет как бы улучшать. То есть, она, мне просто новым показалось то, что вы до этого сказали, то есть, линия выстраивается, ну, помимо того, что там математически она еще и пытается такие вот эти вот white-споты, да, найти, как будто бы. Смотрите. Я правильно поняла вашу предыдущую мысль? Давайте так еще раз смотрим. В целом, модель, она на данном этапе, в этой имплементации, она не переобучается. То есть, она не будет такой, что она будет все заново. Это если вы такой сделаете регуалент, переобучение там раз в неделю, да? Она крутится, у вас крутится, собирает новые данные клиента, а прискорит. Переобучилась, вскорит, переобучилась, потому что существует понятие как даташифт, да, датадрифт, когда со временем до сезональности какой-то предпочтение клиентов или какие-то входные данные системы, они тоже меняются с учетом времени. Допустим, ну, летом люди покупают арбуз, зимой покупают там обогреватели, да? Тоже это, видите, изменение поведения в структуре данных. Третье, для такой проблемы данным уже все время как бы ставят регламент переобучения, чтобы модель, если вы обучите модель, которая там работает на арбузах, она зимой будет хуже, потому что арбузов в принципе нет. Вот, если вы должны какой-то, ну, постоянно переобучать модели, ну, в режиме автомата, ставить регламент, там настрой какой-то какие-то джабы, и они перерабатывают эту модель и переобучают. То есть, модель все время движется с актуальными данными в тренде. Вот. Ваш вопрос, второй посадит вот этих белых спотов, я не всем понял, но постараюсь ответить. Понимаете? Что вся соль в этом алгоритме в том, что мы пытаемся за счет паттерна этих точек, да, вот просто визуально уберите в голове свои, уберите эту красную линию, да? Согласитесь, вы видите, да, какой-то паттерн, рисунок, видите, что они как-то вот туда и туда, вот так вырастут. И по сути, модель пытается создать вот такую же... Зависит, да? Была самым интересным моментом. Самым интересным моментом, да, у меня unstable, sorry. Да, получается, модель пытается вот эту красную линию сама нарисовать таким образом, чтобы вот этот MSE, mean squared error, был наименьшим, понимаете, то есть был самым маленьким. И после того, как она, как бы, она это сделает, это будет линия называется best fit line, да, линия наилучшего фита, да, ну не фита, это совпадение с точками. То есть, видите, вот эта линия, да, она построилась так, вот эти коэффициенты weights и bias выстроились так, отобщинились, что ко всем этим точкам в совокупности, она очень близко проходит, каждый из них. Да, да, да. Просто вы до этого привели пример по поводу, допустим, в случае, если x будет минус один, то у Y, скорее всего, будет минус двадцать, и мы видим, что в этом месте нету данных. Вы помните, да, вы про это сказали? Да, да, да, да. Да, и что в этом и есть суть, что машина продвигтит значения, которых она ранее не знала, не видела. И я подумала, что в этом он реально проходит через white spots и там большую часть. А может быть, что вы имеете в виду под white spots? White spots – это там, где нету точек самих, вот пространство между точками. А, я понял, понял. Да, в целом в solid, да, может быть, как-то Сран сказал, в целом, давайте покрою просто, да? Так, сейчас мы это сделаем, а потом давайте... А, смотри. Сейчас мы это сделаем, а потом давайте... А, смотри. А, смотри. Так, сейчас мы это сделаем, а потом давайте... Смотрите. Смотрите, я сейчас... Что я сделал сейчас? Поняли, да? Я просто загадал модель XO5, да? Ну, давайте N1, если украшение получится, давайте 2.5. Вот, а вот просто зашла новая точка данных значения 2.5. Ну, давайте 2.5. Вот, а вот просто зашла новая точка данных значения 2.5. И он говорит, будет 41. Видите, на выходе 41. Сейчас я понимаю, что как он это сделал? Он просто вот под эту линию, это максимум чистка линии, а просто функция. Он просто на X засунул 2.5 и выпрямил на Y, у меня там, ну вот сколько, видите, 2.5 здесь, примерно, ну, да, на графике там 40 с копейкой. То есть, видите, модель просто с учетом этой новой функции, которую мы выучили, красная функция, линия функции. Она предсказывает новые данные, может предсказывать новые данные. Конечно. Ну, это окей, что сразу не заходит, но это нормально, поэтому посмотрел. Как раз таки, как красная линия считается, как по красной линии посчитать через X, Y, это как раз таки понятно. Ну, хорошо, окей, я просто не хочу много времени занимать. Хорошо. Основной принцип выстроить линию, основная цель выстроить линию, чтобы минимизировать МСЕ. Вот это главная цель, да? А не выстроить ее таким образом, чтобы через white spot и... Все, все, все понятно. Хорошо, спасибо большое. Да, не за что. Дальше, смотрите, я хотел еще подсказать, если про интерпретацию, мы с webisome weights договорились, подали. Давайте теперь вручную это сделаем. Чтобы развивать подручную способом. Смотрите. Вот sample test input. Я беру 10 первых примеров от X-теста. Решеплю, минус один, в один, в один DRA перевожу. Вот мои X-ы. Sample test input. Вот prediction, который сделал автоматический метод, вот этот model predict, вот этот, ну, зашитанный с рефукс. Вот, ну, вход predict 10, ну, и вот вот 25, там, 5, 21, и тд, и тд. Также я хотел поговорить вам, ну, для награды, чтобы вы понимали, как это вообще работает. Я, видите, вручную это сделал. То есть y'1 равно x-тест 1 умножить на model coefficient, плюс model intercept. Как в этом уравнении, да, вот здесь. Понятно, да, здесь? Да, да, да. Да. И на выходе, да, видим, что одинаковый датам. 25, 5, 31, вот здесь автоматически, и здесь уже мануально на путю. Мы и мысль честь сделали, они полностью совпадают. Ну, чтобы понять, что уравнение оттуда идет. Наверное, сейчас мы понимаем, да, для чего надо понимать, да, все эти функции. Потому что они, ну, бывает такое, что в работе вы не всегда можете сделать свою задачу только готовыми пакетами. Нужно как-то кастомизировать свой код и пытаться вот что-то самим там какие-то свои касты придумать. В данном случае, предположим, что предикт не существует, да, метод? Как это сделать? Вот я так сделал. А видите, если бы я не понял, не знал бы уравнения, я бы это, наверное, не сделал. Да? Окей, давайте посмотрим более сложный пример, теперь 2D фича регрессии. То есть у нас такая же линия регрессии, просто теперь у нас сложность выросла. Ну, не сложность, а скорее всего, у нас выросло количество входных параметров. Извините, а у нас по времени сколько еще осталось? Потому что мы уже вышли по издай рамке времени. Да, мы пипец как вышли. Я просто думаю, мы успеем это покрыть или уже лучше? Ну, смотрите, вообще я человек как бы нрава, да, то есть я, в принципе, могу оверворк делать, заниматься. Кто хочет, тот может уходить, кто хочет, может остаться, я могу так и остаться, потому что наверное дальше у нас не будет времени на это. А сколько примерно? Еще раз? Сколько еще потребуется времени, чтобы закончить? Давайте, если мы будем быстро идти, то, наверное, я полагаю минут, наверное, еще 20 максимум. Да, просто как бы ну, я, конечно, все понимаю, но с другой стороны, у нас тоже, там есть соммитменты, да, по вечерам мы планируем, что вот к этому времени закончим и потом что-то планируем. И ну, уходить, конечно, да, мы можем, но не хотелось бы пропускать, поэтому хотелось бы... Можно комментарий внести? Тут Бибарс делает нам одолжение, потому что он как бы остается. И тем, кто, кому некомфортно, он может присматривать видео, видео всегда будет доступное. Да, тоже да, предусмотрение рассчитано на количество вопросов, которые мы сдаем, или мы можем запустить линейные регрессии 2D, которые вот этот код. В целом, как бы, я понимаю, да, на самом деле, наверное, первые пристрелы лекции, видите, у нас была просто предыдущая лекция, на нее половину потратили времени, поэтому да, мы только на предыдущий материал. Поэтому, я думаю, как бы, я тоже, видите, учусь, поэтому я тоже возьму обратную связь с вас, буду быстрее пытаться материал сделать. Ну и поэтому, мне кажется, наверное, нам будет в будущем лучше, если будем чуть меньше до... до сетнима и таких моментов. Ну будем прям очень быстро бежать. Потому что курс, на самом деле, очень жатый, я понимаю, мы должны прям очень сильно зайти по всем моментам, максимально глубоко и максимально широко. Поэтому давайте, я предлагаю 20 минут, ровно тогда 10 секунд закончить. Вот, поэтому... Как вам? Ну, если... Да, давайте так и сделаем. Ну, давайте, окей, по 2D фичам пойдем. То же самое, да, только 2D фичи, то есть добавляется еще одна ось сложности, то есть, видите, n features, n равно 2. То есть все то же самое, так и раньше, просто теперь n features равно 2. То есть, если посмотрим здесь, вот x, тез, да, видите, у нас теперь уже на каждый пример, ось примера, у нас есть 2 оси дополнительные. То есть, входная вторая перемена, x1, x2, допустим. Ну вот. И, смотрите, теперь то же самое делаю, то же самое Minsk-Vaderor, саплот только поточит похитрее, как вот спрашивали, не помню раньше, 2, 3D, формате. По сути, вот 3D формат. Понимаете, смотрите, ввиду того, что мы добавили новую степень свободы в нашей модели, вот эта вторая ось, ну это Tegereo Freedom называется, у нас появилась более сложная реструктура данных. То есть, теперь, смотрите, у нас теперь не линейная разделяющая, а разделяющая плоскость существует. Ну, не разделяющая, а предсказуемая плоскость. То есть, теперь, видите, на 3D визуализации у нас есть ось Pitch1, x1, Pitch2, x2, еще есть target, ну, для значения нашей, точек. И, по сути, ну, это слегка сложно представить, я не смог сделать adjustable динамический график, чтобы покрутить его более понятно. Но, кто может визуализировать, представить, то вот эта плоскость, квадратная красная плоскость, она делает ту же функцию, как та линия красная, только теперь в 2D пространстве. То есть, она в плоскости пытается разрезать эту эту ось, и точек, как они разложились, и также предсказывать дальнейшие 2D фичи, которые будут в будущем приходить, и в прошлое. То есть, теперь, видите, модель стала сложнее, она выросла на два параметра, на один параметр, два раза выросла. Для наоборотности это так работает. А вот какие-то вопросы? Вопросов нет пока. Теперь пойдем про MS implementation. Видите, я прям вставил эту функцию, вот такую самую. Вообще, на самом деле, я опять-таки повторюсь, пытайтесь все восстанавливать с нуля. Я здесь объясню почему. Вот такая функция MSE, вот моя собственная имплементация этой функции, используя пакет Nupai. Читает рино-каталичную ошибку, между листами предсказывает значение min. ytrue-ybred, вот здесь их квадрат, npymin, зацередливает, суммирывает и поделит на количество. Есть уже готовый метод, который из SQLion представляется. Вот он здесь называется. Misc.iterror. Это уже вшитый в SQLion пакет. Я пропадаю, кажется, опять. Да, вы пропали как раз там, где объясняли. Да, блин, что такое. И по сути, что я здесь делал, я просто вам показал визуально, как можно, используя мануальный метод, собственно, готовый вшитый пакет и пакет-разница между ними. Разницы нет. Мы подтвердили, что мы правильно имитировали свою функцию по этой формуле. Она также работает, как та, что предоставляется от SQLion. 103.8.16 Можно спросить, вот эта костыльная функция mse равна где здесь деление на количество? Она в npymin явно отображена. То есть, видишь, npymin делает, суммирует, вот это все. Но npymin это среднее, да? Да, да. И она под капотом еще делит на количество, она знает, сколько количество в массиве, а на длине это выписано. Это не видно, но npymin, если посмотреть, это там есть реализация. Окей, здесь поэто, да? Так, давайте свет сложим в кучу, чтобы фоксивее было. Вот. Окей, перейдем теперь к логической регрессии, давайте здесь немного поинтереснее. Как я говорил ранее, это та же самая функция, что и раньше, просто теперь возбили внутрь сингбойды, то есть, вот она, да? Сингбойда наша функция. В целом, здесь тоже самое используются почти те же самые метрики, я также создал классификационный датасет, 100 сэмплов, дефичи на входе, ninformative это кажется какой-то тоже статистический параметр, я не всем помню, два класса, да? 0 и 1 класс, и количество квастеров на один класс, но это больше для квастеризации, это нам не важно. Опять же таки, рано в 32. Создал датасет x, y, так воспринчиваю его, да? Так вот, создаю логическую регрессию, готовую, обучаю, делаю предикты по ней, ну и записываю accuracy, это наши метрики, да? Мы еще про них поговорим в следующей неделе, это целый отдельный блок, и module evaluation будет. Но в целом, вы уже одержите какой-то контекст по ним, надо понимать. Accuracy это аккуратность, precision это точность, recall это полонта, f1 это fмера, в частном случае fpet. Ну, почитайте, эта штука очень важна. Здесь просто плотинг, здесь ничего такого важного, просто как это все заполнится. Погнали, давайте запечатаем. Смотрите. Что теперь мы пленим? То, что вы видите сейчас, это по сути такой интересный, хитрый плот наших данных. Сейчас, секундочку. По сути, видите, синие точки это какой-то класс A, да? И красточка это класс B, да? Они напечатаны, да? Заполнены на двух их осях, то есть это их ось фич 2 и ось фич 1, да? То есть это мы ничего так не представляли. Это как они лежат, в слове говоря, в своей природе. Не знаю, там, больные там диабетом, не больные диабетом, да, в слове говоря. Ну, такие параметры. Там, вес тела, и там, вес тела, там, и там, давление там, артериальное давление в крови, человек, допустим, например. И вот такое разделение, да, есть? И, видите, когда мы обучили нашу логичническую регрессию, да, мы опять-таки смогли какую-то выучить разделяющую линейную линию, да, в виде того, что это 2D фичи, которые умеют, умеют умело разделить два класса. То есть, понимаете, вот эта вот вот эта вот линия, да? Я покрасил потом красными для красоты, чтобы, ну, понятно, что, как бы, чуть-чуть мотоциклится. Но вот эта линия выстроилась вот этим моделем. То есть, видите, мы нашли какую-то закономерность данных, таким образом, что есть существует какая-то линия, которая умеет линейно разделить два класса так, чтобы они были разделимы линейно. Вот. Ну, посмотрите, в этом вся суть. Поэтому она называется, ну, логичной регрессией. Потому что здесь опять-таки линии существуют. Вот. Какие-то вопросы здесь есть? Я сейчас подробно объясню. У меня вопрос, акурация может быть 1 и 0? Да, да, да. Ну, в реальной мире, конечно, такое очень редко добывает, но в этом случае может быть. 100% это поддание. Хорошо. Вот. Про это почитайте, почитайте, пожалуйста, как аккордеи, как считается формула, как считается пресижен, как считается рекол, как считается f1 score. Это очень важно при построении моделей. Но об этом пока потом. Есть еще Rock-Auth-Curse, да? Да. Receive Operation Characteristic, Array and Deservers, Rock-Auth. Да, но она более продвинутая метрика. О ней тоже поговорим попозже. Это очень крутая метрика. Окей, давайте дальше по этому примеру. Теперь, смотрите, я хотел бы отдельно поговорить про линейный разделитель. Про этот линейный разделитель. Что это такое вообще? Да, сейчас вернемся к нему. Перед этим я хотел бы еще помнить, и про это сегмонию мы говорили, да? Как функция сегмония. То есть, как, условно говоря, модель видит наши входные данные, вот эту линейную комбинацию. Вот эту сейчас покажу. Вот эту комбинацию, да, линейную. Видите, вот здесь B0. Сейчас. B0 плюс B1, X1 плюс B2, X2, да? Давайте сейчас про это поговорим. Как, условно говоря, модель видит вот эту линейную комбинацию. Перед тем, как ее засунуть в оксид-мойду. Здесь понятно, да? Что мы хотим, что мы будем делать сейчас? Смотрите. По сути, вот этот график. То есть, на оси X это наше значение у той линейной комбинации. B0 плюс B1, X1 плюс B2, X2. Ну, и видите, да, она раскидывает, а мы смогли раскидать эти точки по тем или иным уровням. То есть, допустим, вот эта точка, да? Ну, или вот эта точка, да? Вот эта линейная комбинация была там 4 с копейками, да? Потом мы засунули ее в сегмойную функцию, да, вот эта вот голубая линия, это просто такой дополнительный плот. Как еще сегмойная функция выбьет? А вот один вопрос, быстрый. Вот мы же до этого вот разбирали вот этот пример B0 плюс B1, X1 плюс B2, X2. Он же как-то и даже выглядит, а здесь он выглядит линией... Да, потому что я так его построил. Ну, в смысле... Ну, вообще, это 3D, да, здесь они просто находятся почти на одной плоскости, получается, все эти точки. Нет, ну, смотрите, там он 3D был, потому что вы про это говорите? Про сейчас секунду. Да, ну, вот этот предыдущий пример, 2D регрессия, да. Вот это? Да, ну, как бы точки это две, то есть по точкам их 2D разгрессия. Но видите, потому что я еще здесь вывел ну, как бы, тарги для значения, она стала 3D. На самом деле, это две также плоская точка, ну, 2D, плоское 2D пространство, по идее. Ну, там это тоже, по идее, плоскость... Да, да, да, 2D, 2D. Просто я ее, видите, сжал в одну, то есть я, видите, вот это горюжа, вот это, ось, это уже сумма. То есть вот эта сумма, все комбинации сжатые. И, видите, вот эта точка, она лежит на 4.3, все ее переножения на b0, b1, b2, x1, x2, суммирование, вы там 4. с лишним. Потом, мы же помните, по формуле засовываем ее вот сюда, то есть вот это значение в эту формулу, да, и выходит новое для значения кое-что. Так вот, вот это новое значение, оно где-то вот на этой голубой линии, потому что есть сегвойная функция. Ну, и, видите, она, как бы, в виде того, что это значение по спецификации, мы должны выводить значение либо 0, либо 1, то есть мы не можем значение вывести, как бы, там мужчина, женщина, мы не можем сказать там мужчина на 97%, ну, мы должны сказать, мужчина – все, или женщина. То есть самое, здесь какой-то порог отсечения существует. Обычно это 0,5. То есть, допустим, если сегвойную засунули в функцию, там вышла вывод из сегвойной вышла 0, 0.95, так как это вышло в 5 порога, мы автоматически это ставим, превращаем в одёргку, ну, позитивный класс. Поэтому, видите, вот здесь вот эти 6 точки, они, как бы, в виде того, что они были за порогом, вот этим порогом, они все автоматом в одёргке превратились. В соответствии, другие точки красные, которые в виде того, что их комбинация долинейная была вот в этом диапазоне, до отсечения порога, отсечения нуля, в данном случае, их значения были вот эти маленькие значения. А, видите, они по оси, они ниже 0,5. Соответственно, мы их обратили сразу в 0. То есть вот эти классы стали нулями, а эти одёргки стали. Здесь понятно? Или не совсем? Понятно, да. Окей. Теперь, ещё идём глубже, там, где мы покопаем, ещё глубже. Окей, мы набрали секбойды, мы поняли, что точки разделяться должны были с собой. Также, смотрите, а как, собственно, построить вот эту линию, разделяющую? Понимаете, да? Ну, как физически её построить? Вот ответ. По сути, это же тоже когда линейная комбинация формулы, какая-то есть. Её было всё ещё ворочу строить. Вот. По сути, вот эта точка, та линия, которая выше показана была, это вот эта линия, если мы на нуле отсечём ровную линию, да, это она и есть, это же и есть эта точка. То есть, если на этом нуле просто вертикально выведу линию, которая разделит эти точки, это и она и есть. Просто в другом, в другой плоскости много лежит. И, по сути, вот, смотрите, вот эта формула. У нас есть коэффициенты модели, вот модел коэффициенты, 1-1.6, 2.95. Также есть интерсепты, один интерсепт, 1.88. Это уже обученный модель. Вот наша logit, функция logit. Это то, что в секунду заходит, помните, да? Как я говорил ранее, она, мы должны найти разделяющую линию на нуле. На нуле это у этого графика по C logits. Мы приравниваем нулю, так мы хотим найти вот эту линию. И решаем уравнение. То есть дальше ноль приравняли, и преобразованы, ну, обычная школьная алгебра преобразована новое уравнение, да? 9х2 равно там минус b0 делить на b2, минус b1 делить на b2, х1. То есть в таком виде теперь у нас есть уравнение, которое надо решить. Здесь понятно? Да, да. Ну теперь давайте подставим, вот b0 это modresf0, b1 это modresf0, ну, ну, как бы да, вот 0a index, 0a int, расставляем. Теперь давайте решим, да? Вот наш x1, да, и мы не порталдэйцы. Да, вот x1 значение наше, тестовый, да? А вот теперь давайте посчитаем значение x2. Вот наша формула, которая у нас здесь. Засудим ее сюда и посмотрим. И по сути, вот наши выводы x2, да? Это наша разделяющая линия. То есть по сути теперь у нас есть вот эта массив данных, на основе чего мы можем теперь построить эту разделяющую линию. Видите, мы почитали x2 с учетом наших коэффициентов, с учетом наших x-ов, и теперь вот эти значения x2, да? Они по сути должны полностью равняться вот этим значениям на переченье линии. А куда вы взяли значение b0? Вот первое b1, 0, 0, а b2, 0, 1 или 1, 0? Это я не понимаю. Сверху написано. Вот это откуда взяли? Это рассмотреть идет. Видите, вот обучаем параметры нашей модели, да? Модулка коэффициент. Это есть наша бета, вот здесь сохраняется. Это большой многомерный массив. Ну, не большой, но многомерный массив. А этот модуль интерсепта, это наша b0, наш bias. Да, да, да. Соответственно, модул коэффициент это вот этот массив возвращает, но так как b1 получается вот этот терм, b2 этот терм, я обращаюсь к нулевому индексу по первой оси, их нулевому по этой оси, да? Ну, то есть 0, 0. Дальше b2 это тоже вот этот массив большой, и первый даниндекс, вот этот индекс. Ну, вот b0, 1. Ну и так далее там для 0, да? Типа сделал, просто 1. Я понял, все, интерсепт 1 будет. Да, и далее все, посчитаю логиты, и вымели получается, теперь что, да? Вот эти x2, да? Это и есть теперь наши значения на x2 для этой линии. Все, понял. То есть, допустим, возьмете вот эти x, эти значения, да? Это же наши вот эти значения, такие-то вот, ну, какие-то точки. Вот точка, да? Ну, вот точка, вот точка, вот точка. И проведем их к этой линии, да? И там, где будет перечень этой линии, да, касаться, да, с y, эти y ось, будет рано вот эти значения, вот эти значения. Ну, там мы это подходит или нет? Да, да, да, все можно проверить даже. Давайте проверим. Разделяйтого. Смотрите, допустим, возьмем, да? Для прикола там, не знаю. Сейчас. Как его сейчас? Подумаю, надо так. Ну, допустим, давайте, ладно. Какое бы взять, да, число, чтобы было более... Давайте вот этот, видите, вот этот, минус 0.77, да? То есть, вот этот, минус 0.77. Это по x1, да? Да. Вот, минус 0.77, это где-то... где-то вот... Вот как-то. Да, вот он, это вот x. Вот примерно значение, перечень на оси линии, это примерно... минус 1.1 или 2, да? Примерно вот. Все понял. Если посмотрим сюда, вот оно значение, вот оно минус 1.1, видите? Все понятно. По сути, и вот теперь если нам дать вот эти две точки буквально, мы теперь посмотрим, можем ли построить? Ну, потому что линию можно построить, да? Словно гарантии используют, знаете, только две точки. Ну и таким образом, мы построили, видите, зависимость, мы построили тут линию разделяющую. Вот, но опять-таки вы должны помнить, да, что мы построили это, потому что мы отсюда шли. Мы, в принципе, наложили все наши логиты, да? То есть вот это суммарное значение на график, применили logit функцию, да? Ну, логическую функцию, то есть сигмой, в данном случае. Их разная была функция, нелинейная. И они как бы наложили, да, ну, это активационная функция наложила свою линейную функцию, ну, эта активационная функция наложила свою, да еще, паттерн сюда, видите, она их выпрямила, да? Ну, вверх ушла, покинула их. Ну, по порогу, да, типа 0,5. Отвесно, ввиду того, что сигмойда на 0 равна 0,5, мы берем 0, да? Так, вы начали слово, мы, например, берем 0, да? Да, да, да, да, да, да, сори. Да, мы взяли 0 для разделения, да, нелинейное разделение. То есть, допустим, если взять 0 и засунуть сигмой, то будет 0,5, получится. Если взять там 1, будет там 0,75, да, вот, это вот здесь 0. Ну, это как по работе, да? Да, как грубо говоря, да. Весно, видите, мы сами можем, условно говоря, диктовать вот этот порог от течения. Ну, по дефолту его берется на 0, ну, на 0,5. Потому что шансы дать и этим группе хорошо, и этим группе хорошо дать шансы. Бывает более строгие правила, где мы хотим задрать порог до 0,8, допустим. То есть, если 0,8 порог, да, это будет более строгий порог, то есть, отверками вашей задачи будут только очень самые уверенные классы. То есть, когда модуля уверена на 8-7%, то когда да, а если она уверена на 0,75%, это тоже выше 0,5, но это незначенно уверенно. Поэтому мы будем отрезать вниз, да. То есть, допустим, 0,8 это сколько? Это где-то вот один с копейками, да? Тогда от течения где-то будет здесь. Это будет более строгая модель, но, скорее всего, она будет более точная. Потому что, чтобы классом стать отверкой, сигнал должен быть гораздо больше, чем 0,5. Вот. В данном случае я сделал 0,0, потому что, ну, это практика стандартной классики, практика на 0,5. И вот после того, как я на 0 взял, я создал это уравнение, вот это уравнение, вот это преозавал, да, и дальше я вывел уже, смог коэффициенты вывести, и дальше я, по сути, могу построить вот эту ирульту. Просто вы должны понимать, что все вещи внутри этих формул они не просто грабальные. То есть, вы должны максимально все от и до понимать, что от чего зависит, как с чем работать. Вот. А, ну, окей, вот как раз мы в финиш подошли. Здесь я просто показал вам, уже показываю пример, breast counter example, рак груди, пример, да, вот этот сетка из калерты. Использую тоже, опять-таки, магетическую регрессию, когда входные данные, там, женщин, на вход их заходят, и мы предсказываем, да, когда по этим данным фичам, да, там, реген и реген, да, различные данные, да, какие-то астрактные. Мы предсказываем ноль, это типа нет рака, один, это, ну, есть рак, ну, есть, типа. Вот такой скрипт готовый. Допустим, давайте мы времени мало у нас, а теперь проинтерпретацию, если будет. Ну, это missing qualys, показывает то, что у нас античек, что у нас где-то пустые глечения, мы все их закрыли, то есть мы сделали припроцессинг данных. Далее мы построили конфюжию матрицу, я не знаю, как переводится на русском, она называется конфюжия матрицы. Это по сути матрица для бинарных партий, чтобы понимать, как хорошо наша модель работает. В целом... Можно добавлю? Да, да. Это, ну, где, сколько я читал, везде говорилось на русском матрица решений. Отлично, матрица решений, что я буду знать. Спасибо. Да. В целом, по сути, бинарная ситуация... Это матрица ошибок? Или матрица ошибок? Я не знаю. Матрица ошибок, по функциональному, и даже по переводу. Ну, да, типа TP, FP, FP. Нет, не только же ошибки показывают, она показывает и положительные результаты. А давайте я загуну, мне прям интересно. Так, Пистиан, посмотрим по этим... Матрица несоответствий. Вот. На хабли так говорят. Ну, благосвен. Ладно. Смотрите, в целом, что здесь можно понять? Я могу ошибаться, но, смотрите, вот эта матрица, она помогает нам понять. То есть, видите, мы должны всегда понимать любой классификации задачи, мы должны понимать две метрики, да? Ну, не метрики, а два понятия. То, насколько мы хорошо говорим одёрки, предсказываемые одёрки, они оказываются реально одёрками, да? То есть true positive. То есть, истинно положительная метрика. И... истинно ответственный номер. То есть true negatives. То есть, то, когда мы говорим ноль, да, на какой-то класс, и это оказывается правдой. То есть, истинно ответственные качества. То есть, модель, которая хорошо умеет делать true positive, да, rate, и true negative rate, вот хорошая модель. То есть, она говорит позитивные классы только на те классы, которые являются классами, да, позитивными. И она говорит ноль негатив только тем, кто является негативом, да, по факту. Ну, условно говоря, если в данном случае мы поставили модель, которая определяет как можно делать рог груди, да, то модель получит почти там в 99% а в 97% она... То есть, и всех тех кейсов, да, которых она сказала там по одёргам на рог груди, 97% это было правдой. Это так и было. Вот. Да, точно, смейте. Но при этом она еще, видите, она еще, помимо того, что она не только не любит кейсов, но и выбирает, кому одёрг, кому ядерку лепить, она еще имеет хороший охват. То есть, и всех, кто на самом деле в этой выборке, да, имел на самом деле рог груди, да, по-настоящему, она охватила их 99%. Это есть рекол, да, в 1999. То есть, она реально всех, кто по-настоящему имеет, она всех охватила, покрыла. Но при этом она их всех покрывала, да, она всех почти точно скрыла. То есть, поэтому они друг от друга противоречат чуть-чуть. То есть, очень сложно, когда у нас рекол и притивы одинаковые высоки. Такого никогда не бывает. Вот, я пропадаю жестко, конечно, кажется. Вот. Да, в целом, по конфюжин-матрице соответствия, можно сказать следующее, что вот здесь, в 41 стоит, здесь стоит, видите, actual 0. То есть, истинный класс, он отрицательный. А predict, соответственно, вот здесь predict он тоже 0 будет. То есть, мы, грубо говоря, у нас 41 кейсов было, когда эти кейсы были реально нулевыми, то есть, у них не было рог груди. И при этом мы их predicted, мы тоже их правильно predicted. То есть, мы дали 0 этим 41 кейсам. Вот таких 41 кейсы. Это очень хорошо. То есть, вот эта область и вот эта область, они должны быть высокими. Максимально. А вот эта и вот эта, они должны быть низкими. Ну, это типа ошибка 1-го рода, ошибка 2-го рода, да? Это правда тема, говорится. Соответственно, если сюда смотрим, это говорит, что было 70 примеров, которые действительно имели рог груди, actual 0, actual 1. И мы при этом предсказали их отвергами тоже. То есть, у нас есть попадание. Соответственно, вот эта высокая и эта высокая, это как бы круг. Вот. Соответственно, вот эта, допустим, вот эта 2-ка, это значит, что у нас было 2 кейса, которые не имели, две женщины не имели рог груди, actual 0 было, да? Но при этом мы сказали, у вас типа рог груди, адерки. Как бы это, видите, это, видите, ложно положительный кейс. То есть, мы ложно, позитивным случаем обозначили эти два примера. А вот это, это получается, в общем, это скорее всего более опасный случай. Это когда мы сказали, у вас всё нормально, но при этом по факту это чёрт болел. Видите, как бы, с точки зрения этики, скорее всего, вот эти два параметра должны варивляться от этой домейной области. Потому что, наверное, не так опасно, да, кому-то посвятить лекарство, которое, в принципе, он выпит и что-нибудь с ним. Но куда опасней человеку не дать эти лекарства, хотя он их нуждался. То есть, здесь такой момент существует. Но в целом мы хотим вот это максимально, а это минимально. Вот, ну это опять-таки про бинарную классификацию. Когда у нас больше классов, то эта логика чуть перестраивается. Ну, в целом, теперь давайте поговорим про топ мозфичей. Уже чуть вышли, но всё, закончили. Вот это как последний. Смотрите, есть, здесь, я здесь ещё вывел анализ топ десяти самых важных фичей, важных признаков. То есть, видите, видимо, у нас данные такого рода, смотрите, данные вот такие. Вот, у нас, видите, extreme shape, у нас 30 фичей. Если посмотрю сюда, у нас какие-то 30 фичей, я их не знаю, они различные слегка, ну, не понятно, ну, какие-то 30 признаков существует. ArrayError, WorcesterRadius, WorcesterRea, Worcetexture, RadiusError, какие-то медицинские, да, сборы данных. Соответственно, видите, ну, они же, ну, вы можете угодиться, что они не все в равной мере, да, оказывают влияние на предсказательство label. Потому что есть какие-то фичи, ну, типа, которые вообще не коррелируют с тем, что там человека, там, прогрессии есть. А есть какие-то фичи, которые напрямую коррелируются. Ну, это признаки, да, более сильные. Ну, это типа, знаете, я не знаю, типа, скорость роста волос на теле не так сильно влияет, как вообще рост человека в том, что человек станет баскетболистом, да, условно говоря. В примере, то же самое. Вот. Ну, или там, не знаю, для бокса там, как сила природы должна быть. Тестно, здесь мы видим, вы видели топ 10 фичей, которые наиболее сильным образом влияли на вес, предопределили в принятии решения того или иного класса. И мы видим, что вот как-то Worcetexture, видимо, какой-то текстуры чего-то, может быть, я не знаю, какой-то, не знаю, может быть, администический термин, он имеет наибольший сильный эффект на предыдущий лейбл. Радиусы роурта и так подобное. Соответственно, если мы там все 30 посмотрим, да, скорее всего, там последние 25-30, то они будут, скорее всего, вообще, там, минимальными, и от них, типа, ну, там, толку особо нет. То есть, если бы вы использовали, не использовали их, давайте посмотрим. Вот. Ну, вот, смотрите. Допустим, вот какие-то другие, mean smoothness, mean smoothness, какая-то гладкость, общая гладкость. Она вообще, типа, не решает. Ну, решает, но очень мало. Вот. Это важность, как вычисляется? Это хороший вопрос, Жан. Вообще, по идее, там, это целая, отдельная наука, на самом деле, какая-то череповда считается. Но в целом есть только понятие, как F-score. Он так называется F-score. Есть такие уже, есть много. Ну, опять-таки, piecing correlation coefficient, он тоже, кажется, здесь можно использовать. То есть, допустим, когда одна переменная, это будет worst texture, а вторая переменная, это будет not target label. И смотрит, как они корридируют себе сильно, несильностью. Вот. Там очень, на самом деле, много версий. Но, кажется, вот здесь, это, кажется, я, это кто использовал это? Так, это fitreported sort, это от чего? Вот, я и скорее, вот, кажется, это, я не вижу сейчас. Просто висы? Ну, да, да, в данном случае, получается, ну, как бы, да, это тоже правильно, на самом деле, да, в данном случае, это просто вис самодели. Ну, как бы, естественно, скрытая зависимость такая, что чем больше бета, да, бета 1, бета 2, бета 3, если бета 1 в магнитуде финальной модели имеет 60 по значению, а бета там 3-10, то бета 1, ну, 60, которая была, она имеет больший импакт при предупредении решения. Но это в данном случае, то есть, видите, опять-таки, в данном случае, это да, они, я взял как коэффициент именно весом. Но там, в самом деле, есть другие еще признаки, которые более, ну, куда глубже смотрят того, как обредит важность того или иного признака. Вот, вот. В целом, это, наверное, домашний компонент? Да, да. В целом, все. Мы чуть на 13 минут опять вышли, и графика. Какие-то вопросы здесь есть, давайте уже все закроем, да? Вы это все откроете, да? Я все открою. Домашка тоже будет объявлена, да? Так. Да. Все хорошо. Все. Какие-то вопросы? Можно вопрос? Я бы хотел спросить про скейлинг данных. То есть, когда мы тренируем нашу logistic regression model, обязательно ли нам использовать какая-то нормализация или централизация? Или без этого мы можем как-то? Так, это I can, да, Грит? I can? Да. Окей. Спасибо за вопрос. В целом, вы всегда можете и без нормализации, да, и без там, какого-то предобработки работать. Всегда можно сделать. Но зачастую это всегда будет хуже. Что вы считаете, что в честь вы представляете перфоманс модели? Зачастую. Еще раз. По предобработке, вы что имеете в виду? По очистке данных и энкодинг? Да, да, да. В общем, черт так. Кодировка, да, категориальных признаков, ванхот, энкодинг, я не знаю, удаление пустого выбросов, усредение пустых значений, да, медиана, мины, да, ставить какие-то значения там личные. Ну, там масса предобработок, конечно. Или там нормализация, да, по контр-разпределению, да, гауссовому, негауссовому. То есть, эти в основном, вот эти шаги в скалировании, нормализации, стандатизации данных, они имеют нереально огромное количество. То есть, на мой опыт, я работал со многими разными данными в различных моментах, и зачастую, вот реально то и после, там, чуть-чуть применюсь, я из какой-то скейтинг поставил данных в оси своей. Там выхлоп вообще страшный, там сразу модель нормально работает, сразу все красиво работает, красиво градиент спускается, красиво там точно срастет, эррор падает, и модель получается еще быстрее. Там, в самом деле, очень много об этом уже говорить, но в целом, как бы я сказал, что это первое, это математика корректно работает. То есть, в принципе, я говорю ранее, что при условиях, когда работает нормальное распределение, математика корректно работает больше. То есть, вся позади вся эта подоплевка математики, она тоже начнет корректно работать. Потому что все эти алгоритмы, они, опять-таки, возвращаясь опять к максимуму платики и estimation подход, они все под собой подразумевают, что все данные, все ошибки данных внутри, они распределены нормальной распределением. И второе, это это вычислительные способности. То есть, зачастую компьютерам гораздо легче работать с данными, которые работают, которые стандартизированы, какой-то обладной плоскостью. Допустим, если у вас доска есть, да? О, рисовать могу, да? Видно, да, что там? Смотрите. Допустим, у нас есть две фичи, да? x1, x2, x1, да? Предположим, да, что а, ну, это давайте это будет 3D, то есть, это будет 3D, еще здесь вот такая верх ось, мой экран, это будет какой-то уровень ошибки, да? И скажем, у меня есть вот такой тип данных, да? Это мой контур, да, какой-то? Ну, как, знаешь, как карьер, да, сверху, вид сверху карьер, спускается вниз, да, туда. Типа так, так, вот так, да? Типа, вот это выше, да? Это больше, это ниже, да? Это еще ниже, и так далее, да, глубину уходит. Ну, какой-то карьер, контур. Видите, ввиду того, что x1, да, ну, видите, да, если вы точку взять, вот эту точку, да, здесь, здесь, здесь. Видим, да, что x1, да, варьируется, да, от оси там, давайте здесь, наверное, 100, да? скейл, ну, сотня, да, варьируется, да, вот так. А x1, а x2 варьируется там, в шкар до 10, да? Ну, вот он маленький, да, типа прыгает. Ответственно, видите, у нас, условно говоря, фичи торра, да, один в режиме в оси 100 прыгает туда-сюда, да, а другой в оси 10 прыгает в магнитуре. Ответственно, когда мы будем там оптимизацию какую-то проводить, агродиенту считать, наши там, ну, мы все-таки начинаем, мы спускаемся, да, вниз, типа, да, не вируя ошибку. Вот так вот, вот так вот, спускаемся по разному, да, шагами, агродиенту спускаем. Ответственно, вы, шаги, да, для агродиента при высчитывании, да, частной производной, да, от функции потери к отсиде на каждой переменной, они будут неравномерны. То есть, вот этот спуск агродиента к минимальной точке, он будет неравномерно двигаться по x и по y, потому что здесь у него скейн 100, да, а здесь по 10. То есть, вот этот изменение будет такой, ну, кривонить, потому что по x ему так надо бегать, ну, а по y ему так надо бегать. Ответственно, более лучшая картина, да, это вот эта картина была. О, классно рисовать можно здесь, да? Вот, с каром теперь у нас данные от скейн, да, от 0 до 1, от 1 до 1, да, от минус 1 до 1, скажем вот так. Не, от 0 до 2 все едуть. Вот здесь одерка. И здесь тоже одерка. И с каром у нас такой круг, да, будет. Ну, подыграйте, что это круг. Вот видите, теперь у нас вот это ось, да, значение. И другая его значение, она размере, как его, от 0 до 2 сканирована, да, то есть видите, здесь все фичи от 1 до 2, здесь тоже от 1 до 2. Ответственно, если мы будем спускаться, оптимизировать нашу задачу, вниз, да, вот сюда, то у нас градиент, да, и по y и по x он будет двигаться одинаково, потому что у него вот скейлинг, да, выровняем. А вот, и соответственно он быстрее будет считаться, и лучше сойдется. То есть здесь ему потребуется, да, справа, потребуется условно говоря, 50, да, шагов градиентного спуска, это дохера, а здесь ему потребуется 3. Соответственно, здесь это считалось бы там 1 час, условно, здесь считается это 10 минут. И вот этот оптимум, скорее всего, будет еще даже лучше, чем этот оптимум. Я ответил на вопрос? Да, да. Спасибо. Можно вопросик? Только что мы по предобработке данных говорили, уточнили про относится ли переменная, ну или сейчас к категорийным переменным. Вот, это надо как-то явно указывать в моделях или поэтому под капотом понимаешь, что это, ну раз это стринг, то, наверное, это категория. Ну, к сожалению, нет. Короче, да, отличный вопрос, Павелик. Классно. Хорошо задал. Да, вообще, обработка данных, когда речь заходит про категориальные признаки, там мужчина, женщина, пол, я не знаю, стринги такие-то, и какие-то численности, 35-45, там вообще интересно наука происходит. То есть работа с категориальным параметром, да, это отдельное искусство, там есть различные кодировки, да, как вообще с ними работать, ну, под капотом он не понимает. То есть тоже явно указывает, у тебя есть какие-то реальные признаки, там красный, синий, желтый, ты должен как-то закодировать, чтобы машина поняла. Ну, соответственно, One-hot-кодик, да, просто лейбл-энкодик, да, там вот тут тоже какие-то численные эквиваленты этих категорий. Так она не понимает. В этом ноутбуке у нас этого нету, правильно? Здесь категоральных, так, нет, здесь категорических нету. Я могу добавить, я думаю, я думаю, добавлю. Ну, смотри, я все добавлю, я все забыл, вот CatBoost, наверное, CatBoost, это авианосский фрейвор для работки гранитного спуска, гранитного бустника, я подесирю. Он вроде бы как под капотом, криво косо, ну, категориальными перебедами работа имеет сам, под аффолт, вроде думаешь там. Ну, это не проблема, просто надо здесь быть внимательным. А можно один вопрос? На кинезиры. Один вопрос, вот вы тут прямо в коде уходите CVScores, CrosswellScore, Model, ExtremeScale, ExtremeScale CV5. Можете помнить, что значит CV5? Вот там кроссполидатчн. Вот это, да, вот это. Да, CV5, что значит CV5. Да, сорри, я не объяснил. Смотрите, CV значит CrossPolydation. То есть перекросная проверка. Об этом мы поговорим еще в детали, это вообще крутая штука, которую модуле малышем мы затронем. Ну, то есть, в данном месте, понимать, CrossPolydation это получается перекросная проверка с 5 фоллерами. Что значит? Это значит то, что мы взяли вот наш Xtrain, да, датасет, создали 5 различных копий датасета для обучения, где в каждом из этих 5 копий трейн-тест сплит разный. Понятно. Мы создали 5 разных версий для обучения, в каждой из этих версий есть разные соотношения, и перемешаны версии train и test-split. И сделали так 5 раз. То есть перекросную сделали, чтобы... Потому что, видите, вы можете, если один раз сделать, допустим, да, вот как выше сделал, один раз, да, на трейн поделить и на тест поделить, да, датасет. Как бы это в целый работа будет, но вдруг, какой-то случай, если просто так отобралась фичей, ну, или сэмплы в таком-то странном виде, что в трейн будет лучше в тесте похи. А когда мы сделаем это 5 раз, конечно, шанс такой ошибки, и средим это, да, ну, у среди обучения 5 этих разных моделей, то шанс ошибки будет меньше, и она будет более, ну, близко к реальности. Но опять-таки, это закон предотвратим. Чем больше... Ну, не закон предотвратим, а закон больших чисел, да. То есть чем больше итераций мы какие-то соблюдаем, чем больше, тем лучше стремиться к исцепозвучению. Угу, все, понятно. Спасибо.