색 defining impact друзья, начнем потихоньку. Уже время 10 на 6. У меня хорошо слышно? Да, нормально. Да, слышно. Все отлично. Экран второе видно, да? Да, видно. В целом, мы тогда говорили, давайте про BC еще чуть-чуть объясним. Вот вообще BC, как говорю ранее, это такая крутая техника, которая используется не только в мэйбэле, но и там в различных длинных других дисциплинах для того, чтобы ну, чтобы понять размер здесь в первую очередь и второе это получается создать новые такие-то признаки, сгенерировать. В целом, да, алгоритм такой, что перед тем, как PC наложить на вот этот десет, это должно, десет должен быть стандартизирован, да, то есть среднее должно быть равна нулю в этой популяции данных и отклонение, да, стандартное должно равно 1 быть. Это такое самшен, да, первичное, которое надо обязательно сделать перед тем наложить. Далее, да, мы считаем матрицу к вариации, да, декарные фичи, три фичи. Далее, на основе вот этой квадратной матрицы, да, 1n, мы должны посчитать собственные вектора собственное значение, ну или айген вектора, айген валис. Вот в коротце, да, если, ну, в самом деле, как я говорю ранее, айген вектора и айген валис это вообще отдельная тема, на самом деле, которую надо прям пару там десятков часов выделить на это, потому что это, ну, фундаментальные концепты, а не алгебры, вообще, отдельные. Но в целом это, можно сказать, следующее про них, как что бы вам просто какое-то общее понимание было, что это такое, кто не знает. Или, давайте так, кто знает, что такое айген валис? В плане, не то чтобы слышал, а прям знает, что это такое. Математически, знает, как ее посчитать, если что, нужно. Есть такие? Ну, могу единственное сказать, что через неделю будут проходить это на гейна алгебры. Ну, то есть, хода никто не слышал особо. В целом, давайте скажем так. Обычно он интерпретирует ее, ну, я слышал одну интерпретацию очень-очень интересную, которая мне очень нравится, она более-менее понимается. То есть, смотрите, у нас, допустим, есть какая-то, ну, представьте, у нас есть огромная матрица, размерности, ну, она 2D-матрица, и размерности, не знаю, 10 миллионов на 5 миллионов. Я нарисую, давайте. Смотрите, по сути, представьте, у нас есть такая гигантская матрица. Не знаю, ну, давайте, давайте, там, 5 миллионов. Для того, чтобы просто примерно понимать, что это такое, что случится здесь, да, скажем, давайте это клиенты, да, вот здесь это клиенты лежат, база клиентов, да, то есть, там, люди, прям, да, строки это люди. А колонки, да, они обозначают, получается, ну, какие-то, да, скажем, давайте товары, да, то есть, давайте скажем, что мы сейчас делаем дачу рекомендательной системы. То есть, у нас здесь матрица гигантская, да, интеракции. Допустим, вот, не знаю, скажем, этот первая строка это, скажем, мой ин, да, я здесь нахожусь, и вот это вся, основно говоря, вся строчка, да, с колонками. Это разные товары, да, и чеки товаров моих. Скажем, не знаю, там же самый маркетплейс на Каспи, да, у них база, там, из 5-и блендов товаров, например. И, окажем, вот этих товаров, у меня есть какая-то с ними интеракция, да, не только у меня, указывает человек, когда в базе. И, соответственно, новая эта интеракция, скорее, можно как-то какие-то сигналы, да, отобразить. Допустим, не знаю, это 3. 3 там на какой-нибудь там пароварку, да, какой-нибудь, 3 электроэлектрическую. 4 там на какой-нибудь Samsung смартфон. Там 0 на другие товары, да, обычно у 4. Ну, там, на другие товары, на которых у меня есть эта интеракция. Ну, не то, чтобы нет, на самом деле, когда мы сравним задачу рекомендаций, да, у нас эта матрица, да, но здесь логично догадаться, что она очень будет разреженная, да. Можете, скажите, все понимают слово, понятие разреженная матрица, что такое? А как на английском это будет разрежено? Sparse. Ну, да. Мулейда много. Когда до 99% матрицы из 0 лет стоит. Ну, есть два типа, да, матрица. Dance, да, плотный и sparse, разреженный. Естественно, оно здесь легко понять, что оно видно очевидно, что оно разреженное тоже, потому что у меня из 5 миллионов продуктов всех, которые на Каспе существуют, условно, на магазине, 4-5 дома товаров истории. Ну, я не мог весь магазин купить. И то же самое, скорее всего, повторяется для других юзеров всей базы. Вряд ли, ну, есть юзеры, у которых есть интеракция с товарами, покупки, да. Ну, даже не то, что покупки, даже, давайте, покупка это не действие, давайте сделаем даже как просто просмотр, да, на, просмотр приложений, как ивент, события. То есть, ну, окей, там, 30-40-50, ну, у кого-то там очень хорошего, такого лояльного клиента будет максимум там, ну, 500-600, да. Ну, 500-600 из 15, из 15, там, из 5 миллионов это тоже маленькое число совершенно. Вот, соответственно, мы понимаем, что все эти строки, да, они будут пустыми. Я пока еще не говорю о нули, пока пустые. Потому что, ну, мы же не знаем, на самом деле, истинное интент человека к этому товару. Просто, может быть, он его еще не видел, поэтому мы не можем сказать, да, на самом деле, но что это нолик. Может, просто он его не видел, в принципе. Вот, что следует дальше? А видите, понимаете, вообще, вот такая вот репрезентация, да, это вот называется матрица интеракция, да, она очень гигантская. И в целом, как вообще рекомендации системы работают, они работают таким образом, что они пытаются вот эту вот гигантскую матрицу, да, пытаются как-то представить в другом виде. В другом виде, что я имею ввиду, это имейте ввиду разбить ее на такие две матрицы, чтобы она была... Таким образом, чтобы она была, да, какая-то там типа... Я забыл, как написалось. Да, вот так вот, там, на видом. Там, на видом. Чтобы она была предвидением матрицы на предложение двух других матриц, более нижнего порядка. Именно приближение, да, главный фактор. То есть, смотрите, что здесь имеется ввиду. Что мы можем вот эту гигантскую матрицу, да, 10 на 5 миллионов, да, представить как перемножение двух матриц гораздо меньше размерности. Если мы подберем и обучим эти, ну, подберем эти матрицы таким образом правильно, что она, там, условно говоря, вот эта U, да, обычно эти матрица называются user-matrix, да, матрица юзеров, да, а это типа, а это матрица, ну, матрица товаров, да. То есть, в этой U лежат матрица, которая отвечает за... Здесь юзеры, да, стоят у нас клиенты, а здесь какие-то скрытые состояния, то есть, матерское пространство. То есть, обычно она бывает, там, 10 миллионов на, там, не знаю, ну, 64 или 128, не знаю. Ну, что там, ну, почти произвольное число, которое просто имеет хорошую математику степенную, да, чтобы было в 2-х какой-то степени, да. Ну, что потом, или математика билась, или имеется два рам, уделялся правильной оптимизации. То есть, видите, и это U, да, sorry, это U. Таким образом, еще есть V, да, матрица V. Это уже то же самое, только она уже отвечает за не юзеров, а за товары. Здесь у нас товары, естественно, 25 миллионов, да, у нас в нашем примере, там, на 64 тоже. Ну, или, так, 256, да, смотря, какую сложность хотите иметь. И, видите, когда мы перемножим, да, когда мы перемножим эти матрицы, вернее, не так, не это, а вот здесь это должно быть. Когда мы перемножим эти матрицы друг на друга, да, то у нас выйдет 10 миллионов, да, на 5 миллионов. Согласны? Да. Да. Ну, вот, то есть, видите, мы, по сути, можем подобрать такие две матрицы, да, они, в принципе, видите, по размерности бьются. Таким образом, чтобы вот эта штука опять вернулась. Но опять-таки, да, мы не можем это сделать полностью, то есть у нас не будет максимального приближения, то есть сходство с точностью соединенной, потому что это невозможно. Она будет очень близка к ней, да, там, не знаю, но она никогда не будет с ней рядом. Ну, в целом, обычно люди так это делают, то есть обычно, когда люди перестраивают матрицы, вот, эти интеракции, они вот аппроксимируют очень близко к оригинальной матрице. Но для того, чтобы это сделать, мы должны сделать строгие-строгие ассумпшн, да, в положении того, что пустые значения — это нулевые значения. То, что я в начале говорил, да, что, в самом деле, пустые не всегда это нули, потому что, ну, в общем, может, просто их не видеть. Ты не отобрал ассумпшн, да, но как бы в знакомом подходе это, это подход коллаборативный для фильтрации, потом, если рекомендуется, тема будет изучать. Ну, не знаю, на курсе, кажется, это получится. И в целом, когда такое с ассумпшн делаем, мы говорим, ладно, пустые значения — это нули. И после того, как мы это предполагаем, мы можем вот применить такой подход. Смотрите, я то, что объяснил, это, на самом деле, общий подход, да, это просто подход, как это делать. В самом деле, их методологии, чтобы этот подход заимплементить, их масса. Допустим, есть там свд для разложения. Это singular value decomposition, да, это, ну, singular для разложения, если не ошибаюсь. Потом есть там что-то еще, кажется. И вот PCA как раз используется под каполом, не PCA, а, вернее, Eigenvalues и Eigenvectorize используется в этом свд. Потому что, опять-таки, чтобы нам вот эти матрицы найти, опять-таки, видите, это же не просто матрицы, это, видите, матрицы, которые наибольшим образом, ну, вот эти вакцерии векторов здесь лежат, вот здесь, вот здесь, вот здесь. Это не просто какие-то производители вектора с потолка. Это такие векторы, которые могут максимально сильно, в своем условно-скромном размерности, да, как 64, могут в себе заточить такую информацию, которая может описать вот эту размерность, 5 миллионную размерность. В ответственно, видите, эти вектора тоже, да, они должны быть очень репрезентативны, они должны охватывать максимально вариативные данных, оригинальных данных. Вот, по сути, эти вектора считаются за счет Eigenvalues и Eigenvector. Там есть такая прям формула. Сейчас. Ставлю просто, я бы еще посмотреть, удобнее было. Нравится эта формула. Не всем понятно? В целом, у нас есть, да, то, что говорю, чтобы посчитать какие-то Eigen, как его, мы хотим посчитать Eigenvector, Eigenvalues для матрицы оригинальной A, да, мы должны сделать следующее. То есть, мы предполагаем, что есть какая-то матрица A, наша оригинальная, которая переножение на вектор X, да, равна какой-то тоже, какого-то, какого-то, знаете, равна так какому-то Eigenvalues, какую-то на скаляру, переножена на этот X, и они будут равны. Тем самым, ну, то есть, то есть, мы можем сказать, что, ага, если это решить уравнением, да, обычное, переводим сюда его за это или за это, за знак равного, как скаляр. Дальше накладываем еще identity матрицы, да, то есть, просто единичную матрицу, и после этого, ну, приравниваем дел. И находим такое решение для лямбды и для X, чтобы это равнение, оно работало. И после этого, после у нас, полчаса на выходе будет Eigenvalue и X, да, какой-то. Ну, по сути так. Ну, где лямбда? Лямбда это, ну, обычно, как бы, X не пишут, на самом деле пишут, если по-другому, обычно пишут еще, ну, просто пишут в вектор и лямбда, а я не знаю, это Eigenvector типа, ну, или X тоже в том случае. Ну, поэтому настоят рекомендую почитать про Eigenvalue, Eigenvector, что это все такое. Окей. Так, в целом, здесь понятно было, по кавалазе-инфильтрации, про вот эту вот матрицу, могу удалять его? Более-менее, да, понятно. Хорошо. Далее смотрите, когда мы посчитали Eigenvector, Eigenvalue, да, мы их получили, мы такие, ага, у нас теперь есть, где там, этой матрицы, какие-то Eigenvector, Eigenvalue, которые описывают еще, как бы, можете считать, обычно Eigenvalues их еще так же интерпретируют, как ось вращения матрицы, то есть это какой-то вектор матрицы, да, и, кстати, несколько бывает, то есть у нескольких матриц бывают несколько векторов, Eigenvector векторов и Eigenvalue несколько. Вот, можете считать, как будто бы, как бы, если у вас есть матрица, да, то если вы знаете о его Eigenvector, то вы знаете, как вращается ось, вокруг которой вращается матрица. Что это значит? Это значит, что если вы знаете какой-то вектор вращения этой матрицы, то вы в целом можете всегда сделать так, чтобы, условно говоря, ваша матрица, при каких-то преобразованиях геометрических, она не меняла свое положение. Допустим, если как бы кратце говорить, допустим, многие задачи, где есть работа с там, типа, знаете, 3D воссоздание какого-то объекта, да, и вращать его в 3D в этом виде, там зачастую, чтобы здесь проекции все как бы ровно наложились, правильно работали, да, как бы, ну, сохраняли свою начальную геометрию, да, чтобы они не слазали, не выезжали, да, отсюда себя, когда вы меняете угол обзора. Вот это все вроде вектора Eigenvector используется. Вот. В основном, там очень масса, посчитать, есть физики и в других их вещах, если в них используется вот эта штука. Вот. Окей, мы это нашли и скажем давайте теперь, теперь давайте так. Теперь ассортируем Eigenvector по агентозначениям. То есть, смотрите, у каждого Eigenvector есть свой Eigenvalue, да, соответственно, вектор Eigenvector, который имеет наивысший Eigenvalue, да, своей парой, считается более сильным вектором, более приоритетным вектором, потому что у него Eigenvalue выше, у него input, input в эту оппроксимацию, да, понижение гораздо выше этого вектора. Соответственно, мы ассортируем по descending order, то есть decreasing order по Eigenvalue и берем, да, choosing k Eigenvectors, SNU features. То есть, видите, мы берем, там условно будет, вы видите, мы берем топ там 20 Eigenvectors, да, или топ 10, не знаю, сколько это найдется. И только по ним берем, да, то есть, вы видите, срезали, да, какую-то размерность. Вот. И они в случае PCA, вот эти топ k, топ там, не знаю, пять Eigenvectors, это есть наши новые полученные фичи. Вот поэтому я говорил, да, что PCA является еще каким-то feature генератором, да, он признание создает тоже. Вот. Ну и в целом все, то есть, мы вывели новую матрицу, вот, теперь мы можем потом, да, вот здесь, допустим, у нас какая-то есть начальная матрица, да, X, она там, давайте, она с шейпом, не знаю, 10 тысяч на 200, потом мы, вот, это, да, потом мы отсюда получили там, не знаю, ка равна 5, и мы говорим, теперь возьми такой вот 5 факторов. Так. Так. Так. 5 факторов, получается, 10k, да, у них будет размерность, их будет 5 штук, да. А, ой, блин, что я делал? Сейчас. Они будут, нет, они будут, это, они будут, а сам, ну, не размерности 10k, они будут чуть-чуть меньше размерности, потому что они будут, ну, так как мы строим же их не отсюда, мы их строим отсюда же, от KV-RNC-матрицы, да, а KV-RNC-матрица это наша квадратная, то есть, в данном случае, KV-RNC-матрица, она будет выглядеть такая вот, да, sigma, это матрица, да, квадратная, обязательно, да, в данном случае, это будет 200 на 200. Да. А как мы будем сравнить, вот, эти айконвикторы, айконвалию? Айконвалию. У каждого айконвиктора, да, есть свой айконвалию, да, то есть, вот этот айконвалию один, ой, лямбда два, лямбда три свои. Вот. У каждого айконвиктора есть свой айконвалию, они попарно идут, и мы сортируем топ-5, да, векторов по наивысшему айконвалию, то есть, вот это айконвалию выше айкон 2 и так далее, и берем эти векторы. Вот. Смотрите, так как у нас кавалер смотрится рано 200 на 200, да, как офичан офичу у другого видоса, у нас, соответственно, размеры с айконвекторов тоже будут 200, да, потому что мы отсюда считаем их. То есть, итоговая форма, да, у нас если мы сложим, да, сконкатенируем, да, эти векторы у нас будут, получается, матрица такого вида. Матрица 200 на 5. Это будет наша какая-то там, давайте назовем ее матрица, да, трансформация, да. И все. Смотрите, теперь у нас вот теперь Transforming Original Dota Set. Это будет здесь получается, у нас есть какой-то новый уже Reduced Dota Set, да, новый, который будет равен, если мы возьмем и перемножим наш оригинальный Dota Set, да, который был заскалирован, вот этот. Перемножим его на наш Transpose матрицу. Ну и пойду, да, что вот это вот с этим перемножится, 200 сократится, будет 5. И т.е. у нас и результирующая будет эта матрица. 10 тысяч примеров на 5. То есть мы сжали наши примеры с 200 размерности до 5 размерности. Мы потеряли какую-то информацию, но эта информация в целом не такой важной. Вот. Хорошо. Ну, туда мы и доершили, там можно дальше. Вопрос здесь есть? Пока нет. По счёт. Кавариация. Ну здесь вот говорится, да, то есть в принципе я и говорил, что, ну, как вообще считается объяснение. Два доделения имитации, да, мы все тоже проговаривали, что для видовизации помогает понижать размеры, да, помогает вычисление делать быстрее. Вот. Окей, давайте теперь поговорим подробнее о, может, затронуть тему непосредственного этого топика, а это оценка модели, да. То есть вообще оценка, ну, модели мы построили, да, и теперь правильно, да, её интерпретировать и видеть её, да, эволюция заниматься, заниматься модели этой. Понимать, насколько мы реально хорошо идем, не идем и вообще не обманывали мы сами себя. Вот. В целом, да, начнём давайте со сплита, то есть обычно, да, как его? Ну, все знают, что мы должны всегда отробить при обучении модели для любой модели, простейшей даже модели или сложной модели, мы должны делить dot set на 3nt сплит. Вот. Что это значит в целом? Ну, я думаю, если интуитивно всем понятно, что мы должны делить, да, более-менее сбалансировано делить и объективно делить наш данный на обучающую выборку, да, на которую модель будет обучаться, да, фититься, обучиться закономерности, вытаски, и другой, да, остаточный dot set от оригинального, на котором она будет уже себя тестировать. И условие такое, что, да, они не пересекаются, да, эти два dot set, чтобы в тесте не было то, что она видела на train. Ну, дабы не просто вылучить это что-то. Вот. Ну, в целом, да, соотношение такое, это 78% на train, 20-30 на тест. Вот. Следовательно, да, еще обычно это рандомно берется, да, ну, обычный рандом, но бывают еще какие-то, если какие-то требуется более по задачи специфичные сэмплирования, то бывают какие-то еще хитро лунные тесты сэмплирования. Допустим, смотрите, ну, например, что я имею ввиду. Бывают задачи, да, где на самом деле вы не можете произвести такое, давай рандом-рандом, возьмем рандом, наверное, там, 100 примеров там, 20 примеров на тест и 8 на это. Бывает такое, что так нельзя делать просто, потому что там есть какая-то скрытая зависимость другая. Например, это там временные ряды, да, когда у нас задачи во временных рядах существуют. То есть, если у вас будет этот ассет каких-то покупок человека, да, или видеопоток изображений, да, там у вас картинка 1, картинка 2, картинка 3, 4-5, да, они как бы, ну, видеопоток какой-то. Обе задачи, их объединяет то, что там есть понятие, есть ось времени, да, в данных, то есть у них есть аналогический порядок. Вы не можете брать, условно, попадет из вашего видеопотока из 100 кадров или там из транзакционной активности человека за год, да, там его там карточные транзакции, допустим, какие-то, 100 записей каких-то есть. Вы не можете брать там, сувать в модель условно говоря данные, таким, что там, ну, в какой-то модели, что раньше заходят кадры или данные, которые были в будущем, а позже заходит модель там, данные, которые были, условно говоря, до будущего, да, ну, я не могу, условно говоря, скормить модель, но опять-таки, в какую модель, да, если мы там говорить про какие-то классические, там, скорее всего, такой проблем не будет, но там модели, допустим, те же самые какие-нибудь рекламеретные сети, да, или LCTM сети, да, где есть понятие овось времени, да, там Arima, да, модели тоже, там уже тщательно, да, сэмптируют этот Z. Так и ордов, чтобы у вас, условно говоря, в train был, ну, обычно, пусть в прошлом, а TES как бы в будущем. Ну, условно говоря, в train зашли там все данные до апреля 2004 года, а в TES зашли данные после апреля 2004 года, вот, и между собой они рушились, вот, где есть зависимость времени имеется это. Окей, еще немало важно, да, почернуть, смотрите, про стратификацию, да, а можно спросить, вообще, кто знает, что такое стратификация данных? Может, не только в данных, может, где-то еще слышал такую тему. На друг друга накладываются, да. Так, а кто говорит еще раз? Руслутан. Руслутан, да, а что, что, что имейте в виду не симпла, как на друг друга накладываются? Вомо я в биологии видел, когда клетки на друг друга накладываются. А как это понять, ну не симпла, честно. То есть, один лейер клеток есть стратифит, где много левла. Это сортировка как бы чего-то по группам, по тестинг-группам. Ну это, на самом деле, как бы, да, наверное, те же темы, но вообще, это типа про выравнивание, да. Смотрите, если касаемо данных, то это скорее, по-моему, чтобы соотношение, да, одинаковое было. Все верно, да. То есть, мы должны выровнять, да. Вот видите, пишут, что стратифирует sampling для поддержки распределения классов до задачи. То есть, словом говоря, смотрите, когда мы сплетим на train-тест, да, допустим, у нас задача бинарной классификации, да, существует. У вас там может быть такое, что в тесте будет тысяча нулей, да, и, скажем, 100 одерок по стильной классу, да. Вы делите на 80% на train и 20% на тест. Может быть такое, что у вас отношение, да, оригинальное отношение одерок нулей в train и в тесте не сохранится, потому что вы выбрали random. То есть, условно говоря, у вас будет такое, что может быть такое, что в train попадут всего один пример с одеркой, то есть один позитивный пример, а в тест попадут 99 одерок. Понимаете, да, соответственно, у вас в train будет во всем 80%, даже во всем большем, да, в большом датусе, но там будет всего один пример позитивный. Это очень плохо, да. Поэтому всегда обычно балансируем стратификацию, пытаемся, чтобы наше классовое споделение оригинальное самое, самое верхнее, сохранялось и в тесте, и в train. На самом деле, здесь тоже не всегда так, потому что некоторые задачи требуют более хитрых взаимодействий, более хитрых стратификаций. Такое, что, пускай, типа, на train будет, на train, пускай будут у них прям один в один количество классов, то есть и 50 классов нулей, 50 классов однерок, а на t-teste пускай будет уже как в природе, то есть как истинно сохраняется. Бывают разные такие экзотичные кейсы, короче. Также хотел сказать про cross-validation, cross-validation. По сути, я ранее объяснял, да, кажется, его, что мы делим data set, набор данных на к, к одинаковых фолдов, да, фолды, ну это типа, ну слепки, да, условно говоря, или куски. Пусть им пять, да, нас мы делим data set там на пять, как сказать, пять вариантов. Мы берем там первый ка-фолда и обучаем там, допустим, для него там, ну смотрите, берем первый вариант и его обучаем на к-1 фолдов, ну то есть, грубо говоря, мы обучаем его на четырех видов, его сплита, и на оставшееся мы тренируем. И в училии получается от первой модели, типа у нас есть модель 1, да, которая обучилась на первом распределении. Далее у нас так-так, у нас пять вариантов, это вот data set, да, мы, условно говоря, повторяем этот процесс, пять раз. И такие оверзы, мы, условно говоря, на каждой, как сказать, на каждой из этих пяти раз у нас будет получиться пять разных моделей. И соответственно, мы тем самым, что делали, мы по сути, ну, снизили шанс рандома в наших результатах, в наших performance метриках. И, как бы, ну, у нас вырастет время, потому что cross-validation обычно требует больше времени, и тем самым, зато у нас на всех пяти, скорее всего, если мы среди медизначения, то мы будем реально близки к истине этой модели. Это очень хорошая метрика, она, в принципе, здесь используется в классической мэрии. Вот. И в целом, что дает, это более устойчивая эволюция модели, потому что, ну, как бы, грубо говоря, разные кейсы пробуем всегда. И это очень эффективно, когда у нас data set очень маленький. Потому что, видите, data set, когда маленький, там сто примеров всего, ну, там разом поделить, что-то сделать, ну, ничего не дает. А когда мы много раз манипулируем тот же сам data set с пяти-шестьи радых сторон, то мы, как бы, нивелируем до эффекта рандома, что наши результаты вызваны там каким-то шумом просто. Вот. Здесь понятно, да, как фальтация работает? Да. Ну, как бы, в основном, она больше в классике мэрии используется, в каких-то уже таких более прожорливых моделях она, наверное, используется, потому что это очень долго считаться будет. А, ну и вот так. А вот какую лучше использовать, типа, в зависимости от чего как выбирать? От размерности модели получается. Какую лучше что использовать? Ну, именно, в какой именно в этом плане т-тест или вот эту вот cross-validation, когда лучше использовать, для чего, типа, в этом плане? Какую выбрать, например, если у меня будет выбор, например, что лучше использовать? Так. На самом деле, тут, наверное, я не с вами понимаю вопрос, потому что тут я не говорил про что лучше выбирать. Смотрите, cross-validation это, на самом деле, просто методика того, как вы можете обучаться и тестироваться. То есть, я ничего с чем не пока не сравнивал еще. Хорошо, ладно. То есть, в смысле, вот это не сравнится с вот этим верхним. Это вещи, ну, они не взаимосвязчивые. Или вопрос, как-то не я понял. Валя, ладно, ладно, не учи, не учи. Валя, это еще есть такое понятие, как Lee one out cross-validation. Там был ка-фолд cross-validation, а есть Lee one out. То есть, ну, типа, оставь один в ней, cross-validation. Это, в смысле, тоже под вид special case of k-fold validation, где k-fold-ов равняется количеству сэмплов. Но я, на самом деле, не сильно рекомендую этот модель, этот тип, потому что видите, что вы делаете не 5 k-fold-ов, а n-ое количество ваших сэмплов. То есть, вы на каждый пример ваших данных создаете какой-то новый сплит. Так себе, короче, идея, потому что это очень дорогостоящая, в точки зрения вычислений. У вас будет тысячи примеров, например, в датасете, где тысячи k-fold-ов. Ну, для маленьких датасетов это окей, считаю. Для больших нет. Вот, ну и для временных рядов, как я ранее говорил, то есть, нужно учитывать обязательно, всегда в голове держать то, что у нас есть temporal, да, order. То есть, временной ордер есть какой-то. Вот. А теперь поговорим про уже эти, да, про метрики эволюции. Ну, вообще, да, делят их на две, то есть, метрики для классификации, да, метрики для регрессии. В целом, я думаю, мы уже, в принципе, да, ознакомились с ними. Значит, что это такое, как бы, да. Потому что, в принципе, на предыдущих демонстрациях и вообще домашней работе уже, кажется, вы видели все это. И в ноутбуках тоже. В целом, все просто. Классификация есть, да. Обычно мы строим confusion matrix. Ну, confusion matrix, за это не стоит путать, это не метрика, да. Это просто набор данных, куда мы закидываем наши tp, fp, fn, tn. И на основе них мы устроим метрики. Обычно это accuracy, precision, recall, f beta, mera, score. Его обычно часто используется f1, score. Вот. И также еще очень важно поговорить сегодня про микро, макро и взвешенное усиление. Это очень важно. Далее давайте поговорим еще про regressor, да. TMSE, MAE, RMSE, R2, A2, adjusted R2 и min-absolute percentage error. Вот. В целом, про accuracy, да, можно сказать, что это, на самом деле, такая простейшая метрика, которая показывает, отражает нам отношение всех правильно угаданных предсказанных классов на все количество примеров. Вот. В целом, метрика хорошая, она показывает какие-то хорошие аспекты модели, но может кто-то объяснить ее недостаток? Accuracy. Давайте смелее кто-нибудь есть, кто, может быть, хочет про accuracy рассказать? Вы спрашиваете accuracy или? Да, про accuracy. Ага, вообще, что? Ну, это более правильное предсказание в общем. Все верно, спасибо, классно. А если, ну, можете объяснить, может быть, вообще, как вы считаете? Ну, accuracy, короче, есть один очень плохой момент, недостаток большой. Вот, а про это объяснить, допустим. Что она плохо может, ну, нас может кинуть при оценке? Давайте посмотрим на формулу даже. Видите, на 15-15 формула отвечает accuracy, и здесь, может быть, такое тоже понятие, по сути, на самом деле. Ну, окей, давайте объясним. Смотрите. Давайте предположим, что у меня есть задача, какая-то классификационная задача. И смотрите. И у меня, в самом-говоре, там 100 примеров вообще в тесте. И я как бы на них предсказываю, да, моя модель, обученная, да, модель, да, модель 1, какая-нибудь. Она делает таким образом. Она выплевывает positive, да, negatives. То есть, одёрки 0, да? На что она говорит, это одёрка, это 0. И вот она на positive говорит, там, не знаю, 2, да? Тут 98. Вот, смотрите, можно понять, да, отсюда уже понятно можно. Следующая. А, ну да, и получается, смотрите, и получается у нас, у нас же еще даже труда чей-нибудь. То есть, смотрите, у нас, на самом деле, в этих сотнях, житель, словно говоря, там, да, что-то так. Так, что-то просило, била цифра. Ну да, давайте у меня будет 90 негативных примеров, да? 10 позитивных примеров. Это прям true or false. То есть, у меня одёрок, 10 штук, и 0, 9 штук. Смотрите, можно здесь легко догадаться, да, что, в принципе, понять. Сейчас разберём подробнее. Что модель, в целом, если посчитаем её accuracy, да, у неё будет очень высокая accuracy. Ну, минимум достаточная, достаточная хорошая. То есть, смотрите, если посчитаем, у нас, смотрите, сколько это TP плюс ETN. В данном случае, у нас TP будет равен 1, 3 негатив, у нас будет равен 90. Ну, значит, 100, да? Смотрите, в данном случае у нас будет accuracy 91%. Ну, на самом деле, как бы это неплохо, да, 97%. Но, если мы посмотрим, да, то модель, то, в по сути, просто выплёвывает, то есть, она может вот таким образом работать, да, что она может просто выплёвывать на всё 0, да, и в целом покрывать неплохо, да, accuracy держать. То есть, допустим, если у нас дата-сет дисбалансный, да, как, допустим, в этом случае, да, видите, у нас если за 1,0, то 90, то 0, то дисбалансный, да, сильный присутствие в данных. Ну, мы просто на всё 0 говорить, и в целом accuracy будет очень высокий. Ну, потому что верху TN, да, негатив, просто мы можем, даже, грубо говоря, на всё 0 говорить, и у нас будет отличный, отличный будет предпоказатель модели. То есть, надо будет минимум там сколько? В данном случае, это будет минимум 90, просто надо было бы сказать. То есть, я могу, в основном говоря, просто здесь взять, да, в следующий раз, вообще типа positive убрать, 0 поставить, здесь просто ещё увеличить, да, сейчас под 100 поставить, и это будет как бы, а для accuracy это будет ok, тогда она будет как бы высокая, но понимаете, да, что у нас модель, по сути, просто 0 предсказывает. И высокая accuracy держится. Это и есть нехорошо, да? Здесь понятно, да? Что я имею в виду? Да, да. То есть, понимаете, вся суть, вся такая сложность accuracy в том, что она не сильно учитывает дисбаланс классов. То есть, если у вас есть какой-то присутд дисбаланс классов, то это хорошо скаордится на accuracy. Вернее, плохо. Окей. Давайте про precision recall поговорим, да? Такие диаметрики интересные. Можете кто-нибудь объяснить, да, как вообще precision recall вы понимаете? Экспертируете. Season это по-моему выявить, он выявляет что-то и сколько из них было правильно, то есть насколько четко оно выявляет из того, что она выбрала. Recall это сколько она упустила, наверное. По-причиному правильно, recall наоборот, сколько она охватила правильно. То есть, смотрите, precision это точность, recall это полнота, да, переводится. Президиан это отношение правильно угаданных примеров. То есть, из тех, на кого мы назначили позитивным классом, какая доля оказалась такой. То есть, мы говорим, там, в основном, 100 в 100. Мы моделиплимировали 100 позитивов. Из них 98 оказалось по-настоящему позитивным. Соответственно, у нас 98% precision. То есть, из того, что мы сказали, позитив, из них 98% являются по-настоящему позитивными. Это есть точность модели. Recall это немного другая, противопротива такая, идущая сторона. Они обычно по диагонали против друга идут. Recall это когда у нас есть 100 по-настоящему позитивных классов в дандах. И recall это когда, какой процент из этих 100 настоящих классов мы покрываем правильно. То есть, у нас есть 100 по-настоящему позитивных классов, а я, скажем, из этих 100, я позитивно обозначил 93. И 7 я потерял. Ну, я их на них ноль поставил. Соответственно, у меня будет 93% покрытия. То есть, я покрыл, охватил или заполнил 93% всех по-настоящему позитивных классов. Они очень... reprises recall краски здесь хорошо учитывают дисбаланс. Если дисбаланс существует, она неплохо, но это увидит. Вот, акресы чуть слепая в этом плане. Вот, еще бывает такая, как fbeta, скор. Формула справа стоит. Его называют средней гармоническим мерой. То есть, что это значит. На самом деле, как я и говорил ранее, precision recall, они между собой как бы конфронтацию устраивают, потому что, ну, когда у вас порог чувствительности, принятие решения, ну, растет или понижается, вас сильно тоже обидит задачей, precision recall падает. То есть, обычно, когда у вас высокий precision модели, у вас низкое покрытие. Ну, а если у вас, наоборот, низкий precision модели, обычно бывает такое, то у вас обычно хорошее покрытие, то есть, хороший recall. Здесь понятно, почему это так? То есть, смотрите. Обычно, не всегда такое бывает, но там такой, да, common sense такой, что если у вас precision высокий, precision, да, то у вас, получается, recall падает. И, наоборот, также, да? То есть, если у вас огромный recall, большой, 100% recall, на покрытие всего, то у вас и занижается, падает вниз precision. Ну, это, смотрите, на примере, да, что я здесь имею в виду. Вы знаете, да, вот, помните, коронавирус был четыре года назад, да? Были тестники, да, пробы для, ну, там, не знаю, небу, соскоп, с небу брали, слюны, с носа, да, по-моему, цепляли щупом и замеряли людей, болеет, не болеет, условно говоря. То есть, это позитивный класс, да, то есть, это однерко, значит он болеет, коронавирусом негативный, но это значит он не болеет, да, пинарная классификация. И, как бы, там был какой-то процент ошибки, у них у такого дозиметра, не дозиметра, у такого оценочного этого инструмента. И, смотрите, ну, представим, у нас есть, да, у нас есть, наверное, 100 людей в коминете, да? Из них, в самом деле, там, больных являются там 9 человек. Они реально единицы, да? 91 человек, они здоровые. Их нолики, да, обозначим. И, смотрите, мы понимаем, да, инт<|id|> прессижен, да? Чтобы он был 100% равен прессижен, скажем. То есть, вы должны всех, прикиньте, все, на кого одерки сказали, в общем говоря, вы должны сделать так, вы предсказали 9 человек как одерки, да? И они оказываются реально одерками, да? Это будет 100%. Ну, это на самом деле достаточно сложно с задачей, да, при таких конфигурациях прессижена. То есть 100% прессижен, значит, всех, на кого мы говорим, они по-настоящему являются единицами. Ну, то есть, это очень должна быть точность какая-то с этого замера, очень сложная, высокая точность. Соответственно, наоборот, да, когда у нас рекол высокий, мы хотим, я хочу рекол 100%, да? То это значит следующее. Ну, здесь тоже будет как бы 9, да? 9 на 9. Ну, это видите, 9 такая, что, видите, что у нас, да, это Tp по формуле, да? То, видите, fp и fn, то есть в этом случае fp. И в этом случае fn, да? Они должны быть нулями. Согласны с этим? Ну, это значит, что у нас есть таблица, которая работает с этими объектами. Согласны с этим? Да. Смотрите, понимаете, да, что fp это false positive, да? То есть, ложно положительные, да? Должны быть нулем. А у нас, смотрите, потенциально ложно положительные, да, здесь могут быть 91 человек. Согласны с этим, да? Понимаете, да, что прикиньте, алгоритм может быть таким образом работать, да, на таких конфигурациях, пресидианно, 100%. Что, ну, 91 человек и должен попасть туда. Ну, а тут вероятность высокая, да, по ошибица. Вот, соответственно, когда у нас пресидианно огромный, да, то есть у нас очень большой порог стоит, да, высокий порог отечения, да? Там типа 0,90, 0,97+, да? Все, что, все, что выше и равно, да, по скору, да? Чем 0,97, это очень, да, уверенный скор, да? Слишком высокий Confidence Bound. То они заходят, да, туда, как уверенный случай. Соответственно, вот сюда, после этого порога, приходят однерки, да? Только максимально уверенные однерки. То есть, кейсы, когда там 0,63, да, скора вышла, он больше только 5, но он меньше 0,97. Соответственно, мы его классифицируем как 0, обращаем в 0. Потому что мы достаточно уверены в этом. Соответственно, при такой конфигурации, да, когда пресидианно 100% будет, да, у нас скор тоже будет, скор, ну, скоры тоже должны быть очень высокими. То есть порог тоже должен быть задернув высоким, высоко. И в таком случае пресидианно будет реально высоким. Но смотрите, если у нас такой будет скор завышенный порог, да, то у нас же и полнота упадет сильно. То есть видите, смотрите, логично понять, что вы считаете, что это 0,97, да, логично понять, что вы считаете однерками только те, которые там выше 0,97%. Но есть масса других однерок, да, которые тоже болеют, тоже, да, в этих девяти лежат. Но они при этом, а, как его, у них при этом скор может быть 0,85, 0,90, 0,0, 0,83, да, то есть они будут лежать ниже этого порога, да. Соответственно, мы вот эти однерки, настоящие однерки, мы их потерять можем спокойно. Потому что зарядили порог. А если мы их и потеряем, то наш рекол будет, да, падать. Потому что мы не покрыли, да, эти однерки. Соответственно, наш, видите, когда пресидианно высокий 100%, да, в основном рекол у нас будет не спадать. Да, даже там скинул этот, в хэффижную матрицу. Вот. И наоборот, да, смотрите, когда у нас, обычно, когда у нас пресидианно низкий, а рекол огромный, да, мы говорим, что это обычно другой случай, это когда у нас, наоборот, это когда у нас, наоборот, скор, да, и какой-то... порог, наоборот, маленький, да, поставим. Давайте поставим порог 0.1. То есть здесь порог отщения очень низкий. То есть все, что, все, что выше 0.1, да, модели уплевываются, как скор, мы классифицируем это как однерку. Вот. Все, что ниже 0.1, да, это будет нули. Соответственно, мы понимаем, да, что большая часть примеров, так и позитивных, так и истинно положительных, истинно ответственных, да, они могут выше 0.1. Соответственно, мы понимаем, да, что большинство скоров могут даже, да, и нулей, и единиц, да, настоящих, может быть выше 0.1 идти, соответственно, мы покроем, да, гораздо большие единицы, да, больных людей мы покроем, потому что они выше, да, скорее всего, окажутся 0.1. Мы их покроем, да, у нас это более или менее, да, мы покроем. больных людей мы покроем, потому что они выше, скорее всего, окажутся 0.1. Мы их покроем хорошо, корректно. Но с этими единицами сюда попадут и ложные нули. Ну что, они тоже будут выше 0.1, скорее всего, лежать. И тем самым, видите, у нас вырастает количество однерок, то есть у нас вырастает покрытие истинных однерок. Но видите, из-за того, что у нас растет false positive rate, то есть ложно положительное растет, у нас нолики. У нас это растет вот здесь, FP растет. И наш precision падает вниз, потому что это вырастает и precision начинает вниз стремиться. Это понятно здесь? Да. Окей. Ребята, основные как? Понимаете? Да. Да, за все понятно. Угу. Окей. Пошел. А, тогда удаляю. Окей. Смотрите, еще есть такое понятие, как f-мерa. А как я говорил ранее, что precision recall, они как бы идут чуть-чуть... Мы пытаемся найти всегда компромисс между ними. То есть мы хотим и хороший recall, и хороший precision. Но такое редко бывает. Тестно, есть такая другая мера, которая может понять, а как из этих двух мер выбрать такая лучше. Вот эта f-beta, это про это. f-beta в себе как бы хранит информацию по precision recall. Она как бы что-то среднее между ними выискивает. Оптимальное. Вот. В целом формула такая чуть-чуть сложная, но если разберете эту формулу, она окажется вам понятной. Достаточно. Вот. Beta, параметр, это такой параметр гармоники, если не ошибаюсь, называется. Он регулируется, получается, если beta 1, это f1, да, мера. Ну она проще становится. Вот. Вообще beta регулируется. Вот, я сейчас объясню каким образом. Смотрите. Как бы сказать? Ну то есть параметр beta, в данном случае, он регулируется, он регулирует, в рее, ну, куда больше веса давать. Ну веса имеется в виду как бы, что больше важнее для вас. Precision или recall. Вот если смотрите, если у нас beta меньше, чем 1, 0.0, 0.5, 0.6, вы в этом уравнении, то мы ставим акцент больше на precision. То есть precision становится более важным для нас. Если, опять таки, если beta больше, чем 1, то мы делаем recall более важным для нас. Соответственно, если это 1,0, да, как f1, мера говорит, то мы делаем абсолютно optimum между двумя метриками. То есть для нас важное одинаково мере. И precision и recall. Чему я так говорю? Потому что, смотрите, бывают задачи, очень много задач, когда вам нужно понимать, что для вас важнее. Ну смотрите, помните, я, наверное, на лекции пятой, и в самом первом говорил про темы как... Сейчас, про... Помните, болезнь какую-то. Ну да, медицина, допустим. Иногда, смотрите, да, ну, там... Ну, окей, давайте вот скажем про... Куда болезнь, да? И там классификация болеть не болеет, там человек смотрит, осматривает. И в целом, наверное, ну, я не меди, конечно, но на мощь я могу ошибаться, но я так думаю. Что в целом, если человек, допустим, мы на него прооценили его, он там замеры дал, и мы смотрим на него, он там однеркый вышел. Но это ошибка была. Ну, типа, мы посчитали его больным, но на самом деле он не болел. И назначили ему врач какие-то таблетки выпить. Как бы, да, ложное срабатывание, ложное положительное кейс было. В целом, это, наверное, не страшно. Если человек выпил какие-то дополнительные антибиотики, да, он как бы чуть себя отравил, чуть-чуть. Но в целом ничего страшного случилось. Через неделю-две он вернулся, и все нормально. Потому что он не болел. А бывают случаи, когда, наоборот, да, прикиньте, человек был болел, а ему сказали, нет, ты здоров, ноль тебе. Это очень плохо, да, я считаю? Потому что, ну, человек ушел, для здоров, а ему же было лечение назначить. И видите, вот здесь, получается, работала ложное отрицательное кейс. То есть мы вместо однерки дали ноль. И в данном случае, да, видите, оно критично было, да, и там человек словно заболел, сожнение, да, там не дай бог умер, допустим. Тем самым мы можем понять, что в этой задаче, в этой задаче куда важнее рекол, да, иметь выше. Охватить настоящих одерок, нежели быть очень точным. То есть, в данном случае, видите, порог, который я говорил, Threshold, отделяющий Threshold, можно занизить. 0.3, 0.4 поставить. Вот. Ну, дабы, да, как бы, ну, покрытой одерки. Все. Вот, соответственно, как бы, в этом случае, да, мы можем сказать, что давайте бета, скор, бета мера, давайте пускай она будет 1.5. Если бета 1.5, значит мы больше даем веса, принятия решения для f-меры на рекол. Вот. Надеюсь, понятно. Сейчас буквально, я сейчас буквально на минуточку. Ага, мне надо сейчас перепроключить микрофон, что-то садится он. Ага. Ага. Ага. Учитель, да. Учитель, да. Так, смотрите. Теперь. Так, давайте перейдем к микро-макро, да, это очень, мне сейчас посерьезнее, положнее надо внимание отсвить. Всем понятно, да, до этого времени? Да. Окей, смотрите, теперь. Сорри, один вопрос только. Да, вопрос, конечно. Вот получается, f-beta очень похожа на Precision, потому что нет false negatives. Или оно все же? Хотя ладно, не важно. Нет, она, да, ну по формуле, типа да, сверху там TP, снизу FP, да, но это не значит, что она похожа на нее, на самом деле. Ну просто можно FP взять за скобки, и тогда получится 1 плюс бета квадрат в скобках, и это все исключится, и просто станет Precision. Нет, там она выйдет же, 1 плюс бета квадрат, она там не сократится. Тут вроде формуля ошибка, тут вместо одного FP должен быть FP. Да, скорее всего. Да, скорее всего. Так, сейчас я посмотрю. Так, 1 плюс бета квадрат TP. Beta fm. Да, кстати, у меня здесь бета лишняя почему-то. Странно, скрин вроде брал в нормальное место. Окей, я посмотрю, да, возможно, хорошее замечание. Потому что, да, кажется, я смотрю сюда, из бета кажется, из квадрата должна быть еще лишний квадрат, и fm, да. Хорошо, посмотрим. Подправлю. Окей, так сообщение в чате было, че-то. Да, значение бета подбирается. Ну, не то, что подбирается, не так, как бы, есть baseline, да, это bt1. Просто обычно, когда вы ничего не знаете, вы просто ставите bt1. Вот, далее, как бы, ну, посмотрите на нее, вы можете уже понять, да, и с вашей точки зрения, вашего бизнеса вы должны тоже понимать, а на что мне больше аптен давать. Вот. Ну, в общем-то, че-то, я не знаю, как вы думаете, что это будет? Ну, это так, конечно, да, и бета почти всегда, почти всегда. Но вы должны понимать, что это, ну, не всегда так, потому что вам нужно еще, ну, знать, да, где, где че-то применять. Окей. Вот, смотрите, теперь давайте, смотрите, все, что мы так что говорили с вами, вот, при сильной рекулакции, да, Confirmed Matrix, это кейс про винарную классификацию. Когда у нас, словно говорят, то когда либо 0, либо 1, да. А что будет здесь, когда у нас будет 5 классов, допустим, да, что-то, что сделать, в этом случае? То есть у нас будет 0, 1, 2, 3, 4. Как рекул начнет, как при сильной, допустим. Вот, соответственно, мы будем сейчас говорить про Micro, Mac, взвешенное, среднее. Это краски про мультикласс классификации. Ну, резерв. Есть Micro усреднение, да, называют его. Оно считается как бы для всех, для Micro Average, для Precision, да, для рекула, там для F1-меры. И то же самое для Macra из таких, да, 3, и для Weighted из 3. Вот, начнем с Micro. Смотрите, что, как вообще считается Micro? Micro, получается, считает все метрики, да, и все метрики, да, и все метрики, да, и все метрики, да, и все метрики. И считает все метрики, да, независимо для каждого класса. И потом берет среднее. Допустим, как сказать... Ну, окей, давайте сейчас по правилам пробежимся, потом уже зайдем этот, ну, уже с примером зайдем на каждый кейс. Вот. И смотри, дальше что идет. А здесь говорит, что Micro усреднение, да, оно как бы относится ко всем классам, да, ко всем классам, которые у нас есть, допустим, 5 классов, одинаково. Независимо от его support. Support получается от его количества, то есть, количества инстанций до сэмпла этого класса. То есть, ей на самом деле здесь пофига, потому что она на это не смотрит вообще. То есть, у вас, допустим, есть 5 классов, один класс, у него тысячу раз, да, видно в сэмплах или рекордах, а другой класс его видно в 3 раза. Ну, ей, она это не учитывает, потому что она не взвешивает их никак, потому что, ну, она говорит, что я их и колею читаю. Вот. Дальше смотрите. Ну, смотрите, да, что. Она дает одинаковый вес для каждого, ну, примера каждого класса. Грубо говоря, ей, ну, то, что я сказал, все равно, да, что там тысячу или пять, она одинаково это смотрит. И, соответственно, в целом это хорошо используется, когда, да, когда, ну, вот здесь, говорится, да, want to weight the metric by the class frequency. Ну, в самом деле, здесь чучка корелое описание, что я описал странно. Не совсем так, как надо звучать, значит. В целом, здесь, смотрите, можно следующее, я бы, я бы покорректировал, чем мне нравится оно. Оно хорошо, когда у нас есть несбалансированное количество классов. То есть, смотрите, ввиду того, что, да, у нас есть, как его, несбаланс в классе, да, и так она его, она, она, она, в принципе, не смотрит на это акцент. Она, видите, будет третить лучше. Перефоланс считает на то, где у нас классы более в частном виде появляются, более frequent classes. Сейчас пример поберем. Вот, про макро, в целом, все то же самое, почти, только здесь подсчет всех метрик идет, ну, как агрегация. Да, например, чтобы быстрее понять. То есть, смотрите, в макро случае, да? Допустим, когда мы говорим про макро, мы даем также еще какой-то, какеникаж. Ну то есть, мы, мы, мы, мы, мы, мы, мы, мы, мы, мы, мы, мы, мы, мы, мышты, даже, waktu-刚, пока, полгода, � opened Мы взвешиваем каждую метрику одинаковым образом. Видите? Weight, Intense, Quality. И таким образом мы favoring performance on common classes. То есть мы таким образом как бы классы, которые чаще встречаются, то есть их больше имели, мы favor ним больше. Ну давайте я вам еще пример покажу. Мне кажется, пример голого описания не очень мне нравится. Сильнее заходит. Ну и третье, да, это получается взвешенное. Это когда у нас получается идет просто взвешивание всех метрик между собой. И то есть там, там уже как раз Support сильнее заходит, чем допустим, ну в предыдущих двух. Давайте сделаем так. Ну будто что-то закрыть описание. Так. Давайте сделаем так. Я хотел бы метрики, скажем здесь, положить. На него сейчас получают Definition хорошо. Мне кажется больше путать будет. Это Operated Function, да, у меня будет это23 won't don't not don't Don't won't , eventually will not won't don't, wome won't don't . wome won't won't . wome won't won't. wome won't won't won't.って. Не может быть. А как будет удобноûe. 펼bartу, вообще Doctor. там 2 3 класса 3 класса, да? Amount, 1 и 2 смотрите, микро в данном случае будет считаться давайте для примера возьмём давайте будем подсчет делать для только пресижения пресижение будет считаться для 3 классов этой модели и мы хотим сделать в среде для макро, микро и взвешенных. Предположим, у нас есть конфиджи матрицы ну, в таком виде, да. Давайте здесь будут у нас, получается, предикты все, то есть предикты, здесь actual, да, все. Это будет 0, 1, 2, да? 0, 1, 2 ну, то есть она будет в таком виде. Ну, не сложно догадаться, что мы хотим как бы вот эту ось, да? Вот эту ось вот эту ось максимизировать. То есть в данном случае у нас это будет, давайте, не знаю, 7, 10, 15 то есть 1, 2, 3, 4, 1, 0. То есть, смотрите, что здесь мы понимаем, что да, что когда мы говорим, да, предсказываем 7 класс, ой, 7, 0 класс и это на самом деле является true positives, то есть мы, на самом деле, эти 7 штук являются действиями однёрками, то они здесь лежат, да? В данном случае, то же самое, если когда мы предсказываем, однёрка, она является однёркой, это 10 штук. А когда предсказываем 2, она является 2 и 15 штук. То есть, на другие это когда мы не попадаем в их ось, матрицы. То есть, однёрка это когда мы предсказали был кейс, когда мы предсказали 0 класс, 1 класс, но он предсказался, он являлся по факту первым классом. В compris? Вот. Учитывайте, ну а теперь всё ещё нужно объяснить, так, свешивание... здесь смотрите давайте посчитаем precision в микровиде для вот этого класса, для 0 класса. 0 класс будет считаться таким образом, что визуально вы можете отрезать все подходы one vs all. 1 против всех. то есть 1 это будет 0 класс, все что не 1, то есть 1-2 это будет считаться как классы этого. не его классы. здесь смотрите у нас будет 7, то есть мы преобразуем для этого 7. дальше у нас вот это складывается в одно, зато в одно складывается и это тоже в одно. то есть вот эта куча, то есть сколько будет 25 плюс 3, 28 это будет наши получается true negatives, то есть истинные. то есть те случаи, 28 это те примеры, которые являются для нас true negatives. то есть мы их показали в других классах, это оказалось так. это не было 0. смотрите fp и fn. по сути fp это наш false positive, то есть это получается на то, что мы сказали позитивный класс, мы предсказали вот здесь, видите, позитивный класс, мы предсказали их вот здесь и здесь, но по факту это был другие классы, их три штуки. и fn аналогично, но это вот это сразу сложить, это будет 5. давайте теперь посчитаем следующее. ну смотрите, теперь мы преобразовали новую матрицу, мы, видите, мы по сути сжали ее до бинарной классификации, видите, помните квадрат? мы до нее сжали ее. и тем самым, когда мы это сжали, мы теперь можем наложить до предыдущей логики, которую мы еще умеем считать. то есть тп плюс fp. тогда, в случае, что это будет 7, 7 плюс fp до 3. типа 0.7 на слове это. вот. вот. теперь прийти блин убрать это я могу в сторону брать41 здесь понятно пока что все там но ребята как А вот штангу справа внизу посчитали вот это вот. Это precision, да будет? Да, это precision. Вот это вот. Окей, смотрите, теперь... Теперь... Сейчас. Я не знаю, как бы удалить бы это теперь. Окей, ладно. Очень не знаю писать. И смотрите, похожим образом мы теперь... Я удалил матрицу тоже. Ладно, вот ее оставить здесь. Окей, давайте. 5, 12, 10, 5, 0, 1, 2, 3, 4, 2. Похожим же образом мы должны теперь... То, что до этого я объяснял, это было у нас... Получается, это было у нас макро. Мы там independent считали. Мы для всех из них считали independent. То есть мы взяли перекласс, отрезали другие и посчитали так другие. Для каждого из них. Тем самым мы получали похожую операцию. И дальше мы получим все, что мы хотели. Для каждого из них. Тем самым мы получали похожую операцию. Считаем. И для других классов. Теперь берем, получается, допустим, для первого класса. И предиктуем. И смотрим, там десятка. Все, что ниже, там как в FP. В данном случае, допустим, здесь считать. Так, это будет для первого класса. У нас будет сколько? Это будет 10. True negative это будет... Тут все, что вне оси, это будет получиться 20. То есть вот это плюс это, да? И FP. Это у нас будет... Так, false positive у нас здесь. Это получиться вот это. Плюс вот это, да? Skip in. Это будет... 16. Да, 16. Сначала 16. Так, соответственно, Fn, это будет 4 плюс этот. И это будет у нас... Не, не подожди. Сейчас что-то я забудился. да что-то запутался всяка так��а 3 и 10 сюда идетyr💇 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0