 Так, ребята, всем привет! Давайте подождем две-три минуты, потом, в принципе, можно начинать. Так, экран сейчас видно, да? Да. Все хорошо. Урок я уже закинул в Google Classroom. Можете заранее его скачать. Если можно начинать. Всем добрый вечер, ребята. Сегодня у нас будет лекция, посвященная end-to-end project delivery. Сегодня я хотел бы разобрать такие темы, как в современном мире, в частности, в Казахстане, и ML-компании, как можно будет вообще строить от нуля какой-либо проект связан с ML, как это делается в CV, и в NLP чуть-чуть будет отличаться, но, в принципе, общие знаменатели есть. Так, сегодня мы поговорим, в принципе, про базовые концепты, именно end-to-end project delivery. Это имеет в виду, что, условно, у вас компания, у какого-то очень интересного бизнес-стекхолдера или у директора компании появилась какая-то супер безумная идея, как бы создать ML-проект. Точнее, как применить AI или ML или что-либо другое. Сейчас, одну минуту. Если у какого-то product manager, CPU или безразнице какого-то высшестоящего лица появляется какая-то безумная идея применить ML, то обычно нужно иметь определенный базовый компания. Концепты, чтобы как-то превратить это в реальный стоящий продукт. Дальше мы поговорим, что такое стримлит, потому что на прошлый год мы не смогли его, в принципе, как-то его пройти. Но и как раз из-за счет стримлита у нас получится некоторые базовые концепты прямо полностью пройти и по-настоящему, как бы, в целом, в целом, в целом, в целом. Посмотрим, как у стримлита, как строятся простые там приложения на нем, его чуть-чуть функционала и попробуем его с ML-ка соединить. И в конце у нас останется время на ответы вопросов. Перед тем, как начнем. Есть ли у вас вопросы относительно Docker и FastAPI? Амелхан, у меня немножко не... насчет Docker и FastAPI другой вопрос. Смотрите, при... вот этот вопрос возник, когда мы делали последний DZ-шку. Мы занимались... например, у нас есть датасет, и мы занимались чисткой этого датасета, да, там, убирали там аутлаеры, добавляли... делали новые какие-то переменные, за счет которых модель делала какие-то предикты, там, и много-много-много всего. Когда модель запускает, ну, мы же ее потом же, эту модель, как готовую уже запускаем в производство или кедровую, я даже не знаю, как это объяснить, но как эта модель будет делать там предикты на основании там другого датасета, хотя обучался он изначально на том датасете, который мы там чистили и делали новые переменные, вот. Как это все это делается? Не поняли вопросы, да? Ну, у нас имеется датасет, да? На этом датасете мы, на основании этого датасета, точнее, на этом датасете мы чистили, там, занимались чисткой, на этом датасете мы делали все, что можно было сделать, да, на этом датасете. На этом датасете мы, на основании этого датасета, точнее, на этом датасете мы чистили, там, занимались чисткой, убирали какие-то там переменные, ну, не переменные, аутлайер, то есть выбросы, да, потом делали энкодинг, да, грубо говоря, и добавляли какие-то новые переменные на основании тех переменных, которых у нас есть, чтобы лучше обучить модель. Вот, вот этот датасет он готов, и на этом датасете наша ML-модель обучилась, после чего мы там взяли и вытащили эту ML-модель уже там, я не помню, с помощью какой-то библиотеки, да, вот. Джобли, ну, условно, там, большое количество, но да. Да, и запустили ее там в производство на какой-то там сайт или еще что-то этом вроде. Но делалась, этот, эта ML-модель у нас же готовилась на уже очищенном датасете с энкодингом, со всеми делами, но тот датасет, который будет поступать, например, как бы объяснить, ну, который будет уже в работу пускаться, и в эту ML-модель, и она будет, предикто, делать, она же не будет чистая, там, заэнкодированная и так далее, и там подобное. Ну да, да, это понятно. Вы ее, вот как раз-таки во время предикто, если вы хотите, ваша задача, чтобы приходящий датасет, который условно, это называется inference-часть, когда у вас уже есть обученная модель, когда вы ее уже условно отправляли в продакшен, и когда уже идет часть предикшена, какой бы датасет, который вам не пришел, вы должны сделать с ним те же махинации, те же действия, которые вы делали с вашей там моделью, когда тренировали. Думал, что вот этот модель, который будет работать в продакшене, она будет сразу обрабатывать датасет, который будет приходить, и он сразу будет делать предиктор. Разве не так? Или просто на готовом моделе мы будем по-новому другие датасеты загружать? Смотри, у каждого датасета есть определенная цель, да? Я же не могу на датасете, который работал только в Казахстане, и отправить его в Америку, брать данные с Америки. Даже если колонки будут одинаковые, так не получится сделать, к сожалению. По этой причине, когда у вас есть какая-то определенная задача, во-первых, начинается задача. У вас есть постановки задачи, есть данные, которые к ним есть. И когда вы взяли те данные, натренировались, приготовили, условно, вашу модель, и когда вы будете идти в продакшен, чтобы получить максимальный результат, вы не сможете применить эту модель в другом месте, не связанной с этой задачей. И когда дело дойдет до предикшена, вы не можете просто в голову данные скармливать. Эти данные также должны производить определенной махинацией, и всю логику, которую вы делали во время трейна, это как-то такое, типа, энкодинги, условно, да, то, что good – это с индексом, то, что он заэнкодируется как 0, а bet он заэнкодируется как 1. Всю эту логику вы должны также сохранить, и когда дело пройдет на часть продикшена, вы также должны всю эту логику применить и прогнать через модель. И за счет этого она покажет условно какой-то более-менее адекватный результат. У меня получилось ответить на вопрос? Ты постала понятна или нет? Я просто вопрос не понимаю. Я просто вопроса не особо понял, потому что он не связан, короче, вот могу так сказать. Ну, я, может, по-другому поставлю. Смотрите, вот, например, мы берем через CASP приложение кредит, да? Да, ага. Окей, окей, хорошо. Это же, по сути же, модель, которая там, вот эта полоска, которая в процентном соотношении, это же модель, по сути, решает, МЛ модель. Давайте нам кредит? Давайте вам. Да, да. Когда, смотрите, когда любая скоринговая система, которая есть, как она работает, каждую неделю или каждый день, не знаю, зависит от банка, насколько у них данные есть, они каждый день собирают данные. Плюс-минус адекватные о людях, которые возвращали кредит, и кто-то не возвращал, и примерно процентаж от этого какой-то скоринг вытаскивали. Например, вот, типа есть огромные исторические данные, которые плюс-минус адекватные, и вы берете весь этот датасет, тренируете на нем модель. Вы можете произвести, что хотите, то и делаете, имеете в виду, что с данными, как над ними измываться, их максимум что-то удалять, что-то добавлять, как-то агрегировать новые данные, вытаскивать, все, что вы хотите, вы это сделаете. И после того, как вы натренировали модель, она проходит этап валидации. Не на примерных данных, которые должны валидироваться. И после того, как она проходит этап валидации, когда какое-то эноколитство людей, ML-инженеров или датасанитистов или тимилинов решают, что модель попадает в продакшн за счет каких-то определенных метриков, о том, что из десяти тестов она прошла на 95%, все. Это говорит о том, что модель хорошая. И вот как раз-таки эта модель, она и будет работать на продакшне. Хоть даже если она ваши данные не видела, но она тренировалась примерно на сложных ваших данных. Может быть из нас 20 человек, есть какая-то модель, которая на 19 из нас натренировалась, и условно вы 20, и на 20 она сделала предикт. И она, если вы правильно собирали данные из 19 и правильно собирали данные из этого одного, и они все лежат условно в одной какой-то плоскости, то модель даст максимально адекватный результат, потому что вы все работаете примерно в одном диапазоне или интервале, или как это будет правильно назвать. Теперь понял. Хорошо. Так, мы начнем остановились. Основной концепт. Смотрите, ребята, типа когда в любой компании, когда дело доходит до какого-то ML проекта, да, как мы говорили ранее, мы можем, типа там всегда есть определенный бизнес-творение. В вашей модели, если она может там 100% что-то предсказывать, или может как-то типа предиктить заболевания людей, но если это неприменимо и бизнесу это не надо, то ваша модель тоже, она тоже будет не нужна. По этой причине всегда, насколько бы крутую модель не использовали, насколько бы она там классные метрики не показывала, всегда первая задача, которую она должна выполнять, она должна выполнять какие-то определенные бизнес-требования. Куда вы не пошли, в какой вы компанией не работали, даже в академии среде, в бизнес-среде всегда стоит первый вопрос, и решает, какой она бизнес-проблема решает. Даже если мы возьмем какие-то социальные проекты, условно медицина, например, или, например, не, медицина не совсем, там тоже все с бизнесом связано, но социально, условно, помощь им, помощь имущимся, как-то им как-то попробовать найти жалей, или как-то улучшить их состояние, и какая-то ML-модель. И опять же, даже в тот момент это не будет бизнес, это чуть другое, но у этой задачи есть тоже свой некий success metrics. Как мы будем считать, что она хорошо справилась? И также у бизнеса, то что любая ваша модель, любая ваша инжейн, любое, типа, все, что делается в ML, дата-сайенсе, или в дата-анализе, в дата-катистике, все это приводит к тому, что, типа, если доводите это до бизнеса, то какие-то бизнес-требования решают, и как мы будем ее считать, что она правильно справилась? В принципе, всегда вопрос 90% работы всегда относится к данным. Половина, насколько вы максимально к истине вы их приблизили, имеется в виду, что вся аннотация, которая сделана, направлена, и второе, насколько вы ее правильно организовали. Как вы ее сохраните, как вы получаете к ней доступ, условно. Например, я, кажется, говорил об этом или нет, условно вазон там, в день, я просто читал на одном, в однососто<|fr|>. В день у них добавляется примерно 100 миллионов или 10 миллионов позиций. И у них ATL, условно сборщик их витрины, все вот этот базданок, он работает таким образом, все очень сильно распорядиленно, и у них правильно организованы датакаллекшн и сторочи. И среди всего этого pipeline очень важным этапом является припроцессинг данных и анализ фичей, которые мы занимались. Всегда ваши данные о чем-то говорят, это обязательно. И заниматься припроцессингом, этим нельзя чураться, и самое основное, никогда не заверить данным. Прямо никогда не заверить данным. Это прям очень важно. И третье, это data versioning. Data versioning, например, в программировании есть GitHub, GitLab, и все, что с этим связано, когда вы версионируете ваш код. Но у данных тоже есть версионирование, имеется в виду, что словно вы создали датасет, скажем так, в октябре. Прошло полгода, и вы этот датасет меняли 5-6 раз. И как раз, чтобы проверить, насколько ваша модель или ваш подход сильно изменили ситуацию, то всегда нужно понимать о том, что полгода назад у этих данных должна была быть какая-то версия, чтобы максимально можно было упростить вашу задачу поиска и сбора информации, чтобы вы могли в конце сделать максимально объективную оценку о том, что ваша модель реально лучше работает. Дальше, после того, как с данными вы поработали, закрылись эти вопросы. По-любому, к ним всегда возвращаются. Не было такого, чтобы закончили процесс с данными, а когда поработали с моделью, не вернулись. Model development здесь тоже очень играет немаловажную роль, по той причине, потому что сейчас, как бы ни было, вся банковская сфера или очень много подходов, они до сих пор основаны на классических подходах датасайнса и мэла. Не везде применяются нервные сети, применяется логическая регрессия, или random force classifier, или XGBoost. Он решает из-под коробки 90, accuracy 90 или 85. Это реально такой кейс в том плане, что из-под коробки она всегда работает. И модели плюс-минус будут одинаково подсказывать, но там можно будет очень сильно тюнить ваши гиперпараметры, чтобы она могла не переобучаться и не недообучаться. И после того, как вы нашли модельку, поиграли с данными, проходит этап тренировки и валидационный момент. Чтобы избежать тех кейсов, чтобы вы не дообучились и переобучились, валидация играет очень важную роль. Всегда, когда вы перед тем, как показывать продакшн или делать какое-то определенное действие с этой моделью, после того, как вы тренировали, подготовили, вы обязательно должны ее провалидировать. Примерно максимально с реалистичными кейсами по-хорошему. Условно вы можете собрать ваши данные, тренировочные данные могут быть 10 тысяч, валидационные могут быть полторы-две тысячи, и вы можете оставить самую вашу последнюю тестовую уборку, которую точно не было видно вашей модели вообще, и можете сделать на ней предсказание и посмотреть, насколько они схожи. И опять же, чтобы научить вашу модель очень хорошо работать, можно бы проводить на этапе тренировки и на этапе валидации их, условно, distribution ваших данных, он может отличаться прям сильно, и за счет этого вы можете получить тоже какую-то очень странную оценку, по этой причине нужно всегда смотреть на данные. Как мы говорили ранее, данные всегда врут. И самое последнее, это deployment. Насколько бы вы прошли этап тренировки данных, подготовки моделей и подготовки данных, всего, что с этим связано, и дошли до того момента, когда вам нужно показывать, презентовать вашу модель или выказывать в продакшен. Эти два процесса, они, в принципе, чуть-чуть походят, но здесь очень важную роль играет инфраструктура, и как раз-таки то, что вы можете автоматически делать выкат ваших кодов или моделей, без разницы, типа настроенный CACD. Но помимо того, что вы залили вашу модель, она очень круто заливается, или происходит такое, что одна команда написала код, собрался в какой-то контейнер, и он отправился куда-то в продакшен, на какой-то сервер, здесь тоже немаловажную роль играет мониторинг всех этих процессов. Потом, то, что вы точно знаете, что из десяти серверов, на каком сервере, какая модель стоит, условно, может быть, версии у этих моделей отличаются, чтобы можно было легче решать проблемы и находить решение за короткие сроки. Также, в принципе, инфраструктуру и MLOps компоненты, они в принципе похожи, потому что вы должны десционировать ваше приложение, ваши контейнеры. Трекать эксперименты тоже очень немаловажно, по причине того, что условно вы взяли какие-то гиперпараметры и взяли какую-то определенную версию данных и закинули в тренировку, и вы затрекали ваши эксперименты. И у этого эксперимента есть свои артефакты, под артефактом могут быть модели или результаты ваших моделей, и вы как-то можете трекать эти эксперименты таким образом, чтобы знать, что условно 23 октября я тренировал модель с вот такими-то данными, с вот такими-то гиперпараметрами, вот столько эпох, и вот такие результаты у меня появились. И это очень сильно упростит вашу задачу, когда условно через полгода вы должны вернуться к этой задаче, посмотреть, что происходило, и спокойно можете там для себя открыть большой репорт, где все это будет. И последнее, это тестирование. Тестинг никто не отменял в плане того, чтобы обязательно должны смотреть. Тестинг не имеет в виду, что вы модели прогнали через тестинг, а то, что весь пайплайн он стабильный, он работает правильно, он не вылетает, нет ли утечек памяти или каких-то дыр или вообще какие-то проблемы. Также ваш technical needs должен всегда совпадать с business needs. Имеется в виду, что нельзя условно заказывать сервера на 100 миллионов, решая задачу, которую зарабатывает всего лишь 100 тысяч долларов. По этой причине все эти вещи должны очень сильно смыкаться, и они не должны друг от друга прям сильно уходить. И также весь пайплайн должен быть scalable, имеется в виду, что его можно условно запустить как один экземпляр или можно как тысячу. И чтобы с этим проблем не было, имеется в виду, чтобы не было очень сильных привязок к тому, что приложение или весь пайплайн как-то ограничен, что его нельзя запускать на сотни серверах, можно только на одном. Таких вещей, к сожалению, не должно быть. И reproducibility это тоже очень важный процесс, в плане того, что вы ушли из компании, вы очень хороший сотрудник, и вы должны оставить за собой такой пайплайн, такой код или модель безрадиницы, который может любой дурак из-под пилька завести, чтобы она была рабочей. И вся ситуация, которая происходит у вас на компьютере, на вашем сервере, она могла повториться на другом. Это тоже очень важно. Пайплайн в ченнелизе с Docker, как мы проходили ранее, очень ключевую роль играет. И тоже очень щепетильная тема, как technical debt. Это тех долг, когда вы наращиваете компанию, наращиваете ваш техпотенциал, увеличиваете ваш штат, но ваш технический долг растет. Это очень плохо, так и не должно быть. Там должен быть всегда какой-то определенный trade-off, потому что тех долг, на месте того, что те задачи, которые должны быть сделаны, которые бизнес от вас требует, после какой-то определенной интервации, она должна в скором времени закрываться. Также и maintenance cost, о котором мы говорили, что всегда должны быть определенные trade-offs, и с бизнесом вы должны всегда разговаривать, и быть не то чтобы в ладах, а на очень быстром коннекте, чтобы бизнес понимал, как команда из датасиентистов что делает. И опять же, чтобы вы понимали, что ваш модель или ваш результат его требуют, и когда вы его запускаете и смотрите, как он работает, вы можете увидеть живой результат, насколько эффективно вы можете либо поднимать очень много денег, либо решать проблемы на огромное количество бизнес-стребований. И самое последнее, это больше про оптимизацию, про то, что мы должны проводить все это, уходить от того, что это происходит ручками, и автоматизировать. И последнее, это обтестирование, тоже очень важный процесс при датамноветике Breda.science. Перед тем, как я перейду к стримлиту, у вас вопрос по теоретической части. Нет, да? Или есть? Ребят? Попроса нет. Ok. Strimlit. Наверное, здесь будет правильно сказать, это в простом формате. Strimlit – это framework на Python, который позволяет создавать веб-приложения, при том, очень красивые, и которые могут облегчить жизнь, когда вы президите. Именно для… Больше, наверное, не для деплоймента, хотя нет, для деплоймента он тоже подойдет. Но больше, наверное, чтобы, если вот кто знаком с такими темами, как Power BI, W и такой вроде аналоги, когда у вас есть, условно, источники данных, и вы можете показывать огромную аналитику дешборды, демонстрации ваших моделей, результатов ваших моделей. И это, в принципе, интерактивный тул для того, чтобы заниматься датасайнцем, если прям очень коротко говоря. Он очень супер простой, на нем не нужно писать JavaScript, CSS или HTML. Там прям библиотека решает все эти вопросы. И самое важное, что он очень сильно заточен, чтобы вы могли хорошо работать с графиками, с тулзами ML части именно Python, условно с Pandas, с NumPy, с SkyPy, с Skykit-learn. И у него есть возможность даже создавать какие-то сессии, чтобы можно было делиться данными между страницами. Это вполне себе многофункциональный инструмент, который позволяет как раз таки создавать очень крутые приложения, чтобы вы могли больше, наверное, какие-то очень большие, типа библиотонали сделать. В качестве примера давайте представим, что вы какая-то каналтиковая компания, вас зовут TSHO или там саморог Хазна, на то, чтобы вы разобрались в каком-то отделе. И пришло, там 10 человек с каналтики компании начали разбираться. И пришли к тому, что у них накопилось очень много инсайтов данных, что вот, условно, саморог Хазна есть вот такие-то донамные, можно это вот так применить. И, к сожалению, с презентацией смотреть это не особо приятно. Очень классно, когда это можно прямо предназнавать в каком-то деплое на приложении, чтобы, условно, оно всегда было под рукой, его даже онлайн можно было показать. И сейчас мы с вами будем как раз таки смотреть, как это можно сделать. И еще один важный момент про структуру стремлита, типа само приложение, HolderUp, условно, можно сказать, у него должен быть основной его куб, скрипт MainPy, AppPy, без разницы, как хотите. И страницы, которые попадаются, условно, мы несколькостраничные, мы можем вот так по частям делить. И можем еще питомские утилиты создавать, но, в принципе, сейчас я вам буду показывать, как это можно будет сделать. Можете открывать... Так, у всех же доступ есть к кластеруму, у всех есть код, который я отправил туда, да? Буквально где-то час или полчаса назад. Да, да, есть. Хорошо, вы можете его скачать. И сейчас мы будем с вами... Смотри, как это происходит. Так. У меня, к сожалению... Давайте попробуйте пока установить вот так просто. Просто вот так сделайте. Tip installStreamLit, и он там должен его сразу установить. И не должно быть проблем. Открывайте папку Basic. Мы сейчас его запустим. Папка Example. Ой. Нам надо через VS Code это делать, открывать? Или через этот подойдет? Не Питер. Нет, нужно будет вообще через... Да, лучше через VS Code. Не лучше, это один из таких вариантов. Как запускать приложение? У вас есть папочка, называется Basic. У него есть просто один код, в котором написано весь функционал StreamLit. Есть ли среди вас, кто занимался с фронт-анты разработкой? Или быканты разработки вообще как-то сайты создавал? Если есть, то в принципе StreamLit это такая же утилита. Только это происходит все на Python, и оно более упрощенное. Давайте посмотрим, что же происходит. Запускается на таким образом, что StreamLit, Run и app.py. Мы запустили. Сейчас мы откроем... Оно находится у вас, вот, Local URL. Давайте его откроем. И вот, у меня открылось приложение. Это наш сайт, который мы создали за счет StreamLit. Что же мы здесь делали, что у нас есть здесь? У нас есть, условно, какой-то там Hello World. Какой-то хидр. Есть логотип, который мы только что запихнули. Какой-то текст, который можно написать. Есть датасет, который можно посмотреть, прям потыкать. Type to Search, мы с вами, например, Trans. Это прямо полноценный интерактивный инструмент, в котором можно работать с данными. И даже есть очень крутой экзампл, который интерактивный, и по нему можно тыкать, смотреть, и так, который можно сохранять. И давайте посмотрим, что же это данные. Это уровень удовлетворенности людей, в зависимости от стран, где это, если не ошибаюсь, это ВВП на душу населения. И уровень удовлетворенности людей. И как мы видим, чем выше уровень удовлетворенности людей, тем и выше, в принципе, ВВП. И какая-то страна интересная. Это Германия. Это у нас Дания. Синяя, похожая на Исландию. Австралия. И, в принципе, по самой минимальной, это у нас, я даже не знаю здесь, какая-то страна интересная. Она импортируется... Точнее, туда импортируется Power BI. Что? Я имею в виду, туда импортируется, например, Dashboard, который мы, например, в Power BI сделали. Как-то ее можно коллаборировать, так сказать? Я, к сожалению, не знаю, но просто в чем смысл создавать график, как бы, например, и потом импортировать его в стримли, когда можно сделать его в Pandas или там в CiaBorne, Matplotlib и здесь показать. Потому что в Power BI у вас график создается также на данных. А, что-то не подумал, извините. Вот. И здесь, кстати, тоже очень такая прикольная тулза, где мы... Смотрите, ребят, я изменил на 9, и оно сразу же поменялось. Посмотрите, как это работает. Вот, вот, вот. Смотрите на правый угол. Когда я его так меняю, он сам автоматически, условно, прогоняет и обновляет, условно. Ну, эту часть. В принципе, давайте посмотрим, что творится в коде. Первым делом, мы... Altair, кажется, это библиотека как раз таки стримлита. Altair, стримлит. Его тоже нужно будет установить. Это, кажется, да, если не ошибаюсь, да, да, да. Это VEGA Altair Library, типа библиотека именно для работы с... на подобиях, типа как Matplotlib и Ciavorn. Altair тоже такой же фреймворк. Что же здесь происходит? Мы загружаем там data frame. Что же происходит, да? Загружаем data frame, потом создаем title. Здесь, типа, это прям как вверска сайта. Как мы видим, Hello World, у нас есть image, это наш логотип нашего стримлита, который мы здесь указали. Потом мы добавляем какой-то header. И у нас еще есть st-sidebar. Все, что мы указываем как sidebar, оно будет находиться там. Это вот в нашем случае, вот здесь. Типа, стримлит.sidebar, header, типа this is my seed, там sidebar. И здесь как раз таки есть у нас header в sidebar. Это вот, который здесь указан. Это вот slider. Там условно он от 0 до 10. Так, instance. Давайте мы попробуем с вами поставить 100. Потом обновим. Ой. И да, он здесь применился. Потом смотрите, что здесь очень важно. Когда вы вот так делаете st-sidebar-slider, у этого слайдера у этого satisfaction есть данные. Даже можно так делать. What will happen? Давайте, чтобы было более внятно, мы придется так сделать. Вот. Как мы видим, он там два раза обновился, походу. Смотрите, я буду обновлять. И каждый раз, когда я обновляю страницу, он, типа, этот satisfaction принтуется. Сейчас я изменю, этот satisfaction тоже будет принтоваться. 20, 46, 70, 90. Ну, условно это просто так, интерактивный стиль, как можно работать с данными. У нас есть возможность, st-write это, чтобы просто написать текст LookMyData и st-dataFrame, если прям в него клинится, если даже надо документацию посмотреть, это когда у нас есть наш стрим-лит, и мы говорим, что у нас есть какой-то dataFrame. И он здесь показал, что мы можем сделать, если у нас есть какой-то dataFrame. И он здесь показал. Давайте теперь попробуем не все туда запихнуть, а попробуем только хидр запихнуть. 1 и 10 условно. И вот. Даже, кстати, не нужно будет перезагружать. Обязательно можно просто сохранить код и просто перезагрузить страницу. И он, как видно, здесь показал нам только первые 10 по хидрам. Что у нас еще есть? Еще есть chart. Здесь мы передаем Altair. Это особенность библиотеки Altair, что вы должны передать ваш chart там. Почему make circle? А, да, make circle — это вот эти точки. Потом берете информацию нашего Altair'а. И потом, после этого, когда вы создали этот C, это некий объект, куда вы отправили ваши данные, и хотите посмотреть, как он будет принтоваться на вашем сайте, и он вот так покажется. В принципе, это все по первой странице. Мы дальше перейдем на вторую. У вас есть вопросы здесь? Ребят? Все в принципе просто, да? Так, окей, хорошо. Переходим с базика на базик 2. Здесь чуточку другое было, если я ошибаюсь. Уходим, базик 2, стрим лид, рам хап хап хапай. Прикольно, уже новая версия доступна. Ну ладно. Как мы видим, у нас здесь есть какая-то GIF, которую мы смогли. Есть DataFrame, где мы захайлайтили самую максимальную, как мы видим, датафрейм, который мы смогли. И вот, в этом видео, мы нашли самую максимальную, если я не ошибаюсь. И по LifeSatisfaction. Окей, хорошо. Что же у нас еще есть? У нас есть bar chart, которую мы нашли, и с satisfaction сделали по сторонам. Там условно. Потом есть еще метрики. Это тоже очень прикольный инструмент, где у вас есть возможность показывать, насколько условно. Все эти сейчас данные, которые здесь написаны, они вбиты вручную, типа 32-4, но никто нам не отменяет их. Динамически как-то меняют, в зависимости от нашего кода. И здесь тоже, как можно показать, как работает слайдер. Условно сейчас я поставлю на 30, и он сразу же динамически покажет, как он его заранил и, условно, обновил. Можно поставить на 60, и 60 square 3000. И еще, что классно, у вас есть возможность принтовать вот так, типа, красивый код. Давайте посмотрим, как выглядит код. В принципе, по датафрейму здесь понятно. Он сделал sort values по ascending false, потом взял какую-то GIF, после этого написал, типа, посмотрите нам этот сет. И вот здесь очень, кстати, важное, как у нас получилось захватить у нас получилось захватить таким образом, что мы, типа, стиль нашего датафрейма применили, и потом сказали мы датафрейм. А что если мы уберем эту часть и просто обновим, и у нас будет просто пустой датафрейм, потому что мы не сделали хайлайт. Возвращаем обратно, обновляем страницу, и у нас хайлайты вернулись. BarChart у нас здесь также сделан через библиотеколь Таир. И мы можем внести вот эту библиотеколь, через там функции MarkBar. Просто в зависимости от того, что вы хотите, нужно будет копаться в библиотеки как раз с кистериволита и смотреть, как на нем можно будет всегда показывать. Потом мы показали наш S, Altair, и еще это это у нас Subheader, DisplayMetrics. Здесь показан наш график, BarChart. Потом показаны Subheader, DisplayMetrics. Метрики показываются таким образом, что вы показываете, условно, который мы видели, 32, большую цифру, и 4. Если вы перед ним надаваете минус, то метрика упала вниз, или метрика увеличилась. Чем это удобно, как это можно сделать? Условно, вы показываете две разные модели, у которых метрики равны чему-то, и потом вы хотите показать, насколько вторая модель, номер 2, она стала больше, стала меньше, и все это можно делать динамически. И Subheader, в котором мы какой-то код добавили. И даже здесь мы стримли добавить туда код, на нигуи, поэтому давайте теперь попробуем добавить еще C++ код. Код CPP. Соберем. И мы сейчас с вами добавим C++ код. Cpp.Language. а тут есть не ошибая с кп кажется ой и да вот смотрите очень красиво добавлен код типа прям можно презентовать условно если вы перейдете как это функционал которую там добавили его можно здесь написать так переходим дальше теперь здесь все понятно medium стремит рампа парень что же у нас тут происходит здесь если не ошибаюсь датчик более прикольнее это опять у гифка господи с ней мы можем не прям так а здесь уже чуть интереснее смотрите ребят мы скачали датасет хаусинг дата в которых есть логитет и латитет есть какие-то данные типа медианы количество комнат количество кроватей точнее бедрумов популяции и так далее у нас у нас какие-то данные и что что у нас получилось у нас получилось отрисовать как раз ски относительно вот этих логитет и латитет полностью всю карту чтобы у нас есть чем это полезно условно козастане вы спартили крышу насколько бы это не незаконно не было у вас получилось партии крышу вы хотите показать объявления и которые условно вы там предсказали это их цену то в принципе типа можно сделать вот так и показать там именно насколько сильно распространены определенные объявления там в какой-то части регион алматы астана или какой-то другого города что у нас здесь еще есть это наш скатер плот по медиум хаус валию и медиан инком это насколько прикольно это медиана приходящего инкома постановных зарплаты людей и доходы и медианы как раз таки ну словно сколько будет стоит дом и как мы видим здесь есть некая линия связь о том что если скорее всего это вот какие-то от лаеры в зависимости насколько у вас будет увеличиваться у вас инком то и дом в котором вы живете также будет увеличиваться и здесь еще есть другая там другие данные это показаны о том что чем ближе к океану там не рэй или там находится вообще в каком-то острове или внутри типа страны вот такую аналитику они сделали давайте посмотрим на код в принципе там все очень просто первым делом исключаем датасет там его так это ой это basic sorry sorry sorry первым делом искачу скачиваем датасет дальше принципе с этой частью можно не ознакомливаться создать какой-то сапхидр говорите там что это ваш датасет ой ой ой можете принципе ой здесь их очень много можно будет попробовать условно кто-нибудь изменить в этих данных потом это можем брать и вот смотрите у стримлета есть моя built-in функция называется именно мэп она требует чтобы вашем дата фрейне были как раз такие параметры дат лангитют и латитют и в этом случае он может сможет отрисовать типа стмап он сразу автоматически помет насколько типа ваши данные там показаны потом мы здесь показываем медиан инком медиан хаос валют и что мы хотим там именно насколько данные распространены какой-то оушен проксимитии и он дальше там нам полностью отрисует покажет как здесь пока как здесь видно окей принципе по этой части вопросов нет ребят здесь очень сильно зависит насколько вы хорошо знаете библиотеку здесь нужно очень много сидеть наверное и проверять возможности они здесь прям очень достаточно чтобы сделать очень красивый сайт так это последний это не последний там идем два медиам два стрим лид а мне кажется здесь еще ключи стрим лид его можно выключить стрим лид рам от очкопай короче без знаний верски сайта просто изучив эту библиотеку достаточно хорошо можно делать такие классные вещи да да да типа это основной типу именно концепт в плане того что вам не обязательно уметь сс или аштем или древескрипте чтобы там создать какой-то очень классный сайт качестве примеров можно просто зайти на стрим лид и на главных сайт и там будет показано типа примеры приложений вот например типа built powerful apps что же можно сделать можно вообще в качестве самого простого примера вы можете у себя локально запустить ламу вторую если я не ошибаюсь там самое минимальное на каком-то простеньком компьютере может запуститься ну словно какой-то модель куда и можете приязать ваш стрим лид приложение чтобы можно было полноценно прям как с опен опен и чатом g5 общаться что принципе очень круто можно будет зайти давайте посмотрим что типа еще могут делать люди там пред тема сейчас полностью это приложение которое полностью сделано на стрим лиде называется на пред тема давайте мы посмотрим что же она может сделать в селона здесь можно кастомизировать он прям а не фига себе создает сайты вот извиняюсь да не слышно да не слышно интересно салава вот возможно ли на стрим лиде видеть именно момент обучения модели то есть там как не будет ли не на регрессия как она данный момент обучается и вот палка на графике не поднимается по мере обучения да это можно сделать да это возможно потому что там придется самому чуть покопаться с тем чтобы во первых ну ладно данный подгрузить подгрузить мы сможем дальше нужно тренировать модель и условно рисовать график самому а вместо этого можно просто покупаться с tensor board который за вас типа все эти графики можно рисовать просто ему нужно подкармливать данные но да это можно сделать типа вообще без проблем так возвращаемся к нашему руку принципе можете найти можно где можно деплоинуть диджитал оушен или там рейлой если не ошибаюсь стримить даже позволяет у себя где-то вот насчет деплоинта именно бесплатно то или нет я к сожалению не знаю но можно будет посмотреть именно стримит комментик но никто сейчас очень мало сайтов где можно вообще принципе бесплатно что деплоить что плохо диджитал оушен уже кажется все они уже кажется запрещают этот вариант но я проверял год назад тот момент что было недоступно уже можно попробовать но могу вам сказать лайфхак вы можете написать пи с пи с тор это вообще фейс клаут сказать дать тестовый период на неделю они дают очень просто дают супер маленькую машинку с которым можно это будет там заделать посмотреть но вы можете там типа для начала просто у себя на локальном компьютере сделать прям целый деплоинт и прям посмотреть как это будет происходить в этом ничего плохого нету так давайте посмотрим что здесь происходит у нас здесь показано какие-то данные где есть она это опять ты же россия а они это это не gdp они это да этот в в п уровень удовлетворенности и здесь там показан уровень удовлетворенности в в п и в принципе какая страна да здесь даже мы прикольно смотреть здесь можно посмотреть именно саметь и данные они здесь нельзя но здесь что интересного чем отличается от то первое здесь у вас есть возможность выбрать какую страну хотите например корею хочу только посмотреть там сравнить италию точнее корею с японией насколько сильно они друг друга зависима хочу например добавить турцию и у меня данные сразу автоматически подтягивается они изменяются давайте посмотрим в код как это было сделано здесь принципе сделала очень одинаково вы подгружаете в депцент делайте тайтл и делайте сайт бар что же это сайт бар делает он берет выбирать создаете варьей блока он три который которым именно будут ваши страны потом делайте селектор мульти селектор который здесь показан ведь который будет сохранять а точнее он возьмет то что вы взяли селекты точнее в каунтрии тачей в селекты каунтри запихнется то чтобы было внутри каунтри давайте посмотрим что же оно будет внутри себя принтовать когда мы будем выбирать сейчас обновим пока на пусто смотреть селекта каунтри пусто выберем россию и сейчас добавилась россия выберем турцию славакию и сейчас типа стрим лид работает в интерактивном формате что код у вас каждый раз запускается и у вас есть возможность посмотреть то что сейчас у меня установлено на селекта каунтри равен russia турки славы републик так возвращаемся и здесь очень важный момент ваш фильтр дата frame будет равен df country и ваш country из in selected country имеется в виду что вы возьмете за то фрейм эти сэмплы у которых колонка каунтри точнее валют в колонке каунтри будет лежать в этом маленьком листе из in selected country и здесь дальше идет такой момент то что если и в селект каунтри он не равен пустоте то в этом случае ваш фильтр дата frame будет ну как-то повторяет вот этот но если ваш селект от каунтри типа нет то ваш фильтр дата фрейма будет просто равен самому изначальному дата фрейма имеется в виду что когда вы страницу так подгружает то наш типа в текущем формате фильтр дата фрейм даже можно принтовать давайте посмотрим и фильтр df сейчас ой бля слишком много непонятно давайте шейпы выгодим прихватить его шейпы правим 29 и 3 окей хорошо еще раз обновить страницу и опять же 29 3 дальше выбираем россию там была была была и значит он будет 5 и 3 типа 5 цеплов по точнее 5 строк и 3 валют в принципе это у нас россия сейчас мы давайте удалим 3 и у нас он станет 2 строки и что мы делаем дальше с этим фильтр дата фрейма да мы просто берем его говорим стрим лид давай и по сейчас мы дата фрейма и дальше можно даже в чарта запихнуть ваш как раз ки ваш фильтр дата фрейм в который будет как раз ки принтовать всю эту информацию автоматически и дальше в этот чарт бары закидываем эти вольтаир через точнее закидываете стрим лид и он сам понимает всю эту информацию и будет принтовать в принципе это все по сайт пару здесь вопросов нет и если нет то давайте мы на самую последнюю тему перейдем потом даже сделаем очень мини проект наверное вам будет интересно стрим лид рамапай и вот самая последняя она чучку другое что же она делает мы сейчас возьмем тоже дата сет это уровень удовлетворенности а странно и в вп и проведем такой анализ это мы делали play satisfaction там зависимости это уже не интересно теперь давайте попробуем более интерактивно там более так красиво сделать да словно то что условно здесь у нас типа более условно таком более красивом формате наверное если так сказать это по ею он постарается одну и ту же информацию можно кстати да очень прикольный момент стрим лиде можно добавлять эмоджи если не ошибаюсь это сделано через их коды сейчас давайте открываем advanced advanced на воду это tulip cherry blossom rose cibicus sunflower blossom это вот типа через markdown и у нас есть возможность добавлять эту информацию и что же мы можем попробовать сделать что же у нас есть у нас есть дата сет который мы сейчас достаём у нас есть дата сет фильмов где есть рейтинги и жанры это вот типа есть амб рейтинг и как будто ваил байд гроз гроз это кажется не ошибать сколько они заработали и можем посмотреть кто у нас здесь история игрушек что какой-то амб и амд рейтинг он будет в некой степени коррелировать насколько хорошо условно заработает сильно drop down filtering чем же он отличается так у нас здесь это он нажимался нет давайте посмотрим если турцы выбрали они тут уже другая так года я не тоже дата сет закину нет сейчас а это по жанру нет а блин ссори я забыл смотрите ребят я забыл что есть внизу именно управление у нас есть слайд фильтринг это вот условно по рейтингам а нет вот по годам мы условно меняем и там наш слайдер меняется сейчас данные слайдера равны там тысячи девять за 93 мы меняем их и как вы видите информация меняется а у прикольно уж после информации нету дальше 3011 что же у нас еще есть drop down filtering это у нас жанры давайте посмотрим что у нас там по музыкально фильмом это там beauty and beast chicago тоже есть еще словно комедия какая-то классная рататуи мадагаскар и вся эта информация мы условно он подхватывает показывая информацию только ту которую мы здесь делаем и еще последнее это радиобаттон здесь мы меняем информацию именно по рейтингам самого фильма и самое последнее checkbox formatting это мы просто можем там поменять размер как все это сделалось сейчас у нас на данный момент есть наши movies это наш датасет есть рейтинги жанры и сейчас мы должны запринтовать именно как раз таки наш base chart дальше вся информация которую мы будем внизу указывать она будет и поменять именно то что принципе нам надо мы берем базовый именно наш чарт добавляем туда где то что по бокам будет это а и диби рейтинг именно вертикально горизонтально это будет ворлд вайп типа сколько она денег собрала и условно там релиз это по годам да и дальше убрали то что трансформируем условно наш датасет за счет альта и и трансформатора я к сожалению именно не сильно копался в его исходниках функционал просто огромный который типа позволяет вот такой рода именно фильтр не делать и потом зависимо от наших рейтингах мы все это фильтруем и инкодим как раз таки в принципе наш датасет точнее график который здесь есть так я наверное сейчас мы не успеем если сейчас здесь долго останусь потому что следующий проект это связано с мл если сейчас вот вопросы относительно того что мы прошли если нет то мы можем сразу туда переходить или сделаем переменку чтобы мы смогли подавать вопросы ребят ну я считаю что я сам его еще буду изучать и для меня в принципе понятно что функционал функционал у него большой так что в принципе у меня вопросов нету то есть кодингу можно и самому научиться то что вы там делали посмотреть вот эти проекты лично для меня это классно там примеров очень много и поможет посмотреть типа функционал очень неограниченный и все опять же так вопросов по стримлите нет тогда давайте начнем следующий проект можно спрашивать вопрос конечно код запускается он через только терминал только да его то есть надо так выписать ран просто у меня как-то ошибка нет не обязательно вы ну где вы запускаете новой с коде или там на почарме на выскорбе да новой с коде вы можете просто условно если у вас уже ваше окружение настроено там типа пиблитика вы можете здесь вот в правом углу есть просто запуск хотя так если не ошибаюсь он а нет нет нет нет нет нет нет нет к сожалению нет стрим лет так не будет запускаться стрим лет нужно запускать за счет терминала да именно то что перед этим какая у вас ошибка там происходит там то что вообще с библиотека ошибка в ходе а пипин стал тоже сделал он как бы не помогает почему-то вы можете сказать какая что-то случилось может попробуем разобраться если это быстрее по идее библиотека очень простая там если вы взяли конду взяли просто там создали какой-то виртуальный варм ты просто установили он с ним проблем не должно быть а остальных установился всегда я вот запускаю у меня выходит но модуль наима стрим лид хотя вот этот пип стал он я его сделал значит быть не на тот ваш типо пип не под попробуйте пип 3 сделать и 3 стал стрим лид и потом запустить стимлид это вот типа несмотря на ваш код примерно скорее всего так ладно ребят давайте дальше продолжим что же мы сегодня должны сделать я заранее уже прописал какие библиотеки нужно будет установить это холл а наш сегодняшний проект что же мы будем делать мы сейчас попробуем создать целое приложение а стрим лиде которая позволит нам сделать целый дат анализ нашего проекта первым делом я хотел бы наверное показать само приложение потом уже технические детали так можно просто взять стрим вообще если так посмотреть если у вас не вот та проблема у меня тоже браузер открывается брал типа стрим не должен приложение никакого устанавливать не надо все просто у меня уродливая на по сравнению с тем как у вас там в каком смысле не знаю но бифон белый там разбросан я к сожалению не знаю вроде бы нет он типа должен слиться без проблем нормально там да он нормально должен быть он не черный не черный конечно нормально нормально так давайте посмотрим что за приложение мы с вами сделали да так сегодняшнее приложение оно позволит нам попробовать загрузить данные прям целый дат ассет прям у которого есть определенные колонки прям название колонок и потом мы этот дат ассет посмотрим как-то попробуем сделать какую-то аналитику завизуализировать и даже сделать предик в принципе в этой странице ничего информативного нет это просто про проект да та плот мы открыли страницу дата плот что же здесь происходит наберем какой-то у меня уже просто заранее готов этот ассет тысячу строк буквально его сгенерируем давайте посмотрим смотрите ребят пайл аплоды successful мы прям сейчас в наше приложение загрузили целый это frame по целый дат ассет он говорит о том что там есть типа тысячу строк есть какие-то 13 колонок и есть там 51 миссинг моглость и здесь мы видим что этот ассет которым типа мы сделали дата превью у нас есть очень супер разные данные у нас целая типа есть дата есть продукт какой-то есть какая-то категория которую мы можем делать потом есть числовые значения типа sales есть quantity есть числовые значения типа customer age gender регион какой-то потом customer satisfaction тоже который мы тут проходили и более значение там платит или не оплакать discount shipping cost total amount и profit margin который мы получили условно это какого-то клиента да и дальше у нас есть информация по каждой этой колонке где у нас даже показано но ну каунт что у нас к вот этого customer и есть ну вы там в районе 51 это выглядит очень приятно идей плане того что мы сейчас сделали такой супер базово на речку да давайте посмотрим по дата анализа да так ленинг потому что сейчас у нас в нашем датасете если не ошибаюсь есть пропущенные данные и после того как мы сделаем клиндей клиндата у нас так давайте вспомним где у нас бро с файлс сейчас у нас есть в кастомер эйджи у нас есть нул дата давайте посмотрим тут смотрим сейчас мы сделаем калом анализ это принципе сам рестатистик по всем числовым значениям то что в колонке сэлс их общий количестве тысячу есть минное значение стд мин и дальше по квантилям и максимально к сожалению у нас в дата миссингов не было у нас оно есть кажется в кастомер эйджи здесь мы можем заметить что у нас есть уникальное значение 52 но миссинг 1 51 после того как мы назовем клиндейта да так вы не ссссфу и у нас они удаляться если не ошибаюсь теперь миссингов нету или просто было дробь публикейс я точно не помню что это произошло дальше переходим на визуализацию дистрибьюшен плосори лэйшн шоп давайте выверим там сэлс и квантитина какой-то кастомер эйдж у нас есть распределение здесь прямо кстати вот нормальное распределение можно видеть если так говорить такой конечно жизни реально не бывает но просто и бокспорт есть есть максимальное значение а по фэнс 3 квантиль медианная квантиль 1 и их минимальные значения можем даже посмотреть на релэйшн ши плотс когда между сэлс и сэлс но ладно это прям идеально будет давайте посмотрим от профит мерчаном что ваши продажи будут зависеть там напрямую можно спросить да да конечно просто вы показываете экран просто или нет у всех видно экран или только у меня не видно это видно у меня видно а я все это значит только у меня на а у вас не видно экран у меня дать наверное я что-то там нажал извините окей в принципе здесь мы видим что у нас есть какой-то корреляции между ними давайте посмотрим еще кастомер сет сфакшен от вот он а он так но здесь ладно не так сильно информативно кастомер и что же шипим коз от какого-то кастомер эйджи сэлса окей не так сильно видно сфес и корреляции анализ здесь как у всех зависом да да я испугался не зависит мало он то очень сильно зависит от профит кумохан вы очень сильно зависали поэтому то что минуту назад говорили полторы минуты назад тоже просит реп на чем я остановился что вы последний показывал вот вы начали показывать дата визуализации вот этот как раз и а дистрибюршим плоско он показывал да да да да просто следующего дата визуализации у вас начал тормозить короче именно с релайшем когда вы начали говорить а окей все тогда с корреляции в принципе тут тут ничего особенного мы просто видим связи между данными датасете это вот профит маркет очень сильно связан с сэлзами со тот ума там и профит маркет на профит маркет не сильно так ладно это профит марджин зависит от тотал маунта и сейлзов потом тот он мало зависит от quantity sales of и принципе еще профит маркет в принципе и здесь понятно. Типа, наш функционал показал способно какой-то корреляционный анализ. Давайте теперь попробуем сделать prediction. Обучим простую регрессионную модель. Супер простую. Возьмем в качестве... Давайте попробуем profit margin. Выберем эти данные для того, чтобы тренировать модель. Наш target в Array будет profit margin. Принадлежим модель и видим, что у нас R-square. Это, если не ошибаюсь, accuracy. Ну, не accuracy, а числовая оценка нашей модели от 0 до 1. В принципе, модель очень хорош. Но давайте проверим. Что у нас там очень сильно было связано в визуализации? Это sales и profit margin от sales. Давайте попробуем так сделать. Возьмем только profit margin и попробуем предсказать только sales. Тенируем модель. И здесь тоже R-square очень высокий, где root mean square error очень маленький, R-square очень высокий. Никто не говорит, что это модель очень качественная, но как было бы, модель нормально обучилась, и мы увидели, что это можно сделать здесь. Но это не говорит нам, что вы должны этим заниматься здесь, просто я показал, какой функционал можно сделать на стримлите. По большей части он очень классный. Тотально рекомендую. Если на следующей неделе у нас будут проекты, и если вы... Сейчас пока я предварительно скажу, пока мы концепт не обсудили, но используя FastAPI или стримлит, можно очень достойно классный проект сделать, полностью со всеми данными, которые мы проходили все эти 12 недель, и со всеми функционалом можно многофункциональный репорт сделать, как вы это делали и что. И User Settings это от себя добавил, такой очень простой, где условно я могу здесь написать Kamal Khan, выберу тему, какой-то, например, dark, enable notification, color схем, хоть выберу желтую, например, save settings, и у меня оно применится здесь. В принципе, все, это по нашему приложению. Так, теперь, теперь, теперь. А как это оно используется на практике? Ну, я скажу, да, но здесь еще один очень важный, важное отступление. Ну, мои друзья и коллеги, которые работают в банковской среде, они используют, да, типа Streamlit, там FastPi, чтобы выкатывать очень быстрые демки. Здесь больше, наверное, именно для демо больше оно используется, чтобы можно было показывать. Именно потому что, когда вы работаете в очень бурно развивающей среде, независимо в какой структуре, то очень важно быстро показывать MVP, условно каждую неделю нужно делать демо презентации. И чтобы делать демо презентации, никому не интересно, что вы там с терминала будете показывать, или там с какого-нибудь, ну, вашего E-Scooter, например, или без разницы где угодно. Оно будет не информативно и будет непонятно. А здесь мы с вами прям просто взяли этот ассет и прям показали. Power BI и W, в принципе, выполняют ту же функцию. Просто здесь, что круто, у вас есть возможность, типа чисто на Python писать. Я просто сам Power BI не пользуюсь и не пользовался, и я буду там сидеть, наверное, как какой-то дилетант, который, если меня попросит, сделает какую-то презентацию на дежборде. А здесь у меня будет более-менее понятно, потому что я знаю Python, знаю NumPy, Pandas, и, в принципе, получится разобраться именно с функционалом, чтобы сделать более-менее нормальную презентацию. Наверное, типа в таком ключе, наверное, будет правильно. Что получается Streamlit и FastAPI в основном для презентации КБшек инвесторам и другим важным людям? Это один из применений FastAPI. Не только для демок можно использовать его, можно использовать уже на продакшене. Он прям продакшен-реди, так же, как и Streamlit. Все от вашего воображения зависит, по какому приложению сделаете. Если у вас получится полностью этот функционал сделать очень быстро, то оно будет работать. Ну, типа самое первое приходящее в мысли, когда вспоминаешь такой инструмент, это как раз-таки когда вы хотите сделать какой-то проект и просто показать. Так, давайте посмотрим. А, да-да-да, вопрос. Придикшена. Еще раз взглянуть. Вы там один только этот фичер выбирали? Да-да-да, я только один фичер выбрал. И как, по-моему, что для R-Square большое получилось? Там не custom-rash, там другой фичер был. 0.94 потом у вас было. Так, что у нас там было? Это R2Score, кажется, имейте. Р2Score, да, это если однерка, то это уже full. Кажется, Profit Margin было, если честно. Да, Profit Margin. Почему больше R-Square здесь? Типа всего лишь один фичер. А потому что между ними корреляция 98%. А, ладно, все. Типа, как бы они очень сильно коррелируют друг другу, поэтому я просто решил показать модельку на ее основе. Спасибо. Давайте Shaky Cost, например. Но он вообще что-то не обучился. Давайте Sales еще добавим, да. Не, я вообще не смог. Quantity. Не, он что-то вообще тупит Profit Margin. Customer Age мы не можем предсказать, оказывается. И у нас есть еще один фичер. И это у нас есть еще один фичер. И это у нас есть еще один фичер. И это у нас есть еще один фичер. Customer Age мы не можем предсказать, оказывается. Вообще невозможно. Что там еще более-менее коррелируют? Давайте посмотрим. Которое там в районе 59 было. Ну, Profit Margin Total Amount. Ну, Profit Margin Total Amount. Давайте вот. Ну, Sales и Total Amount. Sales. Ой, sorry. Sales. Total Amount. Ну, здесь 38. Нормально. Еще давайте Quantity. Нет, нет. Quantity попробуем добавить. А, нет, нет. Total Amount. Ну, вот нормально. Мы взяли Feature Sales. Мы взяли Feature Quantity. И попробовали обучить на то, чтобы линиевную регрессию построил для Total Amount. И в принципе R-Score нормальный. RMS пойдет. Здесь все очень сильно относительно. И даже мы Feature Import показали. Из-за того, что Quantity и Total Amount это очевидные вещи, что они очень сильно с другой румы корреляют. По этой причине у нас здесь данные такие. Окей, хорошо. Давайте теперь вернемся к коду. Что за код? Ну, это код, который мы делали в этом видео. И мы его делали в этом видео. И вернемся к коду. Что за метрика RMS? Я немножко подзабыл. Root, Root, Min, Square, Error. Когда вы находите ремеси формула. Когда у вас есть ваши ИТ, ИТ реальное, ИТ предсказание, вы находите между ними разницу. Находите сумму квадратов. Делите на их общее количество. Ой, боже, это RMSD. Мне нужен Root, Min, Square. RMS, Error нужен. Почему RMS и Division? Ну да, в принципе, правильно. Короче, вы находите разницу именно между вашими предсказанными и между вашими реальными. Все это находите между ними разницу. Берете, если не ошибаюсь, квадрат, находите их сумму и подкорним. У нас есть Min, Square, Error, MSE. MSE, MSE формула. Вот. Min, Error, Square. Вы просто смотрите. Первым делом мы берем наш YET, типа predicted Y hat. Это, если я ошибаюсь, или Y hat predicted. Нет, берем реальный данный, минусуем от predicted данных, находим эту разницу, берем ее квадрат, находим сумму, делим на их количество, типа усредняем, и это будет наш Min, Square, Error. Root, Min, Square, Error, когда мы хотим взять это под квадрат, и здесь именно такой момент, что насколько сильно вы хотите приблизиться, условно, к тому, чтобы ваши данные максимально были с кожами, то чем сильнее вы будете пеналтизировать, то условно будем считать вот такой лоз. По-хорошему, ошибка не должна быть огромной. Она должна быть весомой, имеется в виду, что от вашего предсказания до реальных данных. Так, я бы хотел сейчас вам показать код, из чего он состоит. Состоит наш проект из нескольких частей. Реквариум стикстей здесь вопросов нет. Конфикт. Здесь мы указали конфиги. Здесь, в принципе, больше для структуры самого проекта. Root Dear, Asset Dear, Asset, где лежат данные, но, в принципе, к ним вы можете забить. Uptitle это наш тайтл нашего первого приложения. Допустим, extension и Exel пока, кстати, я объясняю, вы можете взять какой-то датасет и какой-то простой датасет, который у вас есть под рукой. Можете как-ли взять и попробовать запустить в Питере. И можете посмотреть на результаты. Можете взять на какую-то регрессию. Давайте мы попробуем сейчас, например, что у нас есть? Давайте House Price Prediction возьмем. Попробуем на нем. House Price Prediction Dateset List здесь мне не нужно будет логиниться, чтобы его загрузить. А перская ухаб? Нет. Вот эта карта. Так, что у нас по данным? У нас есть Price, у нас есть... О, классно! Давай сейчас его возьмем. Что-то здесь очень... В принципе, нормально. Аплодируем данные. Смотрите, ребят. Наше приложение только что подсказало, что у нас есть общее количество 545 rows, number of columns и классно, датасете нет missing values. Нам не нужно было делать чистку. Что у нас еще есть? В принципе, Price у нас все это, Integer, Stories, что-то по Stories. Интересно. MainRoute, GuestRome, Baseman, Outquadring, все они представлены как Integers, Prefari, FornishGrid. Окей, прикольно. У нас есть целый датасет. Давайте посмотрим на DataAnalytiq. DataAnalysis. CleanData нам не нужно делать, потому что мы знаем, что они там нету. Давайте посмотрим на Distribution. DistributionPlots, Price, Aria, например. Здесь плюс-минус все похоже на... Но не совсем хорошо. Ладно. Price более-менее нормально распространен. Aria тоже. Но хотя есть вот этот Badrooms. Прикольно. Bathrooms, Stories и какой-то паркинг. А как здесь работает с категориальными данными? Он их учитывает через энкодинг? Да, да. Я пока еще в код не спустился, чтобы вы объясните именно эту часть, что он в категории как он определяет. Давайте посмотрим на корреляцию. Более-менее коррелирует никто. Хотя ладно. Давайте посмотрим. Aria от паркинга и Price от паркинга более-менее коррелирует. Давайте попробуем предугадать наш Price за счет... И да. TrainModel. Но очень большая ошибка. Это прям очень ужасно. Очень плохой RISC-Quer. Здесь странно, что коэффициент фичер импортности Bathrooms у нас как-то сыграл. Это очень странно. Давайте попробуем наоборот. Попробуем предугадать какой-то паркинг, например. Но RMS не такой огромный. И R-Square, значит, такой более-менее. И здесь нормальную роль сыграл для нас опять же Bedroom. Очень интересный фичер. Price вообще никакого там импакта не сделал для предсказания. Давайте теперь посмотрим, к чему нам все это, да? Как у нас это под капотом происходит? Первым делом конфиг я вам объяснил. AppPy здесь это Content, Markdown, который здесь есть. И все. В принципе, стемлить мы создали. Приложение само понимает, где находится название страниц, название файла. Это очень такой прям прикольный момент. Мы говорим о том, что это за сеть StreamlitPageConfig. И когда мы переходим на основную страницу, вот. Оно уже само понимает. Дальше переходим на Datalog. Что уже здесь происходит? Здесь мы создаем StreamlitTitle. DatApload. Есть у них файл-аплоудер. Угол из-под коробки. Ничего делать не надо. И, кстати, AllowedExtension у нас здесь это CSV и EXE. Какой-нибудь. Убираем. Здесь, в принципе, все это понятно. Потом смотрите, что дальше происходит после того, как ваш аплоуд файл isNotNone. Если он none, он говорит о том, что PleaseAploadFileBeginAnalysis. Видите, то, что он пока будет ждать нас. А в случае, если мы какой-то файл подгрузили, хаусинг, например, дальше он проверяет. Потом, потому что у нас просто есть допустимый размер о том, что он не должен быть висеть больше 200 МБ, тогда окей. Иначе он выпустит какую-то ошибку. Вот, смотрите, MaxFileSize 200 МБ. Давайте попробуем ограничить его. Этот у нас весит 300 кб. Не, слишком мало. Сколько наш этот весит? Документы, ампер, HoldUpAssets. Он весит у нас всего лишь 130 кб. Не слишком. Ну, в основном, ладно. Это у нас 200 МБ, 200 000 кб. Давайте попробуем теперь. Сейчас он должен, по идее, поругаться. Документы, jobs, а вот ты, HoldUpAssets. И сейчас он должен, по идее... А, да, вот, FileIsTooLarge. Мы сейчас только что укушили себе жизнь за счет того, что он не будет быть висеть больше. И мы его не будем висеть больше. И вот мы уже просто укушили себе жизнь за счет того, что сказали, что ограничили его. Давайте вернемся обратно. Буду все решить, что это работает, что он реально как-то пошлет от нас, если мы захотим что-то больше добавить данные. Что же дальше происходит? После того, как мы ушли, прошли этот этап, мы пытаемся load data. А, кстати, да, очень важный момент. У меня есть утилиты, которые помогают как раз себе работать. Переходим туда, LoadData. Что же делает эта функция? Она проверяет. Это уже не Streamlit, это уже отдельный Python, типа модуль, который я здесь делал, называется утилита. Он позволяет нам делать все эти операции. Что же делает функция LoadData? Она получает файл и пытается дать тебе какой-то пандасовский DataFrame. Здесь обязательно нужно как-то пытаться работать с трайами, потому что могут быть произойти непредвиденные обстоятельства. По этой причине, если у вас TSP или Excel, он подгрузит и отправит. Что же происходит? После того, как вы LoadData сделали, вы сетите о том, что в Streamlit, так же, как и в любом... Ваше сессия, она... Любая сессия, которая происходит внутри Streamlita, они могут делиться данными друг с другом. Streamlit, там, Session, StateData, мы определяем, что она равна нашему DataFrame. Потом пытаемся сделать дисплей, файл UploadSuccessful, например, который мы только что здесь видели. Кстати, давайте попробуем сделать STError. Загрузим данные, и да, он нам сказал что-то. Потом есть... Я создал фонкцию, которая дает нам BasicStats. Что же нам дает? По основным статистикам дает нам количество строк, дает количество колонок. Это опять же эта функция, DataOperations, MissingValues и NumericColumns. Там. И еще categorical. Вот здесь как раз-таки он пытается определить. Как это происходит? Вы даете количество колонок, в которых ходит под тип NPNumber. Ну, иметь в виду, что мы считаем, что это какое-то числовое значение. А когда мы говорим, что это категоричные данные, то мы определяем за счет эксклюдинга. Что это какое-то число. Что же дальше происходит? Потом... После этого stats мы получили, STColumns. У нас есть... А, да. После того, как мы загружаем, у нас появляются вот здесь три колонки StreamLit. Это вот StreamLitColumns. Количеством 3. Давайте еще четвертую добавим. Col4. Col4. StMetric. Что можно добавить? Например, например... CategoricalData. Stats. CategoricalColumns. Ой. Обновим страницу. Подгрузим туда наш DDS. StMetric. Sorry, да, здесь нет StMetric. И вот у нас есть категория дата. Мы типа знаем, что на нашем тесете есть. Что же дальше происходит? Добавляем некий DataPreview. Можем увеличить его количество до 10n, равно даже 15. Сейчас нам обратно придется подгрузить данные. Кстати, почему у нас каждый раз теряются данные? Потому что сессия обновляется, и еще эта страница всегда пытается подгрузить. Что мы сделали? 15 строк мы подгрузили DataPreview. И последнее ColumnInformation. Какую-то вытаскиваем информацию. И отрисовываем ее в SubHeader. ColumnInformation. Относительно страницы DataUpload. Есть ли у вас вопросы? У меня нет. Все понятно. Переходим на страницу 2. DataAnalysis. Что у нас здесь происходит? Если наша страница DataAnalysis получила информацию о том, что данных нет, она в этот момент говорит PleaseUploadData. Сейчас, к сожалению, вот DataUpload. Данные у нас есть. Здесь все просто. Он показывает функцию CleanData. И когда вы нажимаете StreamlitButton, она запускает CleanData фрейма. Потом пересобирает ваш StationState Data равном вашему DataFrame. И говорит о том, что вы создали сексуальный текст. А что происходит? Мы в этот момент внутри дропаем все дубликаты. И мы видим, что мы делаем сессию. Мы в этот момент внутри дропаем все дубликаты. Определяем, что у нас есть numericalCounts и categoricalCounts. Потом все numerical мы нулевые значения забиваем медианой. А categoricalCounts мы забиваем как раз-таки модой. Модой конкретно по этому DataFrame. И после того, как мы это сделали, заклинили наш data set. Потом показываем статистику по этому data set. SummaryStatistics. Именно describe нашего DataFrame. Ну, всех нулевых значений. Ой, numerical значений. Потом у нас есть еще колонка ColumnAnalysis. Где мы показываем SelectBox. SelectColumn. Любой выбранный здесь именно SelectBox выведет SelectedColumn к разке данной. И потом этот SelectedColumn будет использоваться в нижних данных. Col1, Col2. И покажет нам условно здесь в колонке номер один MissingValues и UniqueValues. Это то, что мы выбрали условно какой-то из этих категорий, например. В категориях у нас есть UniqueValues3 и MissingValues. То, что сейчас у нас SelectedColumn, это у нас категория. И дальше по всем этим данным это переходит. Дальше Col2. Это у нас MoscowValueElectronics. Самый часто встречающийся. И ValueColumns просто показывает какие зависимости от ситуации, то, что это у вас будет висеть. От ситуации, то, что это у вас будет числовое значение или категория. Он показывает разные ситуации. Давайте проверим, например, продукт. Это тоже Date. Это у нас, в принципе, тут CustomRange. Покажет медиану и мин. Окей. Давайте теперь перейдем на визуализацию. Следующий этап DistributionPlots. Здесь, в принципе, также мы определяем, что у нас Title. Проверяем данные, если встаете. Здесь смотрим такую ситуацию, что берем SessionState. И потом Selector, именно какой мы хотим. SelectBox DistributionPlots, RelationSlabPlots и CorrelationPlots. И дальше у нас в зависимости от этого будет меняться картинка. Когда наш видстайп DistributionPlots, точнее, наш видстайп Равен DistributionPlots, берутся 7-мумеричные значения с нашего додасета. Выбирается колонка, которую мы только что выбрали. Давайте посмотрим SalesZone, например, в нашем случае. Потом дальше распределяются чарты по двум колонкам. Это Distribution и BoxPlot. Ой, зачем я обновил? Так, по Distribution, в принципе, понятно. Здесь то, что он нарисовал вам за счет плотный чарт. Здесь не обязательно было использовать... Хотя нет, давайте посмотрим, что же происходит. Кредит Хистограмму. Здесь еще один из примеров, что помимо библиотеки Altair можно использовать еще библиотеку плотный. Здесь в зависимости от каких данных, какие вы данные отправили, вы создаете Хистограмму, BoxPlot, Искаптерплот и еще RelationHitmap. От всех данных, которые мы только что получили. Это третья функция, Visualization, в зависимости от вашего датафрейма, которого передали на эту функцию, он вам вернет фигуру именно от библиотеки плотного. И еще то, что мы сегодня прошли, это Altair. В одних из этих кейсов он нарисует такой формат, где при первом формате он нарисует Distribution. Ой, точнее... Да, да, да. При первом он нарисует Distribution. Хистограмма, в принципе, он нарисует Distribution, а в первом формате он нарисует Distribution. При первом он нарисует Distribution, Хистограмму и BoxPlot. В втором случае он нарисует какой-то RelationHitPlot между колонками, которые у нас есть. Ну, CustomerH, это так себе. Давайте возьмем Margin. Нет. Не Quantity, Sales и ProfitMargin. О, здесь прямо видно определенную линию на связи между ними. И при последнем случае он просто нарисует нам CorrelationAnalysis по всему десенту. Что здесь удобно, у вас есть... Можем как-то его уменьшать, увеличивать. Можно даже скачать. И график этот у нас будет представлен в таком формате. Так, и последний prediction. Как у нас там сделана ML-моделька? Prediction сделан в таком формате, что когда он точно знает, что данные подгружены, он берет себе нумериколс, потом дальше... Ой. Мы выбираем, какие мы фьючеры хотим, условно. Features это будет то, что выбрано в MultiSelect. Если фьючеры есть, хоть какой-то, хотя бы один, да? Что-то просто. Опять зависит, да? У всех зависит? Да, видимо. Нет, не зависит. Все. Здесь у нас есть target. Мы выбираем, создаем свой X, создаем свой Y. И дальше просто подгружить, делим как-то через trainTest и делим как-то... И будем ждать до того момента, пока кто-то нажмет trainModel, где пройдет весь этап fit и prediction, и там калькулируются метрики RMSE и R2, потом все это просто высвечивается и показывается еще feature importance за счет моделиков. В принципе, это, наверное, все. Сейчас, наверное, я бы хотел, чтобы вы заменили... Так. Первым делом... Давайте попробуем... обучить модель классификации какой-нибудь. И вот, вот, вот. Нам нужно для этого категориальные данные. Логистическую нет, не получится. В принципе, хотя нет, логистическую... Нет, у нас не получится, кажется. Sales and Merge. Sales and Merge. Не. Так, какой бы этот сет найти для логистической регрессии? Logistic regression data set. А, Titanic же есть у нас, да, точно. Titanic это сет. Попробуем его взять. Data, data, data, data. 61 килобайт. Успенено достаточно. Сейчас, кажется, модель будет неправильно работать, потому что она будет всегда обучать... Ну, может быть, она будет всегда обучать... неправильно работать, потому что она будет всегда обучать... Line regression. Что для нас не есть хорошо. Missing values. Попробуем тогда датанализию сделать. Каких колонках, кстати, есть эта проблема? Range ESV. Есть колонки Age и Cabin. У нас нужно сделать Clean. Drop data analysis. Age. У нас есть Missing values. Делаем Clean data. А там еще, кажется, есть Cabin. А, нет, он уже все это сделал. Survival. Subcp. Embarked. Survival. Median. Mean. Unique values. OK. Visualization. Distribution plots. Relation analysis. У нас здесь связи. Ну... Fairer. Age. OK, прикольно. И correlation. И последняя prediction. В зависимости... Давайте попробуем предугадать его возраст. Получится ли так сделать? Ну, есть ошибки. Но мы хотели с вами заменить на Logistic regression. Так, теперь самое важное. Нам нужно будет сейчас чуть-чуть здесь подкорректировать таким образом, что наш Future, точнее наш Target, всегда должен быть... Всегда должен быть Survival. Это первое. Если когда Target... Target... Там... Remaining calls. Target, Target, Target. Target должен быть Survival. OK. Нам нужно подгрузить... Logistic regression. OK, хорошо. Target Survival. Если Target есть, Target Survival. XFutures. OK, хорошо. Так, в этом датасете... Как он выбран Survival? Survival, Survival. 1.0. OK, все правильно. Мы распределили и после этого обучаем... Logistic regression. Logistic regression. Logreg. XTrain. YTrain. Viperet. R-Score нам здесь пойдет. В основном нужно... Р-Score. Р-Score мы можем не показывать. R-Square, Matrix. Давайте accuracy покажем. Р-Square должен быть одинаковый. Circy. VipTest, Vipret. Давайте еще добавим F1-Score. Добавим третью колонку. Call3. WithCall3. AestometricF1. Вот это сейчас в нашем случае будет район F1. Мы же его не посчитали. На трех сепетах садится. Сейчас он нам должен попросить загрузить туда Datsun. Мы сейчас туда загрузим его. Переходим на Data Analysis. Нам нужно было H. CleanSolid. И вот это все. И вот это все. И вот это все. И вот это все. И вот это все. И вот это все. И вот это все. CleanSolid. В принципе, он должен был по всем сделать кабину. Хорошо, сделаем теперь prediction. PassengerID. Еще раз подгрузим. Data Analysis, Cloud Data, Prediction. Добавляем Survive. Добавляем все остальное. Привет, Survive, Train Model. И опять сломалась. И метрика. Data upload. Clean Data, Prediction. Model Coif. F1. Более-менее. Accuracy. Rscore убираем. Он нам не нужен. Престижно, кстати, почитать. Precision. Precision score. Recall score. Когда нам нужно будет Code 4. Score нам не нужен. Precision. White test, white bread. Recall. Precision. Precision. Future importance. Мы можем убрать. Странно. Почему Model Coif? У вас же не Model называется. У вас Lockreak называется. Да, это правильно. Так, ассистенцию мы правильно вытащим. F1 score. R2 score мы можем убрать. R1 score. Estimetric. Так, Recall мы посчитали. И с последней колонкой мы считаем Estimetric Precision. Так, точно. Мы же и сделали Quaint Data. И еще можно проверить вот такое, что перед тем, как... Хотя нет, можем пока просто... Он еще хочет. Почему? Так. Precision score. А, блин. Recall score. Что он еще хочет? Matrix Mixing. Matrix Mixing. Matrix Mixing. Да, ему, оказывается, так нельзя. Precision. Precision. Recall. Recall. Precision. Recall. Precision. Вроде бы все. Тысячу раз повторили. Загрузили. Дата анализ очистили. Prediction. Выбрали. И что он еще хочет? Inform StateFrame. Extract Futures. Не, все правильно же. Importance Atachling. Array must be one-dimensional. Странно. Следите с модели. Они one-dimensional. Remaining goals. OK. Котик коэффициентами возможно. Ну ладно. Обойдемся без зататок сегодня. OK. И как мы видим, очень простенькая моделька, которую мы обучили за счет нашей логистической регрессии. Применили все эти данные. И получилось у нас, что, в принципе, Recall у нас такой. Precision такой. И наш... Ой, да. Recall у нас 0.5. Precision у нас 0.77. F1.4. F1.4 у нас 0.77. F1.4.0.67. Это 0.60. Это примерно как раз таки среднее, геометрическое среднее между Recall и Precision. И accuracy. В принципе, все это на сегодня. Если у вас вопросы к стримлиту, или тому, что мы делали. Sorry, что так получилось. Очень такие маленькие ошибочки, В принципе, очень интересный опыт в плане создания целого веб-приложения, в котором можно заниматься AML. В качестве презентации выглядит очень круто по мне. Весь этот код у вас есть? Если надо, я могу обновленный еще закинуть. Или вы сами попробуете сделать? Обновленный тоже отправьте, пожалуйста. Хорошо. OK, сейчас тогда. Так, в принципе, есть у вас вопросы? Потому что мы с вами встретимся только теперь в следующем году. Извините, пожалуйста, у меня вопрос есть касательно предыдущей лекции. Там во время асинхронных операций вы говорили, что конфигурации FastPи-приложения было сделано не так. И из-за этого они запускались, я так понял, в двух потоках, и модель не выкидывала тот exception, что... Да, да, да. Можно узнать, вы говорили, что вы обновленный файл закините? Я его сделал, но, к сожалению, я его закидывать не буду. По причине того, что там, чтобы это сделать правильно, именно в том формате, который я хотел, к сожалению, там будут применены чуть-чуть те библиотеки, которые мы не проходили. Но в качестве ознакомления я могу, наверное, завтра закинуть. Чуть-чуть его дорогаботать и более упрощенно в формате сделать. Если завтра закину, нормально будет? Да, да, спасибо большое. OK, хорошо. Ребята, есть у вас вопросы по FastPi, StreamLit, Docker, по домашкам? По домашкам, скорее всего, сегодня домашку дадут. Вопрос такой, в чем между ними разница? В чем разница между StreamLit и FastPi? Если можно выбрать одну из них, то что лучше будет? В зависимости, можно комбинировать. Да, можно комбинировать. Потому что, условно, давайте представим такую ситуацию, что ваш FastPi... Давайте теперь не будем представить ситуацию, что мы сделали модуль, который распознавает кошек и собак. В качестве примера. И мы хотим, чтобы наша модель предсказывала, что это здесь показана кошка или собака, или если это не кошка и не собака, то пусть им скажут, что это ни то, ни другое. То в этом случае на StreamLit у нас есть возможность описать, как будет это изображение попадать на FastPi. А FastPi вы можете сделать такое приложение, которое будет подключаться к библиотекам, работать с видеокартой и быстро даст ответ вашему StreamLit, который получит ответ от этого endpoint и покажет вам результат. Типа их можно скомбинировать, в качестве примера. Можно делать чисто на StreamLit, можно чисто на FastPi сделать. Инструментов очень много. Говоришь, что лучше, что-то хуже, к сожалению, нельзя. Просто у каждого есть свое правильное применение, где он в чем-то лучше будет. Эти инструменты, к сожалению, не сопоставимы, чтобы их сравнять, потому что у каждого предназначение другое. Потому что FastPi это целая библиотека, фреймворк, чтобы делать endpoint. А StreamLit это больше, наверное, про создание UI и создание веб-страниц. Еще какие бы просто, ребята? Наша следующая лекция будет получаться с B-Borus? Нет, нет, нет. Следующая лекция будет преподавать Артем. И дальше у вас начнется модуль по NLP. Сначала у вас будет две лекции еще. Потом после этого у вас начнется Project Wiki, Demo Wiki. И потом у вас начнется огромный модуль по NLP. И мы с вами, типа со мной и B-Borus, мы увидимся с вами только в следующем году, когда будет начаться модуль по Computer Vision. Что мы будем в Computer Vision проходить? Так, мы будем это... Open, библиотеки от OpenAI, PyTorch, TensorFlow. Мы будем проходить... Нет, в таком формате, типа именно сказать, что мы будем проходить PyTorch или что-то такое. Типа сложно сказать? Давайте я точно посмотрю именно, что мы будем проходить. Давайте я точно посмотрю именно, потому что мы с CV разделились с B-Borus. Если прям точно сказать... В CV мы будем проходить... Одна неделя будет посвящена классификации, детекции и сегментации объектов. Vision Transformer. Потом будем проходить ГАНы. Как сгенерировать изображения в олдскульном формате. Как генерировать изображения из текста. Как текст генерировать из изображения. Image to Caption, Caption to Image. Будем проходить Stable Diffusion. Будем проходить распознавание лиц и Face Spoofing. Будем проходить как бороться с тем, что вас кто-то пытается взломать. Пройдем Zero Shot Learning, когда модели обучаются, типа за счет условно, zero shot. И еще пройдем, типа, Billetech. Попробуем поработать Mid Journey, типа From Text to Video, From Text to Image. Вот в таком формате. А вот эти... Извините, что перебил. Какие мы? Что еще? Image Processing, вот эти тулы, фильтры, типа от шума, чтобы извали... Этого не надо, да? Не, это само собой, типа, такого рода вещи мы тоже пройдем. Dilation, там, допустим. Ты имеешь в виду гауссовые фильтры, нойс фильтры, типа такого? Или что именно? Когда мы один объект от другого объекта отделяем, там, допустим, у него же... границы могут пересекаться, и там, чтобы эти границы... А, это сегментация. Сегментацию, да, пройдем. Спасибо. Типа, там будет три основных модуля у CV. Точнее, нет, шесть модулей. Не шесть модулей, а даже шесть. Каждую неделю будет посвящена определенным вещам. И там прям... Ну, у NLP тоже примерно где-то... У вас будет две с половиной, две с половиной... Ну, прям прилично будет. Начиная с ноября, заканчивая... Ну, два месяца вы точно будете сидеть на NLP. Ну, прям очень много. И потом, наверное, вы будете работать, и как вы будете преподавать Артем и Адлета. В принципе, наверное, все. Если у вас какие-то вопросы... Ну, это вопросы все. Ну, окей, тогда. В принципе, все на сегодня. Я завтра постараюсь отправить вам обновленный документ. Отправит Айдана. И если у вас будут вопросы относительно вашего проекта, можете писать. Попробую помочь. Хорошего вечера. Давайте, ребята, увидимся через несколько месяцев. Ну, точнее, у нас будут еще демо-выки. Так что в тот момент еще увидимся. Извиняюсь, извиняюсь. Да, конечно. Два месяца не увидимся. Не хочется вас обождать. Хотел спросить, какие книги вы можете посоветовать? Для прощения. Или можете обдумать, а потом группу потом... Нет, у меня есть книга, которую всегда советую. Там две книги есть. Первая – это Mathematics and Statistics Modeling. Я не помню названия автора. И еще вторая – Bishop Deep Learning. Наверное, самая стандартная книга. А вы в каком формате хотите их просчитать? Там просто очень сильно... Отличается очень много вещей. Такой формат. Вас плохо слышно. Вы какой именно формат имели в виду? Просто есть книги, которые очень сильно теоретически углубляются. В них все принципы и концепты объясняются. А есть такие простые, где от ORL, которые можно стандарта взять, PyTorch для Image Classification, посмотреть его, там нормально объясняется. Давайте я сейчас напишу сюда. Statistical Modeling... Сейчас я прямо отправлю, подождите, я ее найду. У меня она есть. Одну минуту. Statistical Modeling. Есть. Так, нет, нет, нет. Так, нет, нет, нет. Statistical Modeling. Вы можете группу Valve 1 в Telegram написать? Да, да, сейчас я... Да, оказывается, ее не так просто найти. Она у меня зачарилась. Окей, хорошо, я ее найду. А вот касательно проектов можно ли выбрать именно именно в отрасли QR5-спасности? Да, конечно, вроде там очень открыто. Просто чтобы, самое главное, чтобы могли показать, продемонстрировать те знания, которые вы получали, и выполнить проект, который вы захотели. А что вы хотите именно в киберзву безопасности, чтобы просто самого интересного? Именно использование нейронных сетей, именно, например, банков второго уровня можно использовать. И искусственный интеллект именно в распознавание голоса можно использовать именно в расследовании. Это есть много жестые идеи, которые даннуют, рассматриваем именно в рамках этой киберзву безопасности. Можно ли садить для такой проект, который... Так, посмотрите, там, ребят, на окончание модуля ML-а, кажется, там такой проект будет сложновато. Потому что у вас будет буквально полторы недели, что ли, на его выполнение. И это, думаю, будет сложновато. Но давайте, анонс по проектам будет на этой неделе, кажется. Или на следующей. Скажите, на следующей. Тогда точно скажут. Просто сейчас у нас именно окончательный концепт пока не решен, потому что там это все еще обсуждается, и я пока не могу сказать вам. К сожалению. Хорошо. Ну, все хорошо тогда, ребят. Давайте, пока. Или у кого есть вопросы, давайте. У меня есть вопрос. Я отстрел от топиков, и можно потом задавать вопросы вам. Ну, лично, по играм. Да, конечно. Просто не ночью, главное. Хорошо, спасибо. Не вопрос. Не слышно? Да, слышно. А есть ли какие-то у вас наставления, советы для студентов, которые хотят в этой области работать? Что нам делать? Изучать математику. Да, ну, это изучаем уже. Знает такой обширный вопрос, конечно. Интересно послушать ответ. Просто, ну, наверное... Делать постоянно. Как-то на первом курсе я был на... На втором? Нет, я просто одну историю хочу расправить. Да, извините. Именно. А, нет, вот да. Будучи на втором курсе, я был на очень крутом этапе в Питону, где выступали Альята и еще Ян так скажу, если ошибаюсь. Они только в Казахстан тогда перебрались. Ну, типа первый офис открыли. И тогда, не помню, кто, кажется, Арман Сулейменов сказал очень такой прикольный совет, который применим до сих пор и практически ко всем случаям. Типа, если вы хотите какой-то, условно, в компьютерсайенсе вне зависимости от именно какой отрасли самого компьютерсайенса, если хотите развиваться, то можно сделать это очень просто. Потому что очень такая тупая, жесткая изубрилка приводит к тому, что вы демотивируетесь и плохо... Ну, там материал не так сильно понимается. По этой причине, самым простым, реально применимым советом здесь является то, что можно попробовать брать какие-то очень интересные темы и просто рандомно как бы объяснить. Условно пальцем в нема выбрать какой-то очень простой проект, ну, именно из жизни, да, и попробовать его реализовать в каком-то мини-пэд-проекте. И обычно создание этого пэд-проекта приведет к тому, что вы начнете задавать вопросы и будете пытаться решить эти вопросы изучением. И это применено также к Kaggle, потому что Kaggle, в принципе, отвечает на этот вопрос относительно того, чтобы именно в ML-ке вы могли получить хороший объем знаний и при этом практической применении, то, наверное, это как раз таки Kaggle. Самый основной совет, типа попытаться Kaggle-ить, если у вас есть время и возможности, потому что за счет Kaggle вы сможете понимать какие-то домены задачи, это первое. Понимать технологии, которым можно применять, понимание терминов и понимание, в принципе, как можно тюнить модельки. Вот это самое такое. И знание, которое вы получите от Kaggle, оно очень применимо на работе, типа понимание очень базовых концептов. Наверное, такой совет бы был. Все, понял. Спасибо большое. Ребят, если все, то пока. Хорошего вечера вам, хорошей недели. Спасибо большое, до свидания. Редактор субтитров А.Синецкая Корректор А.Кулакова Корректор А.Кулакова