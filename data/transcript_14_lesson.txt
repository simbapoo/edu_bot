 Так, всем привет! Как меня слышно? Откажите, как меня вид? Отлично слышно. Брат, все отлично слышно. Брат, все хорошо слышно. Супер, супер, супер. Меня зовут Артем, фамилия моя Ричко. Я являюсь ментором на курсе инжиниринга. Мы вроде с вами знакомились наводной. Я буду ввести у вас последние два занятия модулей эмэля. И буду ввести еще, не только помню, полкурсы по NLQ. Так, последние два занятия. Сегодняшняя 15-я, потом у вас начинается Project Week, потом у вас буду ввести полмодули NLP, а остальные полмодули будут от лет ввести. Супер. Давайте вы сначала расскажете, что у вас было на предыдущих занятиях. Хочется ваш feedback услышать. Давайте формат сделаем. Поднимайте руки и расскажите в трассе предложения, что вы прошли. Есть желающие? На прошлых, там был Docker, FastTap, Streamlit. Потом еще. По NLQ там получается много еще было. Допустим, Model Evaluation. Ничего еще. То есть, самимодули не тренировали, что ли? Almasbek, давай. Сначала мы проходили IDE, чистку данных, Data Engineering, Future Engineering, после чего перешли на Emailing. То есть, тренеры рассмотрели в основном основные части алгоритмов по машинному обучению, то есть, линейная регрессия, случайные листры, решения и так далее. Были небольшие квизы. SQL еще проходили. Две проектные работы выполнили. Одну по очистке данных, по IDE, второе по очистке данных, плюс обучение, плюс подбор гиперпараметров. И потом дальше вот то, что Muslim сказал. У нас был FastTap и так далее. В целом все отлично было. А AutoML начали проходить, нет? Что, простите? Начали ли вы проходить AutoML? Вроде AutoML явного не было. Это жалко. Одна. Извините, Артем, а что вы подразумеете под AutoML? Просто мы до этого, например, нам показывали библиотеку, где мы могли визуализировать данные на сайте, через браузер, условно. И там же мы могли, например, загрузить какой-то датасет, и сделать же на этом же сайте, чем в несколько кликов, train, test и так далее. Вот это вы имеете в виду, AutoML? AutoML это условно библиотеки, которые часть моделирования забирают на себя. Это такой определенный сервис, который как минимум проводит оптимальные тренировки классических моделей машинного обучения, словно там двадцатки и сразу проспалидацию делает, сразу нормально подбирает параметры через оптиму и условно выдает публичку в сравнении, так чтобы не руками рендофорост писать и прочее. Очень удобная штука для того, чтобы провести быстрый эксперимент. Ну ладно, если не проходили как новое, то не проблема это показать, как делается. У нас еще занятие этим еще и займемся. Окей, всем архимед, кто рассказался про прошлые занятия, надеюсь, этого будет достаточно. Надеюсь, не собрали все рассказания. Так, теперь давайте познакомимся еще раз немного. Потом расскажем, что будет на этих двух занятиях и с вами придем к своему занятию. Сегодняшняя тема у нас посвящена Intro to ML Dev, Machine Learning Development. Сегодня мы с вами обсудим такие четыре топика, как вообще в целом, что такое Data Science, в чем отличие с AI, artificial intelligence, что такое Machine Learning, как это все отличается, или это вообще все одно и то же. Поговорим, как он делается, как его правильно делать, как его оптимально делать. Поговорим про историю искусственного интеллекта, условно почему оно как зарождалось, какие там проблемы были и почему оно сейчас в таком состоянии, как есть. Потом поговорим, из каких частей у нас будут с вами состоять любые Data Science задачи, любой ML-проект, любой AI-контюнинг или все, что угодно в этой сфере. Поговорим, как AI-задачи должны выполняться, какие там есть у нас с вами по теме и подводные камни мы можем ожидать. И поговорим, как это все делать эффективно. Это будет супер полезно для тех, кто начинает любые новые проекты, так чтобы проект делился не полгода, а можно было разовершить за раза три меньшее по времени, чтобы эта волна занимала. Вот формат проведения. Давайте, если у вас есть какие-то вопросы, то всегда поднимайте руку, либо пишите в чат, я периодически буду смотреть чат. Если также я буду видеть поднятые вопросы, когда я закончу мысль или кончу тему, то когда может быть спросили вживую, так чтобы не получалось что перебивать. И просьба всем следить за микрофонами, чтобы мы не слышали то, чего бы вы не хотели слышать, чтобы мы услышали. Потому что иногда будут очень забавные моменты. Супер! Кратце про себя расскажу. Очень люблю про себя рассказывать. Не знаю, почему. Если хотите, можете зайти на LinkedIn, там какая-то часть моей профессиональной деятельности отражена. Точно не вся. Если вы едете к Кратце и говорите про себя, я сейчас занимаю руководящую должность в группе компании Sun Generation. У меня штат из сегодняшнего дня из 18 человек. У меня есть команда ML Engineering, Data Engineering, Data Marker и вот этим всем я руковожу уже два года. Сам я начинал путь с самого нуля, условно, будучи стажером в году 2016-2017. Постажировался в McKinsey, в Яндексе, в Cosdream. Потом чуть-чуть работал в лаборатории Яндекса, в лаборатории вышки. Работал в трейдинговой компании, которая делает High Frequency Trading, высокочастотную торговлю. Вся алгоритма для торговли. Работал в рекламе, там, условно, баннерную рекламу, которую вы сейчас видите. Она до сих пор считается по моему алгоритму условно в двух-третьих случаях на сайтах, которые вы можете увидеть. Все еще. Потом работал в Cosdream, в какой-то часть. Работал и работаю в Ceylon G. Это компания, когда-то была одним целым с Ceylon G, но сейчас в Cosdream и Ceylon G точно разные вещи. Вот. Потом работал в стартапах разных, в американском стартапе, в финском стартапе работал, в российском стартапе работал. Сам периодически стартапы открываю и, к сожалению, пока только закрываю их. Короче, как-то так. Несловно я прошел путь на текущий момент от стажера, бета-санитиста, эмельщика. Там был дженом, потом был исследователем, как это называется, applied scientist или email researcher. Вот. В лабораториях. Писал статьи чуть-чуть научные. У меня две статьи пока есть. Потом работал в middle, emel, senior, потом работал тех-ледом. И сейчас являюсь ледом над ледами. Несложно, если так говорить. В общем, как-то таковаться про меня. Если добавляетесь в NPDIN, будем дружить друг другу, и чуть-чуть завидовать успеху других людей. Вот. Если какие-нибудь вопросы на данном этапе. Если есть, поднимайте руку или пишите пчатку. За свой жизнь, наверное, реализовал порядка 50 email-проектов, которые до проды дошли в какой-то степени. И, наверное, в раза 4 больше, которые до проды не дошли. Поэтому, можно сказать, здесь точно для меня ничего нового особо нет. Попросы нет? Окей, ладно. А кто может сказать, что такое является Data Science? Или Machine Learning, или Artificial Intelligence? Поднимайте руку, давайте попробуем с вами поговорить. Как вы понимаете, что это такое? Супер, Алмаз Мек? Ну, это наука от данных. По сути, Data Science в данном случае. Извини, изофоновый шум у меня просто гостей нет. Поэтому я его сказал на слушать. Наука от данных, которая изучает все, что связано с данными. Data Science входит там три ветки. Это Data Engineering, ML Engineering и Data Analytics, насколько я могу понимать и знать насчет именно в этих ветках. То есть, все, что связано с данными, на основе данных обучаются модели машинного обучения, на основе данных делаются какие-то предикты, на основе данных делается аналитика, на основе данных. Все, что связано с данными. По Data Science я могу сказать. Мне кажется, неплохое направление вы задали. Вы хотите что-то добавить? Или поправить? Нет, споль, споль это пошли. Так. Так, нет никого, да, кто бы хотел что-то добавить. Интересно, обычно в других курсах мы больше общаемся. Ну ладно. Да, давай Расул. У меня сошло. Так, вот первый человек, который был ранее меня. Можете говорить. Да, я там чуть раньше был. Окей, у меня просто человек с микро было. Ну, я с Ломовиком соглашусь, да, по Data Science. Он просто еще кажется по AI вроде не сказал, да, и по ML. Ну, ML по сути и AI. AI можно воспринимать как такой получается sunset, а ML это будет его subset. AI, то есть мы создаем какие-то интеллектуальные системы, которые выполняют задачи, требующие человеческого интеллекта. Ну ML как бы мы как раз такие, ну он фокусируется на обучении вот компьютеров предсказывать или классифицировать данные. Все, это то, что могу сказать. Окей. Так, еще вроде про то, вот. Так, теперь я через ML и AI могу сказать просто различия. Как уже обретен, уже день, уже сказали. Ну, по крайней мере, машинно-обучение искусственного интеллекта уже как-то честно связано, но имеет разные цели и методы. По крайней мере, ATK-Line Intersections это, ну, это как, знаете, ориентированность на создание системы. То есть он выполняет задачи, обычно требующие человеческого интеллекта, можно так сказать. А вот ML вот больше всего как подножиста и идет, которая позволяет системе учиться на данных, а потом без явного программирования. Ну, в общем вот так говорили. Окей, ладно. Короче. Это самый интересный вопрос, на который сколько специалистов вы спросите из области, столько вы получите ответов. Нет нормальных определений. Все постоянно путают эти определения. Легче всего воспринимать, что это одно и то же. И это все одно и то же. Вот. Могу рассказать, что когда-то это все называлось статистишнанс. Это были статистики люди. Потом статистикам добавилась в середине прошлого века такая штука как компьютер. И статистикам нужно было условно знать немножко в компьютерсайенс. Немножко долго до этого доходили по нормальному. Где-то к 70-м годам нормально пришли, чтобы можно было что-то статистикам подночитать. Вот. В это время появилась логика нейронных сетей. В 40-х годах еще была, даже в 30-х. И потом потихоньку стало понятно, когда начали практики применять это все условно в следующий уровень оптимизации, но появился такой термин как дейнсайенс. Все начали понимать, что работа с данными это вообще отдельная наука. И соответственно, если мы хотим что-то из нее делать, то надо понимать какие-то нюансы, которые есть чисто в данных, и международные различия могут быть, и тому прочее. Вот. Если в кратце говорить, то EI это условная идея пойти создать замену человека. Что-то зациклоинтерректуальное, который умеет думать и тому прочее. Если говорить e-mail, то это методы, которые используются для какой-то оптимизации. Вот. ChargePT это тоже e-mail. Data Science это по работе с данными. Но мы понимаем, что любой e-mail проект это все работа с данными. Соответственно, можно назвать это все одним и тем же словом. Есть люди, которые немного психуют, когда условно мы им всю какую-то работу дату аналитика можем назвать, что он занимается искусственным интеллектом. Но легче пока про это вообще не париться, потому что у него нет нормального этого определения. И все это всегда порадоваться принимают. Короче, термины это значит примерно одно и другое. Теперь давайте поговорим, какие основные задачи мы выполняем. Основная задача в DS, VIA или e-mail проекте, она простая. Она что-то улучшить. Есть такое понятие, как уровни развития проекта. На сегодняшний день существует несколько уровней. Первый уровень супер простой. Кто может сказать, если мы хотим решить какую-то задачу, каким самым простейшим случаем мы можем ее решить, если мы хотим, чтобы она всегда в процессе решалась. Наверное, сначала определить проблему и откуда она появляется, как бы правильно ее описать. Как-то откладить решение, может, какое-то простое. Инициировать его. Вы хотите соптимизировать его сразу. Самый простой кейс, это просто дать людям, чтобы они что-то сделали. Условно, давайте возьмем пример. У нас есть компания. Мы, например, супермаркет. Мы продаем товары. Перепродаем товары, покупаем, сами не производим и продаем. Мы начали развиваться. Мы видим, что у нас несколько точек. И нам понадобился колл-центр, чтобы мы могли отвечать на вопросы клиентов. Давайте, например, колл-центр и будем насмаскивать. Вот у нас есть колл-центр. Первое, что мы можем сделать, для того чтобы отвечать на эти звонки, просто сделать много номеров, посвятить людей, и пускай они отвечают на звонки. Ну, как будто это немножко плохо. Почему это плохо? Потому что мы не даем людям, условно, сценарии, популярных вопросов, которые нам часто задают, сценария ответов, которые можно давать. И как будто нам хотелось бы, чтобы у нас было не миллион номеров, а нам хотелось бы, чтобы был условно какой-нибудь агрегатор, который принимает звонки и просто выдает свои номера. Вот появляется у нас с вами второй уровень. Когда условно сходишь с ручного труда, мы начинаем с вами делать первую оптимизацию. Обычно те, кто... Я знаю, что здесь много ребят, которые работают инженерами. Так, я оптимизацию передаю. Вот тут обычно подключается IT. Оптимизация, в общем, она двумя вещами делается. Первое, там появляется начальник, условно, начинает налаживать процессы, как это все принимается, как звонки должны приниматься, там и прочее. Но если мы здесь с вами, люди в основном из IT работают, то условно вот здесь появляется первая оптимизация. То есть на формате Call Center мы с вами ставим какой-нибудь приемник, агрегатор. На него сыпятся все звонки, и потом он дальше распределяет, кто получит звонок из свободных менеджеров, сейчас нет звонка. То есть мы сделали такой первый уровень оптимизации, потому что мы облегчили сильно работу, и благодаря этому обычно в среднем условно 100 звонков начинают эффективность возрастать до 304 звонков, которые можно принять, и они не теряются. Потому что в первом этапе условно нам два параллелянных звонка идут на один номер, один номер принадлежит одному менеджеру, он принимает телефон, а второй остается без ответа. Кто-то из них остается без ответа. На оптимизации мы как бы не теряем эти параллельные звонки. Потом вступает третий уровень, наступает момент аналитики, подключаются обычные такие люди, как бизнес-аналитики, системные аналитики, может дата-аналитики, если компания понимает их потребность, и начинается оптимизация. Что начинает оптимизировать? Начинает анализировать, насколько хорошо, например, менеджеры Call Center отвечают на вопрос, насколько хорошо решаются проблемы, с которыми звонят в Call Center. Начинается оценивание, например, работы каждого специалиста, условно, грубил он или не грубил. Начинается какое-то распознавание, воспринимается, что потом все звонки мы могли хранить в текстовом режиме, искать условия, к каким-то ключевым словам и тому прочее. То есть появляется аналитика, и процесс еще оптимизируется. Есть четвертый этап, который проходит. Он называется, когда начинается автоматизация репрессионального труда. Полная. А именно здесь добавляется email. Машинное обучение. Где принимается email, мы на входящую линию ставим, например, бота, который общается за человека. Соответственно, нам нужны технологии распознавания речи, нам нужны технологии синтеза речи, распознавание речи, чтобы звуковой аудиосигнал, который принимается, чтобы он конвертировался в текст, по тексту мы, как минимум, могли простейшие кейсы делать, например, отправлять его в какие-то ветки сценария, по которому идет общение, либо, как минимум, в лучшем случае просто определять, на каком языке с ним говорить. Второй кейс. После того, как мы получили запрос, мы должны его проанализировать, выдать какой-то ответ, и этот ответ должен быть озвучен. Мы можем, конечно, его озвучить, как это делалось примерно в 90-е или в нулевые годы, выбрать и говорить очень роботизированным языком, но хотелось бы нам говорить словно люди говорят, да, интонацию, ударение ставить, держать манеру речи и прочее. Для этого появляется синтез речи. Соответственно, синтезом речи это тоже эмельный проект, который автоматизируется. Плюс нам бы хотелось бы, чтобы мы понимали у каждого клиента, у каждого звонящего в концентра свои какие-то потребности, какой-то свой плюс-минус уникальный вопрос. Нам бы хотелось знать условно какую ветку его, понимать смысл его вопроса, понимать из сценариев ответов, которых у нас есть, куда лучше пойти и тому прочее. При этом нам нужно уже модели, которые будут понимать естественный язык, понимать намерения. Этот термин называется Intent Classification, соответственно, уже третья модель и так далее, так далее. И мы условно создаем вот такого аналога человека, который будет общаться. Но он будет состоять в том, что оценивается из кучи<|ru|>zęмельмоделей. Будет отдельная модель, которая у нас будет располагать речь, отдельная модель, которая будет синтезировать речь. Отдельная модель, которая по тексту выбирать для сценария, в какую ветку пойти, потом всегда перепроверять, в нужной ветке мы находимся или нет. Потом нужно модель, которая будет выбирать один из лучших вариантов ответа, в синтез речь происходит, ответ получен и так до тех пор, пока мы с вами не остановимся говорить. И всегда нужна на всякий случай модель, которая предскажет, если клиент недоволен, чтобы мы переключили его на обычного человека, на человека у менеджера, полцентра менеджера. И есть пятый уровень, какой пятый уровень автоматизации развития любого проекта. Так, четко-четко писали. Тест. Ну, тест пройден, сообщение получено. И я. О, совершенно верно. Да. И третий кейс, это мой, и пятый уровень, это географичная ясность. Кейс, который будет сразу генерировать ответ, желательно быть модальный, ну, как вы сказали, на чат GPT, да, вот если его посадить на колл-центр, дать ему какой-то контекст информации про компанию, в идеале подготовленные примеры и вопросы в ответах. Ну да, это как раз кейс, когда можно очень хорошо заменить колл-центр просто одной моделью и контакт лично серверов. Вот. Мы должны понимать, в каких уровнях применяется датсайкс. В первом уровне, как думаете, применяется датсайкс или нет? Ну или ML, либо AI. Нет. Ну если это, конечно, очень плохой инженер, ML-инженер или AI-инженер, то применяется. Просто как рабочая лошадка. Вот. На втором уровне, как думаете, применяется? Я и ML, либо D. Да, применяется. Так, а можете продолжить мысль тогда, Дилон Радка, применяется? Ну, допустим, для… Так, я вот так поменяю. Еще раз. Ну, допустим, для оптимизации процессов каких-то, ну вот автоматизации он должен, я думаю, как-то эти звонки все определить. Нет, смотрите, на в том этапе вам пока не надо определять звонки, нам надо пока принять в это слово звонков и нам надо его словно выдать свободному менеджеру. Здесь тоже, если применяется DSML-специалист, то, если честно, он будет применяться просто как инженер, как софтвер инженер, либо сетевой инженер. И, как мы понимаем, обычно датсантисты, мальчики или AI-инженеры, они плохо пишут ход, если честно, очень грязно, мало кто знает стандарта написания кода, хотя бы даже в их любимом Python'е, мало кто может рассказать, что такое PIP-8, как по нему нужно считать, как по нему нужно писать код, опустить документацию и тому прочее-прочее. Вот, поэтому на втором этапе, на автоматизации, они особо не упрямляются. Вот, начинается третьего этапа, когда нам нужно оптимизировать, улучшить после того, как произошла автоматизация, там действительно уже можно нанимать датсантиста на третьем этапе, либо датсаналитика, хотя бы датсаналитика на начало, датсинженера для начала, и можно датсантиста нанять тоже, человек, который будет проверять гипотезы и играться с моделами. Вот, четвертый этап, это уже у вас должна быть выстроенная мэль-команда, которая, либо у вас должна быть нормальный процесс, если вы с аутсорса забираете эти модели, но у вас должно все равно, это в девелопменте, все супер сильно хорошо учитываться, да, если у вас даже и нет мэл-команда, если вы на четвертом этапе, это, конечно, странно, но это возможно, как бы другие бизнесы могут аутсорсить все из-за этого, поэтому это можно, да, собрать, но тогда надо будет точно эмалопсию осуществлять, чтобы мы понимали, насколько качественно это работает. Вот, на пятом этапе, так вообще, тем более. Вот, итоги, суммитом, нормальный DS-DAR-аналитик, он обычно появляется с третьего этапа, на четвертом этапе он строго обязателен, на пятом этапе тоже сам. Вот, теперь мы понимаем уровень развития проектов или уровень автоматизации проектов, это так тоже можно назвать, уровень давайте, развития автоматизации проектов. Соответственно, когда мы, теперь мы можем рассмотреть любую проблему, любую IT-компанию, на том, на каком уровне она находится, чтобы было понимание, какой специалист нужен или какое количество, или какую работу надо будет выполнять. Если мы говорим про третий этап, там, скорее всего, все будет очень плохо с данными, там, скорее всего, не будут настроены процессы сбора данных, там будет, скорее всего, неоптимальная лапа и LTP. Вот, если мы про баз данных говорим, то на этом этапе очень будет много работы с муссовыми данными, очень много надо будет идей делать, очень много проверять первых нулевых гипотез. Если мы, условно, устроимся в компанию, которая есть четвертый уровень, то мы с вами уже понимаем, какие проблемы будут сталкиваться в работе этого человека. Здесь уже тогда будет выбор оптимальных моделей, здесь будет уже оптимизация рентайма, либо инфраенса модели, здесь уже будет уровень проверки гипотез, какой-то ресерчер, или прочее. На пятом уровне так вообще все интересно, здесь уже совершенно другой уровень проблем. Здесь уже будет очень много проблем, связанных с инфраенсом, соответственно, с оптимизацией рентайма. Здесь уже будет очень много проблем с проверкой гипотез, с промд-инжинирингом и т.д. и т.п. Вот, это если мы будем говорить про уровни развития проекта. Не всем проектам нужны все пять уровней. Где-то хорошо на первом уровне оставаться, например массаж. Вы бы не хотели, я думаю всем приятнее получать массаж не от кресла массажера, а от другого человека. Либо разговоры с людьми, мне кажется, мало кто кайфует, если даже он и общается с VoiceMod, chatgbt. В некоторых вещах очень важно. И обычно выход на любой новый уровень сулит больше денег компании, потому что сулит больше конкуренции преимущества с другой компанией. Вот, как-то так. Это условно типа фильтр, через который мы можем воспринимать любой проект, на каком он уровне относится и в какой части, где обычно нужен иинжиниринг. Нам соответственно нужен в основном на пятом этапе, где хотят оптимизироваться. Но это не значит, что, например, проект, который находится на втором уровне, он не может смотреть в будущее и начинать засматривать на генеративный элит. Поэтому он может начитать делать какие-то эксперименты. Но знаете, что у вас будет дофиги еще проблем, которые не решены на третьем и четвертом уровне. Вот, как-то так. Если мы будем говорить про разные проекты. Вот, если мы будем говорить про эмэльные задачи, то существует три направления эмэльной задачи, по которому мы можем решать. Первое направление называется Supervised Learning, либо обучение с учителем. Это когда у нас есть данные, и у нас есть либо классы, либо какое-то непрерывное значение в каждой строке, которое мы хотим с вами предсказать. Яркий пример. Мы хотим с вами предсказывать цены на акции. На вход у нас с вами получается текущая цена, либо какое-то количество предыдущих цен. И мы хотим предсказать, что будет там, например, через час. Какая будет цена на акцию, либо какой-то актив. Для этого у нас с вами есть размеченные данные. Мы можем пойти просто в историю и собрать, например, берем все цены на акции на час и добавляем к ним как фичи, условно, цены за предыдущие часы. Вот у нас будет авторегрессионный подход для предсказания цены на акции. Обычно различают два вида задач Supervised Learning. Я уверен, что вы с этим знакомы. Просто повторяем. В случае, когда у нас с вами непрерывные предсказывания фактор и когда у нас с вами дискретно, нужно выбрать из какого-то набора. Их еще называют регрессия и классификация. Вот пример задач регрессии, например, предсказания цены акции. Пример задачи регрессии. Можно перефразировать первую же задачу, например, остаёт ли совершать сделку или нет. Предсказать вот эту сделку или вот эту акцию сейчас нужно покупать или нет. Если мы предсказываем покупать или нет, то это уже будет у нас самая задача классификации. Вот. Также есть направление в Data Science в МЛе как обучение без учительного, английском оно называется, Unspirited Learning. Это когда у нас с вами нет размеченных данных, но мы с вами хотим достичь несколько целей. Первая цель, которую мы хотим сделать, мы хотим как-то преобразовать текущиеся данные, чтобы мы их использовали, например, для визуализации. Например, мы с вами работаем на заводе и у нас есть, например, цех по выплавке стали. У нас есть огромное количество датчиков, которые следят за температурой, давлением и тому прочим на всех этапах, условно, получения стали. И мы с вами хотим, например, два года работы этого цеха визуализировать на простом графике, чтобы мы видели, когда было плохо, например, когда что-то перегревалось или из-за высокого давления выходила спроса и когда там условно было хорошо. Что можем сделать? Условно наши тысячу датчиков, это наши тысячу колонок, мы эти тысячу колонок можем превратить в две колонки, да, такой подход называется Dimensional Reduction и сможем с вами на таймплоте и в трехмерном графике и показать, как у нас с вами менялось условно показания наших датчиков. И соответственно, когда все было плохо, то будет очевидно по этой маленькой размерности графики. Вот, это первый кейс. Второй кейс, когда у нас с вами нет размеченных данных и мы хотим подчеркнуть какие-то инсайты, есть такое понятие как пластеризация. Это кейс, когда мы с вами не сжимаем количество колонок, а мы с вами пытаемся навесить на похожие строки какую-то группу, сгруппировать их по похожести. Для чего это используется? Самый яркий пример, это используется в рекламе. Если вы получаете рекламные смс, то знайте, что вы были кластеризованы и сегментированы. Ваш кластер людей, как вы и похожих на вы изучили и как это описали условно, это люди среднего возраста, женщинского пола, которые живут в таких районах города, которые обычно покупают такое-такое время. И это есть, например, сегментирование. Вы были сегментированы, выделены в одну группу, которую на основе данных создали, основы которых не создали на уровне знаний. Например, я хочу, чтобы у меня навешался класс людей, которые очень красивые, супер умные. А которые были созданы от данных. То есть условно, были какие-то статистика, были накопленные данные у покупателя какой-то компании, и потом их кластеризовали, описали эти кластера, и потом дальше эти кластера использовали для рекламы. Вот это тоже кейс обучения без учителя. Как еще частный кейс обучения с учительом, который здесь не как кейс, а как подкатегория, который здесь не написан, можно обозвать его мачингом, либо можно его обозвать еще рангингом. Это все, что касается поискового выдачи. Например, как работает Google? У него проиндексированы миллионы и миллионы сайтов, и когда вы пишете поисковую строку какой-то запрос, он пытается выбрать вам наилучший сайт, который подходит. Поскольку никогда уверенности у Google нет, что он вам предложит топ-1 сайт, то он дает вам много-много отсортированных страниц. И вот сортирование вот этих страниц, типа какая будет первая выдача, какая вторая выдача, какая третья выдача, четвертая и так далее, это называется ранжирование или рангинг на английском. Вот, и это тоже кейс обучения без учителя, потому что нет достаточно данных, нет размеченных данных, что там на каждый запрос вот это вот имелось в виду, но здесь есть понятие, что больше подходит. Этот кейс очень часто используется, например, в задачах NLP, и особенно стал супер часто использоваться в задачах, связанных с LLM, которые мы с вами будем проходить в следующем модуле. Вот, потому что там есть такая технология, это Crack Retail Augmented Generation, и это по сути есть ранжирование, ранжирование на убеденков ответов, убеденков и вопросов. Есть третье направление в Data Science или машинном обучении, какого типа моделей могут быть, они называются Reinforcement Learning. Это интересный кейс, это примерно как Charge GPT в процессе себя подправляет. Если мы будем смотреть на ответ Charge GPT, на те самые вопросы с течением времени одной и той же самой модели, которая не фонтюнилась, ну, которая с первой с длерлингом не фонтюнилась, то мы имеем в том, что она почему-то дает разные ответы. Потому что она в процессе взаимодействия с людьми от того, прервалось ли общение или нет, от того лайкнули вы ее ответ или нет, от того задавали, переспрашивали вы ее или нет, открывали окно, начинали ли общаться на похожую тему, нет. Еще примерно таких сотни параметров, она это все считает, превращает в один какой-то балл и она учится от взаимодействия с вашими людьми. Еще как пример, например, робот и Boston Dynamics, вот они тоже кейс, которые учатся на Reinforcement Learning. То есть нет никаких размеченных данных, есть просто какое-то environment, какая-то среда, от которой взаимодействуя с ней модель условно учится таки вести себя. Вот, и это есть, как у нас называется, Reinforcement Learning. Очень классная вещь, но я еще dyed сейчас SeoulLWhat. expectations. flakes, оemary, оMP3. Ладно. Ну, например, вот здесь есть две команды и они условно учатся взаимодействовать друг в другом. Как я помню команда первой была задачей спрятаться, команда красных должна была как-то проникнуть к ним. Вот у них было environment это например вот такой вот мир до квадратной формы. У них были стены которые были с дырками и у них были объекты например коробки. Команда синих пыталась выставить эти коробки так чтобы на следующем ходу команда красных никак не проникла. А у команды красных например была такая подъемная штука. Почему они пытались точнее что оптимизировался этой задачей в этой задачи оптимизировалось действия или поведение. Благодаря этому учился синий чувачок синий чувачок и два красных чувачка учились делать какие-то действия так чтобы команда красных не выиграла. Вот самое интересное здесь они например научились пойти забрать там плин себе и потом заставить коробками. И скорее всего на этом обучение уже закончилось и environment поменяли чтобы они стали еще более умными. Плюс reinforcement learning это то чего боятся я из камеры. Те кто очень сильно боится в том что искусственный интеллект погубит. Потому что если сделать хороший reinforcement learning на EI или на EGPT то мы с вами придем потихоньку в сингулярности потому что EGPT начнет сам себя пытаться улучшать. Благодаря этому сам будет улучшаться. Благодаря этому еще лучше себя начнет улучшать и там все переходит в функцию экспоненты. В какой-то момент он просто уничтожает людей и мы с этим ничего сделать не можем. Есть вот такое направление задачи. Если мы с вами любой ML проект, ML задача мы с вами можем понять в какую сферу он пойдет. Какие типы модели надо использовать. Потому что у каждого из своих направлений есть свои типы модели для классификации, регрессии. В основном один и тот же набор моделей. Вы с ними уже познакомились. Например обучение безучительного, свой тип моделей, reinforcement learning, какие-то части вещей. Решается как с обучением с учителем, потому что они все равно немного похожи. Но тоже есть свой класс моделей, свой класс подходов. Теперь если мы приведем маленький пример, то условно мы можем увидеть следующее. Supervised learning, если у вас учитель условно учит. Unsupervised learning, когда учитель говорит, давайте сами пойдите, чему-то научитесь. И потом посмотрим, что происходит. Условно reinforcement learning, когда вас просто выпихнули вместо школы в жизнь и сказали учитесь как хотите. Почему вы научитесь. Вот давайте попробуем разобрать пару примеров и попробуем определить какому типу направление email они относятся. Например в компании хотят создать систему алертов, которая будет уведомлять условно техподдержку о том, что например какой-то тип источников у нас пропал или какая-то запись данных в таблицу у нас хуже идет или какая-то машина вышла и стоя и не отвечает. Вот так мы хотим сопоставить систему, которая будет показывать нам аномалии. И тогда alert нас обыкновит. Как думаете это case supervised learning, unsupervised learning либо reinforcement learning? Поднимайте руки, чтобы ответить или пишите в чат. Наверное supervised. А почему supervised learning? Мы же конкретно знаем, каким классам нужно определить тюльпную ситуацию. А кто не согласен, что это supervised learning? Да, Демрад. Я думаю, что здесь unsupervised learning, потому что, как вы сказали, он какую-то аномалию будет замечать. Она будет отличаться от всех остальных каких-то явлений или действий. И за счет этого она будет выделять его в отдельный какой-то класс или кластер. Да, совершенно верно. Это очень хитрый пример и по сути каждый из вас чуть-чуть правы, но это все-таки больше case unsupervised learning, потому что у нас нет с вами размеченных данных. Мы должны просто светить данные, которые не похожи на все остальные. Но при этом мы все равно будем с вами замерять, насколько хорошо построена у нас система по аномалиям, кейсом как классификация. То есть она условно, сколько из реально нами узнаемых кейсов, когда было плохо, сколько она предсказала, сколько она перепутала, как неплохо, сколько из кейсов, которые неплохо, она показала, что это плохо, и сколько из неплохо, она действительно неплохо и показывала. Это будет кейс unsupervised learning, который будет притянут за уши, условно, в unsupervised learning классификацию. Давайте следующий пример попробуем разобрать. Например, charge-a-pt. Какой это кейс? Ча-ча-пи-ти. Человек, допустим, общается с ча-ча-пи-ти, он может построиться под его то, что он требует и под его общение. Если кто может возродить, возродить. Я как раз не возражаю. Вы просто до этого говорили на основании чата gpt, что это reinforcement learning. По причине того, что со временем он начинает разные ответы давать, потому что улучшаться будет моделька на основании каких-то дополнительных вопросов или лайков и так далее. Поэтому я с ним согласен. Тут, мне кажется, он все-таки гибридный, потому что он не подходит полностью под reinforcement, от того что он изначально не нулевой. У него все-таки есть какой-то интеллект, он уже какие-то данные имеет, и мы когда с ним общаемся, мы не общаемся с ним с нуля. Он уже знает какие-то ответы, которые он может дать, а потом уже по ходу дела reinforcement learning скорее, наверное, это больше какое-то дополнение к нему, нежели его основная функция, на мой взгляд. Мне кажется, здесь все-таки, возможно, тоже unsupervised learning, либо и тоже unsupervised, но он будто какой-то гибрид из всего, с олянкой в общем и раздых. OK, супер. Диана права. Изначально это был case unsupervised, у нас не было размеченных данных. ChudgePT в основе себе имеет языковую модель. Языковая модель имела выход очень простой. На основе контекста, который к ней приходит, ее задача была предсказать следующие слова. Все. То есть условно всем привет, меня зовут, стоит вопрос, проходит череденированную сетку, предсказывается слово с вероятностью, например, rtl. Там есть всякие разные техники, как потом на основе этой модели происходит генерация текста. Мы с вами будем проходить на модели NLP. Благодаря этому она начала генерировать текст. Потому что это все равно в пор вот в этой модели идет. Благодаря этому появился chatbot. И потом, чтобы сделать chatbot этот супер интересным, чтобы с ним можно было работать, его практически использовать, чтобы он вам нравился и тому прочее, мы прикрутили туда reinforcement learning. Этот кейс называется новое наведение reinforcement learning, как reinforcement learning with human feedback, lhrl.hf, который еще называется. Вот. И он был прикручен еще к chatbot. То есть условно добавилась новая loss функция, которая потом у нас модифицировалась. А сейчас есть третий вариант, который они придумали. Это тоже из темы reinforcement learning родилось. Это то, что сейчас появилось последние полтора месяца в chatbot модели O1 preview и O1 mini. Это некоторый кейс, как перестать вообще промтить в chatbot. То есть вам могут сказать в том, что если просто писать в chatbot, то он будет плохо отвечать. Вот это была попытка бороться с этим, чтобы он сам пытался думать и пытался более лучше отвечать. То есть условно chatbot первоначально это unsupervised модель, которая потом дотюнивается каждый раз через взаимодействие с пользователями. А это уже read-post-component. Давайте другую кейс приведу. Чуть полегче. А как думаете, например, предсказание цены акции, например, это какой кейс? Супервайс регрешн? Да, совершенно верно. Почему я вас сейчас попробовал задавать эти вопросы, чтобы вы понимали, как каждый ML-проект нужно относить к какому направлению? Потому что очень важно, что для каждого направления существует свой набор метрик, с которых оценивается, насколько модель тренируется, потом насколько они связаны с бизнес-метриками и тому прочее. Поэтому очень важно понимать, на какой стадии ML-проекта вы сейчас находитесь, в каком направлении вы идете, и того, чтобы вы могли выбрать правильные метрики. Если вы введете неправильные метрики, то у вас будут плохие результаты. Например, если вы решаете задачу кластеринга и вам нужно решить задачу кластеринга, странно, если вы будете использовать KL-диригенцию, например, которая в метриках используется в ринфоршном плерниге. Не будет давать хорошего результата. Где используется датсайнс? В Крации, наверное, сейчас уже почти везде. Датсайнсы и ML- или EI-проекты могут быть совершенно разные. Это может быть основное вождение, это может быть проведение тестов, для того чтобы понять, стоит ли нам красить какую-то иконку на сайте красный свет или нет. Это может быть, словно, автономные дроны, это может быть, словно, то же самое, что GDT, это может быть, словно, предсказание цены на что-то, это может быть, словно, рекомендаторные системы, это может быть предсказание рекламы и тому прочее. Если сейчас говорить, то, пожалуй, email используется везде. То есть нельзя прожить день жизни и не быть, и не повзаимодействовать с email-ом в каких-то продуктах. Условно, Алмазбек, какой вопрос? Вот я сейчас читаю, где используется датсайнс. Тут написано NLP, Computer Vision, и меня вот заинтересовало anomaly detection, то есть обнаружение аномалий в данных. Это специальное уже направление, да, или какая-то библиотека отдельная или программное обеспечение для обнаружения аномалий? Уже есть такое? Да, есть. Из последнего кейса мой последний проект, например, был на магистральной Телькомовской сети определение DDoS attack аномалий. Так что, конечно, при приставе довольно точно определялись. Может, благодаря этому немножко поменьше DDoS. Да, есть библиотеки. В самом простейшем случае anomaly detection это всегда supervised case. В некоторых случаев не всегда это так. И в процентах 70 кейсов anomaly detection сводится к outlier detection. То есть предсказанию выбросов. Для outlier detection есть огромное количество библиотек, огромное количество алгоритмов, например, HBOs, histogram ways, outlier system и многое другое. То есть действительно есть библиотеки, например, по IUD можно посмотреть в библиотеку, кто делает outlier detection и она в себе содержит, не знаю, модели может около сотни алгоритмов. Окей. Спасибо. Если мы возьмем телефон и в телефоне начнем смотреть, где email используется. Откроем клавиатуру, сверху мы видим, рекомендации каких-то слов. Вот я, например, написал какое-то количество текста и мне удались рекомендации. Эти рекомендации являются соответственно результатом работы email. Следующий кейс, например, я что-то гуглю, это тоже работа email модели, которая ранжирует сайты, и предсказывает вам какие сайты вам указать. Когда-то это есть даже статья, первая версия, одна из первых версий Glowindex, как они работали. Алгоритм это называется PageRank. И если интересно, можете почитать, как он работает. Очень классная статья была написана про него. Потом я смотрю прогноз погоды, прогноз погоды это тоже email-кейс, который предсказывает погоду, так чтобы, условно, если я вышел, то мне действительно нужен был зон с собой взять. Потом, условно, я пользуюсь банком. В банке что делается, каждой транзакцией проверяется, фрод это не фрод, антифрод системы, чтобы избежать отношений, это тоже кейс, когда email используется. Когда я в YouTube смотрю, не высвечиваются какие-то рекомендации видосов, которые я смотрю. Это тоже кейс, предсказание email модели, открывает TikTok, там дофига рекомендаций. Я иду в магазин, товары на полках, тоже были предсказаны email модели, где они должны будут выставлены, как они должны будут выставлены, если мы говорим особенно про большие сетевые магазины, скорее всего, маленьких магазинов, вряд ли используется моделя. Если я смотрю на цену, эта цена тоже была предсказана, которую нужно поставить так, чтобы компания заработала максимально много денег, но при этом так, чтобы она заработала много, покупатель должен покупать, поэтому она оставляется, пытаясь предсказать максимум, чтобы каждый покупатель мог потратить, и все-таки купить. Пойдем, захочу поехать куда-нибудь, мне надо будет условно вызывать такси, таксист выберет, точнее, моя сделка будет предложена к кому-то количеству таксистов, которые с большей вероятностью ее возьмут, они будут проранжированы, и выдана сначала определенную количество таксистов, таксист условно выбрал, он по мне приехал, потом ему нужно поехать, чтобы он мне не говорил, а ты знаешь ли дорогу, используй навигатор, в навигаторе маршрут строится тоже email, email проект, который выстраивает маршрут, и там условно предсказание времени, сколько это займет, это тоже email-кейс, потом, который еще от меня выставится, за проездку такси, это тоже кейс, где сказывается email, условно как можно больше, чтобы я потратил на это, но при этом совершил сделку, и прежде прочем, если мы будем говорить про email, то мы не будем говорить о компаниях, которые везде, везде, то есть вряд ли существуют компании, которые, вряд ли существует достаточно, существует ли проект, в который email нужен, существует ли направление бизнеса, существует, но либо там email появится, либо, либо он там просто не нужен будет, еще больше и еще больше вот и опять же не забываем про уровни до там типа автоматизации которые существует то есть словно некоторые компании может сейчас этому только будут доходить до про дети как так как вы понимаете мл и дайдес аинс используется очень много где окей у любого дс или мл или я и проекта у него есть определенный цикл цикл через который он должен будет пройти так чтобы проект был завершенным чтобы он до так чтобы он был завершенным чтобы словно попал в против это конечная цель любого дсп проект вот любой дсп или мл проект начинается с бизнеса бестендинг бизнеса джейн и это значит что мы должны понять типа какая сущность бизнес потребности для задачи которые мы хотим который мы пытаемся сделать на бизнеса джейн и мы должны с вами понять какие у нас данные будут какие типа модели можно использовать какую правилу какой набор гипотез мы должны будем проверить и тому прочее и тому прочее вот это обычно общение с бизнесом это общение с конечным заказчиком с folder или олдером того кто будет владеть результатом вот следующий этап который здесь обычно начинается он называется дайдей афизишен и understanding это в том чтобы вы могли понять входные данные которые у нас есть какие-то подходы не походят сделать разведочный анализ данных да идеи которые вы уже сталкивались возможно заняться чисткой данных подготовкой данной нормализация данных и тому прочее и тому прочее обычно из идеи дайдей афизишен и understanding кейса хотят вычленить несколько выходов первый выход это разобраться с данными понять достаточно их или нет чистая или нет и там привести в какой-то более нормальный вид второй кейс это понять зависимость в данных которых есть сделать некоторые дейт истории да типа понять что от чего зависит понять воспроизводится ли это в данных или нет условно если надо пойти поговорить с экспертами так чтобы проверить доменная специфика присутствует данных или нет вот проявить какие-то какие-то статистические гипотезы например на этапе идеи для того чтобы понять может какие-то свойства из мира статистики применяться к нашим данным и мы можем воспользоваться в лейк-хаком этих свойств для того чтобы что-то сделать я пример воспользоваться лайк-хаком что изменение ценности распределяется по нормальному распределению соответственно мы знаем что в районе трех сигнала нас будет находиться 99% наблюдений данному также будет в нас есть словно 10 или 100 процента которые у нас будет больше больше чем чем три сигмы сигма у нас у нас это и не будет сотая процента изменения вот соответственно мы будем знать что 1 10% изменения у нас может быть больше да мы просто могли подготовить к рискам если что выставить стоп вещи чтобы не потратить деньги в риск менеджменте можно использовать совершение из дела вот это стапа идеи плюс этапа идеи последнее что мы должны вынести мы должны вынести гипотезы какие модели стоит попробовать гипотезы на каких метрах мы должны будем смотреть и гипотезы может какие-то архитектуры модели в котором используется на этапе моделирование который выйти находится здесь он обычно идет третьим чередом здесь какая основная цель здесь основная цель найти какую-то модель которая бы нас удовлетворяла да которая нас будут удовлетворяла по точности по антайму которая удовлетворяла по необходимым ресурсам для для ее воспроизведения и хранения и хотим все-таки самая первая цель да это соответственно словно какой-то метрку точно не важно средний патриотичный пук точки либо на разницу диригенции мы хотели бы улучшить до максимума так чтобы она была очень классно вот и последний этап после того как мы и продумали получили готовый модель которые на входных данных предсказывает что-то то что нам нужно последний этап это дипломат мы должны наши модели выкатить в против это должно автоматически все считаться оно должно считаться устойчиво там не должно быть каких проблем как деда дрифта да и если есть деда дрифта у нас должны они вовремя переодучаться и специфицироваться специфицироваться должна быть решена проблема с генерализацией данных да это все должно быть у нас устойчиво вот круто звучит да типа 1234 но чаще всего вы будете видеть какой-то трос между этими между этими этапами это вполне нормально что вы с третьего этапа будете перепрыгивать опять первый этап пойти узнавать условного бизнеса природу происхождения этих данных надо для того чтобы понять использоваться или нет тому прочее вот и не все мэльные проекты можно потянуть в одного поэтому например в компаниях возникают мэль команда да где не один мэль человек он там единицы или десятки сотни тысячи десятки тысяч мэльчиков могут работать компания вот потому что проект бывает разной сложности вот либо разного уровня сами мэльчик вот это в принципе как в рации любой дес либо мэль проект будет это через что походил например создание чаджи птички вот если мы будем говорить про каждый этап и что на нем этапе на что язык на этом этапе потребуется специалист который этим занимается в первую очередь на бизнес understand и гейф это проведение встреч это отчетение каких-то научных статей может поиск на кабе открытых решений если мы будем говорить про второй этап про этапы дей то обязательно нам нужны какие-то вычислительные мощности это то чтобы мы могли поработать с данными там чаще всего используется язык пайтон чаще всего используется юпайтор используется много статистики здесь используется много всяких преобразований здесь да которые мы делаем основе калкулса либо линейные алгебры на этапе моделирования у нас уже подключается модель машинного обучения опять же научные статьи если мы проверяем гипотеза например вот здесь вот укулус функцию поменять использовать что метрики возрастли либо вот здесь вот для при процессе чтобы это было проходило твентайма мы должны будем использовать например матричные преобразования использование готовых библиотек использование рат и языков программирования чаще всего и пайтон использование не витер библиотек да как как удобного инструмента в котором мы можем проверить много гипотез и так что нам не надо было постоянно запускать пайтон 3 но инфа и на этом кейсе обычно потребуются видеокарты либо какие-то вычислительные ресурсы так чтобы мы могли моделировать проверять гипотезы и так чтобы это не занималось слишком много времени да с помощью видеокарты все-таки многие алгоритмы решается намного быстрее не все алгоритмы до условно линейные три грейс у нас не будет шанс быстрее с помощью видеокарты потому что там нужен хороший cpu центральный ядерный процессово но не рано сеть и словно редно корост у вас будет или и джи буст будет намного быстрее сработать с видеокарты вот и на третьем этапе дипломата все легко используется базовые вещи которые в любом соцсетях инжиниринги используется то есть дженткинс тапка если нам нужно хранить данные загрузка будет бд либо загрузка на на веб соответственно использование API модели где-то надо хранить для этого например будем использовать амалиопс или мэль флоу точно будет использоваться мэль опска ту ветка девок до вопса да словно докеры сон аэрфлоу словно дженткинс если мы говорим про хранение моделей версионирование модели то часто используется мэль флоу да вместе с air flow если мы будем говорить про на чем это воспроизводить то опять же сервера с видеокартами или сервера cpu на ком языке написанный код если какая-то часть кода может спокойно переписать другой язык программирования с пользой оптимизации и тому прочее то есть если условно в обленках говорить то вот так выглядит любой тимальный проект 4 основные стадии и мы видим и и технологии да или какие вещи используется на каждой стадии если как-нибудь вопрос на вот этом этапе может кто-то хочет поспорить или кто-то не согласен или спросить зачем здесь используется вот это вот это мэль флоу это такая штука в которой можно хранить во первых эксперименты для типа условно вы запустили модель с такими параметрами рандофорста вот вы оценили по всем метрикам сохранили и сохранили версию модели такой условно репозиторий для сохранения экспериментов вот плюс поскольку там сохраняется эксперимент мы там заодно сохраняем разные версии моделей которые создавались и соответственно дипломатии так до дипло с модели мы условно провели эксперимент посчитали значение метрик и просто загрузили на мало слову модель с галлайдчиком условно что ее можно вот выкатывать а в пророде есть уже афантических от сервиса например которые обрабатывай данные они просто могут подцепить новую модель и сразу за за дипло идти на больший количество ровно то есть первая мэль флоу используется для сравнения экспериментов да что пословно вас были не скриншоты метрик каких то что вы могли сравнивать разные эксперименты а второе для того чтобы условно чуть облегчить версии нервов не модели подачи спасибо вот вроде у вас очень и самой неудачность и самая плохая вещь то что понимаете что у большинство экспериментов они никуда не придут. Есть статистика в том, что McKinsey ее проводила, она изучила порядка 10 тысяч компаний по всему миру и опросила их насчет того сколько имали проектов в Дании или доцензий проектов они начинали. И пошло на статистика, что 87 процентов всех начинаний не дошли до продачи. Это значит, будьте готовы, что то, что вы на чем работали месяц, может просто закрыться и вы можете пойти заниматься какой-то следующим проектом. Если мы будем говорить про классический software engineering, то здесь статистика намного лучше. 50 процентов проектов могут закрыться, которые находились на пилоте, то есть они давно попали в прод, они задеплоились, а в DSE и в MLA они могут и не задеплоиться. Потому что какие-то проблемы возникли либо на этапах, либо нет понимания, нужно или не нужно, или потом оказалось вам, что она не пользуется популярностью или работает настолько плохо, потому что на каждом этапе может быть огромное количество нюансов, о которых мы сейчас проговорим. Да, Саджан. Хотел спросить, вот это большой процентаж провалов, одна причина из этих провалов может быть то, что датасайз проектов намного больше, чем софтверпроектов. Намного больше, чем датасайз проектов. Возьмите любую IT-компанию в Казахстане, уйдите несколько продуктовых команд, ну или там, давайте какой-нибудь пример компании разберем. В Аспе разберем. Там работает, насколько я помню, порядка 100-150 человек, которые близки к датасайзу. Да, это датаманеры, и они называются image и вот прочее. И порядка несколько тысяч инженеров, софтверпинженеров. Например, такое соотношение. Это просто по людям, соответственно, сколько в команде в среднем, 3-4 человека, да, это меньше команд. Ну и количество команд мы тоже увидим, сильно большую разницу. Нет, это связано скорее с то, о чем мы первым говорили. Типа, очень мало компаний в мире, которые вообще даже на третьем этапе находятся. Словно то, что делается руками, мне кажется 80% компаний в мире, которые на первом этапе находятся. Это не значит, что это плохо. Это значит, что этого и так достаточно, что, не знаю, еще процентов 15 компаний в мире, например, занимаются автонатизацией, аналитикой, я уже за 100 ушел, ну, в основном, здесь и не в сфере. В общем, там процента 3, там где мы есть, и там меньше процента, которые генеративные не делают. Ну кто генеративные не делает? Компании 40-50 по миру. Давайте дальше. Вот, в общем, если на каждом этапе примерно стек, которые используются для решения проблемы этапа. Теперь в том, что идеально, когда у нас это все последовательно, но из-за того, что сфера новая, сфера до конца, условно не оптимально решенная, то чаще всего вы будете видеть в том, что у вас будет с одного этапа на другой этап вы будете тратить время, перебрыгивать точнее. Вот, если говорить в среднем, сколько, вот я, например, месяц рассчитывал, что будет решаться какой-то проект, сколько времени каждый этап займет. Обычно процентов 5 это занимают всякие переговоры, всякое общение, всякое выделение, чтение статьи и тому прочее. Процентов 60 времени на любом эмилиз-проекте вы будете заниматься идеей, работе данными, анализом данных и прочее-прочее. Процентов 20 еще времени у вас будет занимать моделирование, и 12 процентов будет решаться вопросы инференцера. Как это задеплоить, написать неоптимальный кост, я бы не так долго считалась, или обработка данных не так долго занималась. Вот, и обычно на DSML проектам есть ряд позиций, которые занимаются своими задачами. Здесь у меня нет, ладно, тогда чуть позже поговорим. Вот, если будем говорить про историю этого всего, как это все зародилось, и через какие этапы это прошло, это тоже очень важно знать, потому что, во-первых, история иногда циклична, а во-вторых, мы должны понимать, откуда произошли многие вещи в эмиле и в AI. Вот, первые идеи, условно супер-исторические, в первых фантазийных рассказах появились про идеи искусственного интеллекта. Например, в религиях много идей, сидит концепции о искусственных вещах, которые обладают разумом, могут что-то делать. Есть такое понятие, как големы, такие неодушевленные сбуски глины, которые умеют двигаться, которые понимают, как двигаться в пространстве, которые умеют слышать речь, воспринимать, выполнять приказы и тому прочее, но при этом не умеют говорить. У древних евреев считалось, что говорить это только человеку возможный процесс. Немножко мы нарушили их планы, когда придумали модель синтеза речи. В целом, можно сказать, что весь AI, а еще какие придумы, вспомним, там в 12 век было такое произведение от Джазари про универсальных роботов, которые могли воспроизведать музыку, которые работали от того, как двигались быки. В мифе о древней Греции, тоже есть такое очень старое, там был такой робот Талос, который сидел на Крите, понимал, какие корабли из проплывающих являются своими или чужими, если они являются чужими, он брал большие куски камней и кидал в них. Если бы МЛ не переводить, то это у нас здесь компьютер-вижн-кейс, в том, что нужно делать классификацию свой чужой. Во-первых, сначала задлицировать корабль, есть он или нет, потом классифицировать свой он или чужой и потом еще делать какую-то матч, чтобы предсказать, как надо кидать, чтобы в него попасть. И вот вплоть, условно, до середины 20 века, все, что было про искусственный интеллект, где бы оно ни встречалось, это все либо был полный скам, вот как, например, в XIII веке Альберт Великий изготовил типо искусственного интеллекта, который умел разговаривать с людьми, умел вести беседу с ними. В реале, скорее всего, это работало из-за того, что просто на другом этаже, в спрятанной комнате сидел человек, который вместо этой головы словно работал. Механические рыцарь, например, Леонард Довинча тоже был, тоже не был искусственным интеллектом, тоже не обладал внутренним разумом. И все фантазии... Вот интересный класс был Автоматон Жак Адрю, он сейчас под Парижем, он в музее до сих пор сидит, он мог нарисовать человека, но это тоже не было кейсом с искусственным интеллектом. В общем, до середины 20 века все, что у нас было по искусственному интеллекту и вот такого вот виску уровня автоматизации, это все были какие-то концепции либо какое-то полное мошенничество. С середины 20 века у нас появилась первая математическая логика, как должны работать нейронные сети, нейронные сети как главный инструмент, который, условно, может поплатить искусственный интеллект, по крайней мере, как кажется, на текущий момент времени. И базировалась логика, которую в 43 году Макаловны и Питс придумали, и они написали математическую логику работы нейронных сетей в том, что, условно, у нас есть нелинейность на входе, нелинейность на выходе, что у нас может быть вот так, условно, не оптимизироваться, например, путем почета часто производных какой-то функций потерь, в том, что вот так можно их условно компонировать так, чтобы они еще лучше начинали работать с каждым уровнем, воспринимая все больше, все более сложные закономерности и тому прочее. Ну, их работа, понятно, она работала на идеи, которые придумали Ньютон, которые придумали Гальтон, которые придумали Байес, системы дифференцирования, условно, теория вероятности, как основной концепт, наверное, теория Момбайеса, на основе которой мы можем с вами получать предсказания на основе входных каких-то данных и тому прочее. Вот. Словно с сороковых годов можно начинать в том, что у нас что-то появилось практически правдоподобное в сторону реального применения, как минимум в простейшем случае на какой-то специфичной сдаче попробовать заменить человека, заменить человека в какой-то мыслительной деятельности. Самое простейшее, это, например, детектировать свой чужой корабль, например, проплывает или переводить текст, например, переводить речь в текст для того, чтобы потом подняться к чужим словам и тому прочее. Вот, условно, вся эта движуха начинается у нас с сороковых годов. С сорок третьим годовом, Макалан и Питц, вот это вот Макалан был профессором, это было его сначала фичдистидентом или аспирантом и потом стал тоже профессором. К сожалению, Питц закончил жизнь самоубийством, хотя последний, на чем он работал, это было 3D нейронной сети, математическая логика для 3D нейронных сетей. Так что, поканчив собой, возможно, бы сейчас нейронные сетки, которые работают в 2D, и не бывают 3D нейронных сеток, возможно, сейчас были бы 3D нейронные сетки и было бы все намного лучше с обучением модели. Потом был Алан Тьюринг, благодаря нему появились компьютеры, потому что он первый создал нормальный работающий компьютер, который использовался, а кто знает, для чего использовался первый компьютер, который был использованный для того, чтобы создать компьютер, Да, расшифровки Эникмы. Да, совершенно верно. Была такая шифровальная машина, которая использовалась в ЖИС во время Второй мировой войны, которую они использовали, чтобы обмениваться сообщениями. И вот эти сообщения перехватывались, один аппарат захватил разведка великопитаний, с именем МИ-6, и его задача была условно, чтобы он был в безопасности. И задача была условно, понимая, у них была Эникма для того, чтобы они могли экспериментировать и пытаться угадать шифр, и условно они угадывали этот шифр, благодаря этому они его расшифровывали, а шифр всегда был на каждый день уникальным, да, поэтому они научились расшифровывать, и там половину войны они понимали, что мне это все делать. Самое прикольное, они до последнего не делились. Там кто знает, есть классный фильм про Аллана Тьюринга, чем советую его посмотреть. К сожалению, тоже человек закончил с очень печальной судьбой. Он подвергся, сколько помню, химической констрации, он был в нитрационной ориентации, и условно за это очень сильно поплатился, в тогдашние времена. Он создал вот это, то есть он создал первый компьютер, он конечно работал на лампах, у меня было еще некие транзисторов, он много чего туда использовал, из булевой математики, чтобы вычисления работали, да, основной бинарные потоки данных, которые можно было делать для вычисления. Вот, он придумал компьютер, и еще что придумал Аллан Тьюринга, он придумал тест Тьюринга. Кто знает, как звучит тест Тьюринга? В одной комнате тест проходит человек, в другой комнате проходит компьютер, и идея тестов, чтобы узнать, какой именно тест прошел компьютер, какой именно прошел человек. И он говорит, что один AI станет таким супермощным, когда тест не сможет угадать, какой ответ был от человека, какой был от компьютера. Это как мы сейчас воспринимаем тест Аллан Тьюринга. В самом деле, Аллан Тьюринга по-другому сформировал свой тест. Это не совсем, как он его объяснил, свою версию, ну точнее реальную исходную версию, которую потом чуть упростились, и в общем-то в самом сознании есть. Он говорил не совсем честно, что компьютер будет превратиться в человека, а человек будет просто всем собой. Поэтому сам тест, он выглядел, как возьмем мужчину, который будет притворяться женщиной, и компьютер тоже будет притворяться женщиной. А на второй стороне будет сидеть респондент, который будет пытаться понять, словно женщина или не женщина за бортом. Ну и соответственно, реально женщина. Из-за немного хмп-повестки эта концепция пропала. Основная локация, что как бы, и я должен имитироваться так же, как словно имитирует человек. И вот в этом сравнении, на этом тесте, или машина должна человеку происходить. Ну мы сейчас живем в мире, где формальный тест Аллан Тьюринга проходит. В начале 5-ти давно прошел тест Аллан Тьюринга, мне кажется, еще с 60-х годов первые прохождения прошли, может даже с нулевых годов. То есть Тьюринга пройден как бы, но супер-попутопузовость спустя интеллекта у нас у нас на руках не одна, которая может заменить человечество. Окей. И короче, с 50-х годов нас с вами в 56-м году, наверное, в самый по обратному момент, от которого читают начало ей, пришел так называемый Дарпорский семинар. Короче, собрались ребята, довольно классные ученые до того времени, но еще молодые, еще пока они не стали супер-известными. И чтобы позвать как можно больше людей на метап или конференцию, они заявили его, как они короче такой анонс сделали на конференцию. Мы предлагаем исследование искусства интеллектом сроком в два месяца с участием 10 человек летом. У 56-го года в Дарпорском колледже Ганновера, город Ганновер, штат Нью-Гэмшер исследовано основанное предположение, что всякий аспект обучения или любое другое свойство интеллекта может в принципе быть столь точно описанно, что машина сможет его симулировать. Мы пытаемся понять, как обучить машины, использовать естественные языки, формировать абстракции и концепцию, чтобы решать задачи, сейчас подласанные только людям и улучшить самих себя. Мы считаем, что существенное продвижение в одной или более из этих областей вполне возможно, если специально подобранная группа ученых будет работать над этим в течение лет. Короче, собралось 10 человек, именитых на тот момент ученых. Из них, например, были Джон Маккапни, Марвин Минский, Клод Шеннон, Наталья Родчестер. Многие из нас, поскольку многие работают в компьютерс саинс, многие, наверное, знают кто-то по-мински. Один из первых физиков программирования, Лисп, это Марвин Минский создал, например. Шеннон, например, создал теорию информации, по которому много чего рассчитывается в нашем мире. Родчестер тоже что-то крутой сделал. Маккарти тоже много чего крутого в компьютерс саинс сделал, то, что я сейчас не вспомню. Короче, четыре очень именитых чувака из этих десяти было. И они действительно два месяца потратили для того, чтобы попробовать создать искусственный интеллект. И, как вы понимаете, у них ничего не получилось. Но зато отмыли, хорошо провели время на грант, который собрали под это все. За последние два года очень много интересного. В дарпентский семинар там было про классную тусовку, которую они проводили в «Куту-Пятница». Рассказывали интересного. И вот с этого момента, с 1956 года началось условно-планомерное развитие практической части. И вот в этом моменте, в этом моменте, в этом моменте, в этом моменте, в этом моменте, в этом моменте, estaban велич medals, уже forced Normals, который<|ko|> perhaps was passed after 성nąού, вот сейчас мы живем с вами в четвертом наверное или третьем я и будем и вот и в 50 60 годах тоже самое было тогда придумали много классных вещей придумали перцептом разом блата это условно такая очень это было кратч первое пред практическое применение нейрон на сети как из состоящей из нет одного нейрона да который действительно там на первой карте и в которой приходила две точки она придел какой точки ну типа слева или справа точка на первой карте находится вот в 60 56 году например придумали лоджик теорист это была такая экспертная система которая могла доказывать очень много очень много математических теорем не супер много но кто знает есть такая книга принципа математика который рассказывается математическая логика там очень много расписано математических базовых теорем вот условно она и смогла в большую часть доказать вот потом 60-х годах например придумали первого словом чат вот это вот системы и лиза которая могла общаться вот здесь у вас есть пример да который притворялась психотерапевт ам и могла условно человеку металлина и здоровье фиксировать все тогда придумали например такой алгоритм кинем да и наверное очень часто кейс который до сих пор применяется в машинном обучении продакшена уровни там например а вот самый простой кейс условно ставят камеры камеры и камеры детектирует где находится на человеке лицо лицо условно там 30 или 35 параметрам переводится вектор какой-то имбейдинг и потом поэтому вектору с помощью кинем до индифицируется уже какой-то определенной личности пример и все вот это 50 до 60-х годах происходила вот на таких компьютерах тогда еще транзистор насколько помню я был придуман в 70-х годах был придуман и вот тогда это все было довольно много и довольно плохо короче 50 60-х годах чтобы запустить линейную регрессию надо было где-то неделю две потратить то что мы сейчас с вами можем сделать за пять минут и получить результаты поэтому как вы понимаете тогда любой эксперимент стоил сотни тысяч а может даже миллионы долларов провести 58 году было классная статья первая статья про искусство интеллект что это будет компонент замены человека раз он была короче придумал искусство нейрон действительно его обучил который мог определять на какой части перфокарты прокол это прокол это дырка вот он на это потратил где-то ляма 2 долларов и он говорит подайте мне еще 100 тысяч долларов на следующий год и я короче и сделаю помню замены человека он у меня будет бегать ходить воспринимать речь и на луну мы будем отправлять мне людей этих как понять тогда началась космическая гонка 50 году была уже или это было преддверие космической гонка в 61 но тогда были ракеты уже вот и спутники спутник тоже запустите поэтому там была кейс то что на луну полетят уже автоматические дроны что-то не тратить на скафандры вместо вместо людей ну как вы понимаете мы выделили деньги но не создала замену человека вот в 70-х годах это прям такой был бум когда начали сотни компаний применять искусство интеллект вот на 70-х годах короче будем теперь на систем произошел сперва тема это типа вот мы хотим за решить какую-то задачу давайте там мы подумаем как ее можно решить и сделаем вот здесь такой алгоритм здесь такой алгоритм это большое дерево и по нему будет что-то делать самый яркий пример это вот на пиросе темы мейцин была которая могла предсказывать какой из видов антибиотиков нужно рекомендовать людям при течениях болезни классная штука была там она могла выбрать более оптимальный антибиотик из большого кулы антибиотик у него было около 410 правил а это условно как и фэлсы да вот либо либо и как одна из возможных операций вот я неплохо работала несколько несколько недель в среднем увеличивала на полторы недели в среднем увеличила вылечивая месть от болезни вот но проблем в том как она работала тогда еще не было интернет как вы понимаете короче по телеграфу передавали на первой карте заполненный тест заполнять минут 10 он пересылался какое-то количество времени обрабатывался какое-то количество времени короче ответ по тому какой от тебя пить он там приходил после того как человек уже выздоровел болезни в среднем вот короче это было вообще не про не про работу в реал тайме не про применимость но исследование вот и начале 70-х годов начинается первая зима искусственной интеллектии когда я перестали выделять на исследовать вот произошло это сколько я помню для британии первая не академия но сейчас университета короче то ли на ком-то собрание у них это было то ли то ли это было рекомендации естественного образования прочь начали резать бюджет на следующий год на все я и проект потому что считали что они занимаются просто трата денег и интересно они делают вот сразу подлетелся в одном штата по всему миру и условно наши поставили на ползу все исследования виде искусственного интеллекта в слои места истал институтомандах по миру шло исследование искусственные искусственного интеллекта потом это все перейдет переворотилась там т.к. 3-4, следовательно по миру что-то делали, это планировали. И тут сильно зарезаны бюджеты. В 80-х годах это все тоже продолжалось. Вот там условно было маленький сплеск в 80-е года, когда первая компания, насколько я помню, DEC, Stole Keep Incorporation, в 86-м году там наняла EIDL, который позволил ей экономить какое-то количество денег, около 10 миллионов долларов в год с помощью оптимизации, потому что предсказывал траты, и не основав я этого, можно было сэкономить. Ну, там условно в 70-е, 80-е годы, это там черная эпоха искусства и интеллекта, им занималось точно три группы, три из этих группы получили филцевскую степень, каждый руководитель этой группы. Это, например, Benji, это, например, Likun, это, например, Hinton. Вот эти три человека 80-е годы, они были теми, кто занимались изучением искусства и интеллекта. Hinton, например, получил нобелевскую премию по физике, в том, что он не был физиком, он получил нобелевскую премию по физике за создание искусства, ну, за создание нейронных сетей. Вот. То есть, понимаете, 80-е годы, когда говоришь, что ты в мире науки занимался искусством и интеллектом, было супер непрестижно, да? С тобой здоровались еще, но когда узнавали, чем ты занимаешься, резко хотели уходить от тебя или переставали тебя слушать, и тому прочее. Вот. В 90-х годах даже была такая классная фраза Джона Дейпхера, очень знаменитого журналистов в Штатах, что нейронные сети — это второй лучший подход для того, чтобы делать ничего, да? Не получить никакой пользы. Вот. В 90-х годах, вот, в 90-е, ближе к концу 90-х годов начинается какое-то развитие искусства и интеллекта. Он сделал ребрендинг, да? Больше уже не назывались искусством и интеллектом, а стали называться машиной обучения, да? Потому что уже появился какой-то класс созданных алгоритмов, которые в 20-м веке в основном посоздавались, некоторые в 19-м веке посоздавались. Вот. Каких-то подходов к решению задач. И уже стали доступные компьютеры, да? Они уже стоили не миллионы долларов, а уже стоили, там, условно, тысячи долларов, и даже, может, дешевле тысячи долларов, и они стали более доступными, да? Благодаря этому там любой эксперимент или любая тренировка модели, она там занималась не недели, а уже могла в течение дня быть решена. А то, представьте, там занимались такие же люди, как и мы с вами. Допускается какая-то ошибка, и вы через две недели понимаете, что результаты неправильные. Вам надо опять две недели сидеть, ждать, пока посчитается неисправная вещь. В Data Science проектах мы с вами видели в том, что там надо всё вот так вот пляшет. Соответственно, это всё там не неделю. Один расчёт занимал несколько лет, надо было потратить. В том, чтобы решить задачу за эти несколько лет, словно приходит новая версия компьютера, на котором это можно считать, там, словно за один день, да? Потому что у нас с вами компьютер сильно быстро улучшается. Был один материал, я сейчас его не вспомню, в 0-х годах классная статья вышла, где он писал в том, что назвал печальный эффект или что-то типа такого. Он, на примере, рассказывал, как он три года боролся с решением одной задачи. Надо было, ну короче, на текущий момент, в который задача работал, не было ни одной нормальной архитектуры, которая бы её решала. Для этого он начал создавать свою архитектуру, придумал суперизящную функцию ошибки, через которую это всё будет считаться, потратил на это несколько лет. И там, к концу, когда он опубликовал статью, потом, когда он об этом расширял, он назвал печальный эффект в том, что за эти три года, пока он боролся с этой логикой, пока он составил большой клипс экспериментов, просто появились лучшие компьютеры, на которых предыдущая архитектура начала лучше работать. Потому что быстрее можно было посчитать, и то расчёт, который он занимал раньше месяца, можно было уже за неделю посчитать. То есть такие печальные люди тоже есть, если долго работают. В 90-х годах начинается опять развитие мэля. За это время, по идее с 86-го года работает, но в 90-х годах... короче, три важных события. Первое, создается алгоритм IBM DBLU. Это не было модель машинного обучения, это была экспертная система, которая по перебору возможных и фальсов обыграла Каспарова. Они там встречались три раза, насколько я помню. Первые два раза Каспарова побеждал DBLU. В 97-м году, в третий раз, он DBLU впервые обыграл Каспарова, и поэтому стал супер большой новостью, типа вот ещё машина с пустым телек заменит людей, потому что шаг потонноучились побеждать. Это не было модель даже машинного обучения, алгоритм IBM DBLU. И плюс, никто не говорит, что в двух остальных партиях Каспарова обыграл шахматы. Но сейчас мы знаем, кто играет в шахматы, в том, что типа компьютерой невозможно обыграть. Нет ни одного человека в мире, который бы мог обыграть компьютер, поэтому существуют запреты, например, на шахматы в турнирах использования компьютеров, там существуют некоторые правила, которые бы мониторили, чтобы не же умеешь идти и не пытаетесь в компьютере словно находить более оптимальные ходы и прочее. Вот тот занимается шахматами, играет в шахматном детале. Да, играю. Ну вот, подтвердите, есть ли это правило, которое борется с использованием машин? Ну да, в общество сукомия буквально могут все это запанить. Были случаи, когда компьютеры, допустим, путали просто там пол, да, и по, допустим, клетку, да, на полосу B&D, поэтому было видно, что человек четыреть, но компьютер просто перепутал полосу B&D, и поэтому у него проиграл вот это 8, и так его выясняли, что человек четыреть. Вот, прежде всего, очень много таких случаев было. Ну вот, плюс шахматы это все-таки такая штука, которая хотя бы сложна, но перебора в доступное. Дафига будет вариантов всех возможностей событий, и еще может закончиться партия, но перебираем, может около триллиона будет, если партию из 20-ти подобств каждого игрока рассматривать. Вот, De Blue, потом в 98 году создалась красная история, когда Янли Кун, это французский математик, компьютер-санитист, он создал алгоритм, который по сетке 10 на 10 вроде, пикселя или 32 на 32, нет, несколько было линетов, линет это модель, которую он создал, которая могла предсказать, какая цифра, какая рукописная цифра, какой цифре действительно принадлежит. Я вот здесь пример сверху увидим. Ну, короче, разные размерности приходили изображения, он был 28 на 28, потому что был 32 на 32, я не помню, 10 на 10 было или нет, вроде 10 на 10, не нависовать. Ну, короче, в очень маленьком размерности, примерно как здесь вы сейчас видите, рукописная цифра писалась, и он предсказывал очень точно, за 97% вроде точно с него вышла, какая цифра это написано. И самое прикольное, что Ликун для этого со своей рабочей группой, если это не один человек, это была при университете целая команда, которая делала просто Ликун руководителем и являлся и создал алгоритм, на котором сейчас держится весь компьютер Vision, он называется свертка, либо convolution в английском, это условно подход, как мы в нейронную сеть можем подавать изображения, так чтобы с этих изображений на каждом уровне нейронной сети мы с вами вытаскивали все более-более-более полезные и более сложные абстракции или закономерности. Вот, короче, это человек, который придумал мне свертку. Самое прикольное у него было в этом году, даже этим летом, война с маском, это было очень забавно читать эту переписку в твиттере, как человеку из профессии, потому что Ликун сказал про маска, маск ему ретвитнул, типа да что ты вообще сделал, условно твои свертки вообще не используем в компаниях, а типа ты что-то сделал, Ликун ответил ему, либо да вот я придумал свертки, на которых все работает, маска ответила, нет мы свертки не используем, а в том, что ты, когда компьютер Vision занимаешься обработкой изображений, либо видео, ты в любом случае используешь свертку, потому что лучшими нет ничего для обработки, особенно на первых слоях любой свертки в твиттере. И было, наверное, очень весело в команде разработки Иоанна Маска в Тесле, когда ей надо было переписывать, чтобы не использовать свертки. Плюс сейчас Ликун, я не помню, то ли он CDO, то ли он CIO, или как он себя называет, в Фейсбуке сидит, в Мете, либо он ушел год назад. Вот, но он получил премию Филдса, премия Филдса по математике считается как условно нобелевкой, потому что какой-то из версий нобелевки премию нобелевки по математике не существует. В 2000-х годах было три самознаменательных кейса. Первое, с точки зрения архитектуры научились тренировать нормально нерванные сети, условно тренировать их еще начали вот в начале 90-х, к концу 80-х человек Хинтон придумал обратное распространение ошибки, придумал также чейн рулл, по которому можно оптимизировать нервные сети, условно там типа натревать нервонку с пятью слоями перестало быть проблемой. Там была огромная проблема с индициализацией весов, очень долго с ней думали, как ее делать, и поэтому там не всегда нервонки заводились. И в нулевых годах, как словно, эту проблему решили, как ее подавать нормально в оптимизацию, через день на спуск работает. И в нулевых годах стало возможно тренироваться нормально нервонки на данных. До этого это все-таки было какое-то искусство, чтобы это все завелось. Потому что там предопработка, предтренировка использовалась, много страшных шабов, которых мы бы сейчас уже с нули сделали. В нулевых годах был найден нормальный pipeline работы сетками, именно в том, что мы при процессе данной переломим, нормализуем условно, даем сетку, берем нормальную лосфункцию, и вот условно в таких супер простых шагах мы ее типа дотренировали. И понятно, как нам улучшаться в момент, когда нам надо либо выбрать получше функцию, либо нам нужно сделать просто побольше сетку, либо нам нужно просто добавить больше данных. Короче, это свелось до меньших параметров, по которым мы можем улучшаться. Раньше было сильно больше, поэтому это было искусственно что-то сделать. Например, когда делали пункты, это было искусственно сделать. В девятом году произошло первое событие первых хоккатон, условно, за очень большие деньги за миллион долларов, которые объявил Netflix. Netflix работает с 90-х годов. Только в 90-х нулевых годах это было не Netflix.com, это было пойти арендовать кассету. Понимаешь, те, кто 90-х годов рождения и не старше, может помнишь, что можно было в детстве пойти в нулевых годах и сделать кассету на прокат. Netflix этим занимался, он давал кассеты на прокат. То есть, возможно, тот чувак, кто-то магазина, он мог бы создать свой Netflix. Потом они задались вопросом в том, что хотелось бы как-то рекомендовать какие фильмы брать, чтобы с большим вероятностью люди подсаживались к наркотикам, наверное, просмотров фильмов, с жадной рекомендовать хорошие фильмы, чтобы с большей вероятностью они брали, потом еще возвращались, еще брали, брали, брали, брали больше фильмов. А за каждое такое братье фильмов, они, например, зарабатывали деньги. Нулевых годах он запустил первый сайт, и вот я не помню, тогда в него был просмотр или нет, или он в году 12 появился. Вот. Ну, короче, Netflix нужно было системы рекомендаций, какая-то лучше, чем они до этого использовали. До этого они использовали что-то типа топа, по топ, топ, топ фильмов за неделю, за месяц или в регион, какие-то фильмы больше всего берут, вот типа они рекомендовались. Вот. И они объявили хакатон, типа вот, мы платим миллион долларов для тех, кто вот по таким-таки метрикам условно придумает алгоритм рекомендательных систем, чтобы рекомендации стали лучше по этим-по этим метрикам. Вот. И чем это знаменито? В том, что это был самый дорогой хакатон, потому что за него на хэкзо платил миллион долларов победителям. И второе, с него можно начать. Начался такой практически бум хакатонов и компетишенов в области email и ds. Почему это важно? Потому что много чего из этих хакатонов и email-ных компетишенов оно потом вылилось в вещи, которыми мы пользуемся, да, и модели, которые мы используем, или артификтуры модели, которые мы используем, или методы тренировки, которые мы используем для них. Вот, например, там после этого появился Kaggle, да, вот форма, надеюсь, которую вы уже слышали или знакомы с ней, на которой можно соревноваться. Вот. Я, например, на Kaggle был экспертом, пока мне не забанили мою почту, и со второй почты я не хотел уже дальше продолжать. Но дошел до эксперта, это считается круто. Вот. Потому что следующий, остается только последний этап, это, это игра, не на эксперта, я до мастера дошел. Следующий, последний уровень, Gradmaster на Kaggle. Вот. Славно мастера дают за одно золото, либо три серебра, либо пять бронз, вроде. Это, это приз Netflix, плюс еще был создан GPU, точнее GPU в 90-х годах, в 90-х годах они стали доступные, появился большой спрос на GPU, потому что геймеры начали играть в очень много игр, так классно получилось, что для многих алгоритмов в ML, GPU действительно ускоряет обучение, причем так значимо. Вот. Особенно для нейронных сетей, которые сейчас являются условно базой в решении большинства ML-х проектов. Вот. Также появилось много вещей крутых в нейронных сетях, в которых мы с вами поговорим, наверное, на Curse.NLP. Первое занятие у нас будет о том, как работать в нейронной сети, и поучим в торчей их как делать, и тому прочее. Такие вещи, например, hydropower, batch normalization тоже из нулевых тедов, которые действительно помогают сегодня тренировать устойчивые нейронные сети. Вот. То есть их очень хорошо генерализировать. А в десятых годах бум практически в ML, да, то есть основывается Kaggle в десятом году, буквально через 4 месяца после приза. В 9 сентябре, как я вам сколько помню, Эндрю Нэн, это один из гуру преподавания ML-а. Любой уважающий себя ML-щик, наверное, проходил курс его на курсере про нейронные сети, про диплерн. Сейчас у него тоже дофига классный курс появился у LLM-ки, например, на диплерн AI. Классный курс, в котором мы с вами тоже брать какие-то нюансы и перерабатывать его курсы на Enabled. Вот появляется Kaggle. Создается Siri в 11 году, да, на iPhone становится доступным также. Создается AlexNet. AlexNet это модель, которая была обучена ImageNet. ImageNet это такая большая гугловская база размеченных картин, на картинке что находится. И тогда сделали классный кейс обучили нейронные сети, которая точностью 74, 75 процентов могла предсказывать, что на картинке находится. Это было тогда в ауте, тогда воспользовались большим объемом данных, которые надо было обучать, вольнотрабайты и данные, на них надо было обучиться. Вот с 12 года вот условно пошла аэродиплерн, действительно глубоких и больших нейронных сетей. Так, помним про поднятие руки. Как еще раз курс называется? Куру ML, очень популярный курс еще раз. Про e-learning, это то, что вот у него курсы есть на курсере. Вот, вот этот чувак, я не знаю, где он живет. И его курс, здесь есть искусственный интеллект для всех. Вот этот пункт, когда у него только один здесь доступный был. Он, кстати, один из создателей курсера. Это один фаундер-курсера, один из таких фугуру преподавания. Вот, он у него сейчас просто огромное количество разных курсов. Курс был короче по нейронной Вот, я когда его проходил, там надо было программировать на ужасном языке программирование Octave. Просто ужас. Вот этот ссылок, который скинули. Соответственно, его курсы, курс в этом все. При преподавании ML, e-learning и прочего. В 2012 году создается AlexNet. Условно с этого начинается эпоха больших нейронных сетей. Эпоха действительно глубоких нейронных сетей, который, я понял, стал большим. В 2014 году, ну и в 2019 году, благодаря нему, сейчас условно тоже благодаря нему ездят автопилоты. AlexNet, например, создал и соц.кейвер. Снаменитый человек. Тот, кто сместил сам альфмана на 5 дней в ноябре 2023 года. Все статьи, которые через 3 месяца ушли. Его учтей из компании. В 2014 году, когда еще инженер Толик Булат или Хейсбука, забыл. В 2014 году был чешский исследователь. Создает вортовик. Модель, на которой изъезжен сейчас весь NLP. И с 2014 года начинается эра NLP. Когда уже не проблема оборватывать тексты и тому прочее. Появляется классный ЧА-2. В 2014 году прошел костюлинга в нормальном сетинге. Так, чтобы к сеттингу не предаться. В 14-го года уже тастирование про дин, точнее, не способно проходить. Ставить моделей, которые проходят его. В 2017 году появляется Альфа Гол, который побеждает КАЗЕ по Гол. Самое интересное, после победы Диблю, модели над Каспаровым, сказали, ну ладно, там типа шаг модель, просчитываем мы. Если победят Гол, тогда действительно будет искусственный интеллект. Победили Гол, они говорят, ну ладно, Гол, окей, это не искусственный интеллект. Но если вот это, то это будет искусственный интеллект. Это мнение диванных экспертов. В 2018 году Google выпускает BERT. BERT это короче арситуры. Еще в 17-ом году прочего вышли трансформеры. В 14-ом году вышли аттеншена. Аттеншена, которых, например, сегодня построен Charger 5. Он построен на арситуре трансформера, который вышел в 17-ом году. BERT это была первой моделью, лэмбидж модел, который использовал трансформеры. А трансформеры, в свою очередь, построены на механизмы внимания, который вышел в 14-ом году. Вот. То есть типа вот сейчас мы видим пик реализации арситуры трансформера, который в 17-ом году была придумана в виде Charger 5. То есть мы просто оптимизируемся, оптимизируемся, оптимизируемся. В 20-ом, 21-ом году DeepMind создала AlphaFold второй версии. Самое прикольное, сейчас доступна третья версия AlphaFold. Она вышла буквально этой весной. Она короче в мире медицины популярна, потому что она позволяет очень точно, со второй версии 100% точно, реконструировать 3D структуру белка для того чтобы экспериментировать по влиянию, например, комиссийских приправ, и тому прочее, прочее, прочее. Вот. И вот с 21-го года они дали 100% прочность. То есть они не ошибаются. Причем это значит невозможное решение аналитическим путем. Поэтому для этого e-mail используется. Вот. С 20-х годов там вообще все, историю очень трудно будет написать. Нам надо будет, не знаю, неделю, чтобы сказать, что в 20-х годах было делано. Ну, если вкратце, вот только весна 23-го года. Огромное количество событий, условно, которые происходили раньше только раз несколько лет. А потом еще какое-то продолжение. Ну, в принципе, все свидетельники. И самое важное, днобелевское премиал в этом году сразу двум. Вот. Ладно. Двум важным ученым в области искусства интеллекта, нейронных сетей, e-mail, data science, up to you, as we want to call it. Hopfield придумал нейронные сетки, которые были, короче, очень сильно помогли мне в некоторых решениях физики. Но не прямо так, что везде. Вот он, словно, сделал похоже на реальный метод реализации нейронной сетки. Hinton придумал back-prevocation, обратное сопоставление ошибки, благодаря контролируемому человеку нейронной сетки сегодня. Плюс Hinton, блин, не знаю, он воспитал огромное количество крутых ученых и практических чуваков, которые в нашей сфере работают. Например, он был профессором. Его аспирантом был Glesadskiever. Его аспирантом был Andrey Karpaty. Плюс... Плюс Brockman. Плюс... Они причем учились в одном университете. И Маск чуть не пошел к Hinton, он тоже работает. Но он и переехал в Калифорнию. Но он тоже учился в втором-то университете, откуда как раз Hinton, где лабудет, лежал до сих пор держит. Вот. Плюс прикольный мим, да, о чем думали эмейл-специалисты, когда прочитали эту новость. Вот. Вот. Вторую партию нобельской премии дали по химии за сознание как раз AlphaFold, потому что она динамически изменила биологию практическую. Мне кажется, это самый быстрый кейс получения нобельской премии. Потому что AlphaFold 21-го года, Nubia Nubia 24-го года. В 21-го года это стало условно больше, кастиська была больше 2 миллионов использований уникальными пользователями AlphaFold в открытом доступе в мире. Примерно столько же как раз и специалистов, которые работают условно практиками на затонии лекарств и тому прочее в мире. Условно каждый специалист в этой области ее использует. Вот. И они как раз дали нобель по химии за это. То есть в этом году мы увидели прям, мне кажется, очень классное признание искусственного интеллекта, нейронных сетей, машинного обучения, дайте сайенс, потому что две нобелевки в одном направлении. Вот. Будущее я и, я думаю, все примерно понимают. Его будет просто больше еще где. Как один из вариантов, это будет просто замена людей. Да. Смотри. Мы уже с вами просто я думал, мне будет мало о чем говорить, а оказалось, ничего к нам не успеваем. Окей. Следующие два модуля мы быстро обсудим в следующем занятии. И на следующем занятии, что мы с вами будем делать. Мы с вами пройдем тогда IMEI Design. Поймем, какие этапы должны быть в практической точке зрения. Сегодняшняя у нас разговор так все обо всем. В DSE, в IMEI. А в следующем занятии мы с вами попробуем порешать пару практических кейсов. В том, с каких этапов оно должно состоять, какие подводные камни на каждом этапе у нас будет находиться. Попробуем разбирать их на реальных задачах. И последнее, чем мы займемся с вами, мы с вами поговорим про IMEI. Кейс, который может вам моделировать чуть-чуть лучше. Вот. Какая основная цель на следующем наше занятие? Так, чтобы вам было легче выбрать ваш проект. Эмейльный проект, который у вас стоит. И в том, чтобы в какой-то часть ложных путей реализации этого эмейльного проекта вы избежали. Потому что, вы видели на примере, так делать лучше не надо. Потому что это вот это у нас является. Вот. На сегодня, в принципе, помните материал, а тогда все. Давайте ответим на ряд вопросов, которые у вас сегодня возникли. Поднимайте руку или пишите в чат. Любые вопросы, ответ будет у меня. Есть ли вопросы? У меня есть вопрос. Не по теме сегодняшней лекции. Что у нас будет вообще по всему курсу на науке? Такой краткий экскурс. Чего мы научимся вообще по окончанию двухмесячной программы NLP? Окей. Давайте расскажу. Короче, с чего мы начнем? У нас есть три модуля, которые мы на курсе NLP создали. Первые модули мы с вами обсудим. Вот NLP плюс BirkBase Models. Мы с вами обсудим вообще, что такое NLP. Мы с вами обсудим, как работают нейронные сети. Потренируем маленькие нейронные сети. На первые четыре занятия. Потом, ну условно, для вас не будет проблем. Что-то делать в торче. Или там легкую нейронную сетку накисать с нуля. И там для какой-то задачи ее уменить. Вот. Потом мы с вами начнем погружаться в терми, в домен NLP. Что такое обработка естественного языка. С вами поймем, как работают архитектуры языковых моделей. Потренируем языковые модели. Поймем, как их с вами адаптировать. Держитесь правее. Окей. Буду держаться правее. Вот. Поймем, как их адаптировать под ваши задачи. Да. Пару задач попробуем порешать. Следующее. Разберемся, как работает распознавание речи. Посмотрим, как создавать модели распознавания речи. Как их адаптировать. Подключать к продакшналу. Как вычислить, как изменить пользу. Тоже самое про синтез речи. Чуть-чуть затонем обработку естественного языка. О, обработку аудиосинала. Потом у вас начнется курс по LLM. Это вот вторая часть. Поговорим, как подключать API, условно, к ChargPT. Как ей пользоваться. Какие там техники есть. Как вы можете использовать для этого модель. Какой делать действительно хорошим. Как применить арагии всякие. Короче, большая часть. Половина курса у нас будет, условно, не про какие-то специфичные домены. А вот как конкретно работать с LLM для того, чтобы на основе них какие-то решения проромировать. То есть на выходе, что вы получите, на выходе вы будете готовы для того, чтобы забрать любую готовую модель, которая сейчас есть. И использовать ее для вашей задачи. Как ее использовать или нет. Как ее поменять в том, чтобы она, условно, понимала казахский или русский язык. Как сделать так, чтобы она нормально работала. Как ценивать результаты. Потом как работать то же самое с ChargPT аналогами. Чет от GPT. Как мы можем какие-то кейсы автоматизировать. Как делать так, чтобы это эффективная была работа. Ну и много кейсов с вами проезжаем на этом пути. То есть, вот, кратце. Про NLP и про Computer Vision. У вас примерно то же самое будет научиться работать с изображениями. С видео, как генерировать изображения. Как, условно, распознавать что-то на изображениях. В том и прочее. Вот. Фишка в том, что вот эти следующие два модуля они будут и супер-супер-супер практичными. И будет довольно много теорий. Поэтому за теорией это точно не на этот курс, а на другой. Здесь мы очень много вопросов решаем. И я вас заметил, что в программе NLP вы будете... То ли в программе NLP, то ли адаптация LLM. Будет библиотека Лора. Могли бы... Мне вот она интересовала очень давно, но такие руки не доходили. Чтобы ее прочитать в документации. Могли бы вкратце объяснить, почему она нужна? Да, Лора это не совсем библиотека. Лора это все подход. Это подход к обучению. Это подход к как можно зафанчонить очень дешево какую-то большую модель. Чтобы вы понимали, модель LLM весит не предлично много. Ее ни на одном компьютере не воспроизвести. Да. Словно возьмем открытую самую большую LLM, которая есть. Например, Lama 3.2.405 миллиардов параметров. Чтобы ее воспроизвести в самом сжатом виде, где у вас каждый вес будет хотя бы 2 бита. Я 2 бита весить. Вам нужно где-то под 1 байт видеорама. 1 байт видеорама это условно 8 с инпутом нормальным. Чтобы вы могли с контекстом работать. Это условно 8 H200 видеокарт. Каждая из них стоит по 200 косарей. Лям 200 он надо. Лям 800 вам нужно в долларах. Просто чтобы купить видеокарты, чтобы воспроизвести эту модель. Очень дорого. А вот чтобы это фонтюнуть, нажать на 4. Если просто Influence такой дорогой, то для того чтобы модель дообучить, супер дорого стоит. Lora такой классный кейс, когда вы можете переобучать модель не всю, а вы ее переобучаете только на каждом слое по отдельности. Переобучение на слое. Он приоткручивает слой, зажимает его до одного нейрона, переобучает только в третьей связи. Фонт идет дальше, фонт идет дальше, фонт идет дальше. Lora вот так работает. Есть там еще всякие аналоги типа Q.Loy. Это квантизированная Lora. Она еще чуть экономнее. Техника экономного переобучения языковой модели. Она может работать не только на языковой модели, а на нейрона сети. В библиотеке, насколько я знаю, нет Lora. Потому что зачем она нужна, если я могу в торте просто реализовать. Или в Jaxx, или в Tanzarkflow. Но я лично не попросил никакой библиотекой, потому что там можно все сделать за 200 строчек кода на торте. Мы получается на торте будем все это делать. Или раз на выше. А, все понятно. Ну, смотрите, большинство продакшен, рейди моделей выходит на торте. Поэтому тортик вы должны понимать. Поэтому будем работать. Все-таки это является более благородным работать в индустрии на торте, чем на чем-то другом. А на Tanzar? На Tanzar RT, или Tanzarflow. Tanzarflow. Tanzarflow. Ну, меньше 5% выходило 5 лет назад. Кстати, которые Tanzarflow использовали. Tanzarflow классно об этом. Там Onyx появился для того, чтобы вкрутку на разные архитектуры, девайсы можно было проявить. Там заполнено, сколько помню, не поддерживается с 22, 23, 21 года. Это проект. Да, нас библиотеками еще ошибки выдают. Ургаются постоянно. Tanzarflow, короче, будет, когда вам на Android надо задеплоиться. Там Tanzar RT появился, аналог. Плюс Onyx давно это умеет хорошо поддерживать. Я не знаю, что это пишет на Tanzarflow. С моего хуба общения не знаю ни одного, кто бы с 22, 23 года на Tanzarflow ничего не делал. Короче, тут такой старый кейс, он уже давно не поддерживается. Там Google выпустила Jaxx вместо него. Вот Jaxx, я считаю, что это уважаемая штука. Но я знаю только пару компаний в мире, которые работают с Jaxx. Например, вот Jaxx. Это стартап Маска XeI, который в рекламе состоит. Но Jaxx работает. И Jaxx знать. Да, следующий вопрос. Я не увидел, кто задал вопрос. Но если вы работаете с Tanzarflow, если вы с Jaxx, он проще, чем Torch. 100% проще. Но там Torch один раз понять, потратить несколько часов. И открывается огромный мир. Что к бизнесу стоит знать, чтобы быть крутым датасантистом? Что в бизнесе можно заработать больше, чем датасантист? Или что, имеете в виду? Чтобы в принципе нормально работать. На найме хотя бы. Когда вы даже... Ну ладно, короче. Я не знаю, что сказать. Я не знаю, что сказать. Ну ладно, короче. Любому, кто занимается задачей решения ML или адаптации AI, датасантистам. Ему важно понимать бизнес-задачу на уровне, как бы ее понимал Project Manager. То есть это довольно высокое погружение в бизнес-логику. Вы должны точно понимать, какую бизнес-задачу вы решаете. Тем более, если вы один решаете эту проблему. Если в команде нормальный ML построенный, тогда не так важно специалисту знать, как бизнес-задача выполняется. Какие у нее подводные камни есть, какие процессы внутри построены. Какая перетечка между ними. Задача. Если вы один ML-специалист или AI-специалист, который работает над ML-проектом, тогда вам 100% нужно понимать, как бизнес работает. Как задача работает. Во-первых, потому что, мне кажется, треть всех задач до AI-меля неправильно ставится. Она там вообще не нужна. Третий задач. Никто не задается вопросом, что нужен здесь AI или нужен здесь ML. Нужен здесь, здесь или нет. Просто такие, окей, у нас есть бюджет на следующий год. Вот нам надо его куда-то потратить. А давайте тоже себе закинем AI. Я не самому лучшему продукту онеру, или руководителю направления. Он его забирает, тратит очень нерациональные деньги и создает команду. Они непонятно чем занимаются. И благодаря этому, одна из причин, почему 87% любых ML-проектов даже до депонентов не доходит. Поэтому, если вы решаете кейс один, если вы один solo делаете проект, вам точно надо понимать, какую бизнес задачу решает сейчас применение модели. В одном уровне, если вы руководили командой разработки. В каком уровне нужно понимать. Если в компании настроенный ML, например, яндой, Google, наша компания, я работаю, то вам не так сильно важно понимать весь бизнес. Это будет очень круто, если вы будете его понимать. Потому что благодаря этому можно сэкономить много времени. Можно не работать на некоторых гипотезах, которые просто не надработают. Потому что они в бизнес процессе никогда не смогут строиться. Поэтому бизнес понимать нужно. Какие примерно скилзы или навыки? Знаете там знания? Примерно. Примерно что? Какие знания? На рынке ценятся или что? Или нужно для работы? Для работы. Если ты в solo в проекте. В solo в проекте? Вот здесь все было понятно. То есть умение общаться с людьми. Вы должны не быть точно человеком, с которым не хочется общаться. Тем более, какие-то инсайты рассказать. Поэтому наигодной улыбкой надо учиться. Как минимум. Там много чему надо учиться. Словно сообщение надо вытягивать полезные инсайты для себя. Чтение статей. Скорее всего это знание очень хорошего уровня английского языка. У меня, например, я не беру ниже B1-level. Потому что все в MLE, VA и DSE, оно может к счастью, может не к счастью. Но надо делать в английском языке. Умение работать с гитом тоже очень важно. Потому что многие вещи можно просто готовые взять. И просто как-то адаптировать для себя. И вот этот курс в основном построен на том, чтобы мы в первом приближении могли довольно качественно применять MLE. То есть либо брать какие-то готовые вещи, их очень правильно адаптировать. Либо мы какими-то быстрыми способами могли быстро проверить пикотису. Словно, здесь MLE нужен или нет. И как он будет работать. Мы сейчас не говорим на этом курсе. Целью не является создание суперкрутой модели, которую 99.9 период будет читать. Нет. Мы скорее здесь про кейс, который предпредит. И быстро проверить правильно и довольно качественно. Быстро проверить гипотезу. Вот здесь MLE поможет или не поможет. Плюс сейчас слишком огромное количество готовых реализаций есть. И с нуля холитьев. Это очень дорогой процесс, который для маленьких компаний, для средних компаний довольно дорогой. Хороший ТС или MLE Search, или Data Engineer. Специалистами стоит довольно больших денег, чтобы команду набрать. Хороший пешпло в свободном норме. На Exploriter и Data Analyzer нужно знать статистику, работать с Python, уметь программировать. На модели мне надо знать алгоритм машинного обучения, уметь читать статьи. Потому что многие гипотезы и состетей надо будет читать. Например, вот эти чуваки вот эту функцию использовали. И сюда я прикручу. И будет всего лучше. Кейс надо на моделирование решать. Плюс какие-то вещи с моделирования можно забрать с открытых проектов. С GitHub, например. Или статьи на Medium. Можно найти туториал какой-то. 5% на Python делается. 70% на Data Sentinels и Data Engineers. Работа через U-Pyter. Какая-то часть в S-Code работает. Еще какая-то часть. Очень странно работает. Типа VIMO. Плюс моделирование. Важно работать с видеокартами. Потому что там есть свои нюансы, которые надо знать, когда ты работаешь с GPU. Это не как работать на обычном CPU. Как минимум у вас зайдет в часто использованные команды NVIDIA SMI или GPU Start. На Deployment все очень просто решается. Попадет пару API накинуть. Или чуть сложнее, если там лифтсинирование нужно. Мы это все с вами в следующем занятии. Нюансы на каждом этапе есть. Если кратко, то по стеку надо уметь программировать. Понимать матан. Не обязательно знать его так, чтобы можно было преподавать в универе. Но понимать, что дисперсия, насколько разнообразие это данных. Дисперсия, например, если распределение смотрим, нормально распределение, насколько оно широко будет. То есть такое понимание математики. У дешмата должны быть дискональзеры, инвалиды, вероятности, статистики. Это точно должно быть хорошим для инженера. Не потому, что он каждый день будет решать линейные уравнения, а потому, что много где надо будет логику проверять. И потом, чтобы не выяснялось, что он неправильно интерпретировал процент. Или неправильно построил график. Потому что у него логики не хватило, какой график здесь нужен, для того чтобы ответить на вопрос. Поэтому хотя бы базовый понимание математики нужен. Поэтому в отрасли работают люди, которые училися в инженеров. Близкая с математикой что-то, а не serious pudency. Вот. Ответил, да, вроде, на вопрос? Понял. Спасибо большое. Ну, короче, базовый программировать, базовый понимание математику это достаточно будет для того, чтобы начать что-то полезное делать. Если хотите условно получить работу в этой сфере, тогда там условно надо точно знать нормальное программирование. Условно тот же самый Python алгоритм и прочее. Надо достаточно неплохо понимать статистику. Нужно будет знать ML алгоритмы, как они базово работают. Если мы говорим про работу я-инженера, который с фокусированным за вкус. Практический опыт работы с готовыми моделями. Как их деплоить, как инфинитить, как их адаптировать. Как условно тот же самый Charge PC через API прикрутить. Как условно фотоагента из него сделать для решения какой-то проблемы. То есть про этот курс. И сейчас будет очень много позиций на я-инженеров. Вот которые как раз в основном заточены в том, чтобы взять что-то готовое. И из этого на этом готово приносить максимальную ценность компании. Бизнесу управлять гипотезой, новым функционалом выкатывать и прочее. В чем условно 2 года сидеть пытаться. Думаю новую архитектуру. В кейсах я-инженеров, вот это будет доля C на меньше. Это будет скорее где-то под 30-40%. Может под 50, как у FB свои. Поэтому я нашла и нацеленную курс. Чтобы мы сели больше по делу бизнеса. Что по бизнесу? Окей, это я. Так, и давайте последний вопрос. И наверное сегодня будем заканчивать. Сегодня у вас условно такой ритрит. Дотокс от написания кода. Да, Жан? Так, по поводу GPU и его масштабизации. По каких-либо проектах. Вот допустим я. Мы начинаем сейчас с каких-то простых ML, классик ML. Потом может быть на NLP перейдем. Компьютер-ривизинг какой-то. И может быть какая-то идея. Кто-нибудь стрельнет до какого-то стартапа. И в какой момент проект должен масштабиваться, закупать GPU. И когда финансирование становится критичным. Сможете как-нибудь прокомментировать? Окей, это вопрос из ряда. В какой момент повару надо использовать печку? Вот когда ему надо что-то выпить. Ему надо использовать печку. Для проверки может... Короче, GPU это сервис, на котором ряд алгоритмов машиного обучения работают достаточно быстро. Могут ли они работать на CPU? Могут работать на CPU. Будет ли это хуже? Будет хуже. Когда критично, что это сильно хуже работает. И недозволительная компания обычно приходит на GPU. Но опять же, переход на GPU. На GPU он работает только для алгоритмов, которые держат параметризирование. Из алгоритмов, которые вы прошли, это Random Forest. Это алгоритм, отличный на GPU, раскладывается. Практический пример, когда Random Forest надо было на GPU раскинуть. Я занимался банерной рекламой когда-то. В 2019-м, я работал в компании, которая делала алгоритм, который по хупам предсказывала, какую банерную рекламу вам сейчас показывать. И модель у меня получилась на Random Forest. В ней нужно было... Она в день отрабатывала порядка 400 миллионов уникальных пользователей. И вот на этих 400 миллионов уникальных пользователей нужно было предсказать сегмент. Это делалось через Random Forest, потому что тогда это более-менее оптимально работало. С лучшими значениями метрик. И вот тогда, да, Random Forest я сначала запараллелил на CPU. Это прямо из-за скалерной можно... Когда Random Forest запускаете, там есть какие-то параметры. Это как-то из вас. On jobs. Не underscore jobs. Вот, и вы делаете минус один, у вас на все треды CPU выкидывает вычисления. Потом я на GPU перевел, потому что на GPU тоже классно проглелится. Вот. Ну, например, линейный регрессий, если у вас вроде крутится, и вы понимаете, что у вас там, типа, провод решения задачи зависит от линейного регрессии, то там нужна никакой GPU, потому что вы с него вот тут не получите. Ну, давайте так. В 80% случаев у вас это будут какие-то нейронки. Нейронки, да, они всегда на GPU идут. Ну, GPU, они обычно в двух вещах выкидываются. В отличие на большое количество GPU, в двух вещах. Первая вещь — это когда вам нужно... Инфинанс поддержан, да, условно, с миллион пользователей подходит, вряд ли вы через один сервер с одним маленьким GPU будете инфинансировать миллион разных кодексих данных, да. Соответственно, она будет парализировать. Ну, классно GPU это вообще выкидывать. Второй кейс, когда у вас слишком большая модель будет находиться. Вот, например, ЧАД-GPT поднять одной картой не хватит. Не важно, почти под 4090 или 4070. Надо будет, если в 4090 терминах говорить, для рандайма там ЧАД-GPT, может, 1... ЧАД-GPT 4, 1-2 крыльона. Соответственно, он надо будет где-то под... Где-то под 4... 4,8 терабайта видеорама. 4,8 терабайта видеорама — это очень много видеокарт. Не один сервер столько видеокарт не тащит. Соответственно, вам нужен кластер видеокарт. И вот тогда вам надо, ну, если какой-то тяжелый, какую-то махину модель заинференцировать. И то только одну, да, потому что каппализировать надо, если вам сказать, на грустный и кум прочим. Вот. То есть более вероятно отвечай на вопрос в этом кейсе, когда у вас идет масштабирование по использованию мр моделей. Да, либо это пользователи, либо это все сервисы, которые комитически считаются. Вот в этот момент надо будет GPU-GPU. Но, типа, курс не настроен на это. Курс не настроен на это. Берем бы есть специально обученные люди, которые закончили другие курсы. Этот курс скорее на первом нотаре, который вы можете найти в интернете. Так, чтобы не было 87% удачных проектов, так чтобы было намного больше удачных проектов. Надеюсь, ответим на вопрос. Если нет, тогда задам. Да, окей, спасибо. Это получается, мы весь курс можем проходить на наших локальных компах, и ничего рендовать не надо будет. Нет. Окей, спасибо. Вот. А если вы хотите, чтобы мы не раздавали этот курс, то вы можете сделать это. Да, я думаю, что это будет очень интересно. И вот, если вы хотите, чтобы мы не раздавали этот курс, а если и надо будет, то в плане топлива не выбраться. Я могу на своей крипе показать что-то. Вот. Вот как так. Так, это был последний вопрос. Всех поздравляю сегодняшним первым занятием со мной. Если понравилось вам занятие сегодняшнее, то просьба кинуть пальцы вверх. Чем более темного цвета будет этот палец маленький, тем, значит, вам больше понравилось. Если вам не понравилось сегодняшнее занятие, то просьба кинуть пальцы вниз, чтобы я понимал, что вот то, что я сделал вам, мне нравится, соответственно, что-то другое сделать. Вот. Если что-то среднее между нравится и не нравится, то кидайте любой другой смоли, который бы это проинтерпретировал. О, черные пальцы вверх полетели. Я не могу черный палец поставить. Потому что я не российец, да? Нет, там у меня нет такой функции, кажется, я смотрю. Мне тоже черный не получается, но я очень рад, что я получил такой эффект. Спасибо. Кайфовому тоже. Да, да. Супер. Если выходите палец вниз, поставьте тоже, не стесняйтесь. Наоборот, для меня тоже много, что значит. Потому что я буду знать, что я что-то делал не так, что я делал не так, что получится. Так, ну окей, супер. Со всеми был рад познакомиться, с кем успел пообщаться сегодня. Те, кто не пообщались, давайте на следующем занятии все-таки будем общаться. Есть уникальная возможность поговорить с человеком, который в этой сфере очень давно. И мне кажется, мне не может быть даже успешно. Вот, в этой сфере я 8 лет. После этого я заканчивал шат. Если знаете, 4 шат. Я был, наверное, четвертым казахом, кто заканчивал шат. Пластно. Первый был Аннуар Эмолдин. Второй имла Чистелли. Третий Женя был. Ну, третий, четвертый. Сейчас это... Ну, надо лучше. А что такое шат? Я, может быть, извиняюсь за такой вопрос, но... Полуанализа данных Яндекс. Да, да, да. Мне кажется, хардовая штука. Точно лучшее место в по-советском пространстве, где можно научиться эмелю. После нашего курса поедешь в Ниндю. Ну, короче, какая хардовая штука, которую надо поступить, надо просто быть нереальным бючматом. Я этот бючмат с третьего раза смог нормально сдать. И на котором очень трудно учиться. Там 2-3 люди вылетают оттуда. А я еще в это время совмещал с работой. И набрал 20 килограмм в своей жизни, который очень долго отталкивался. И я, наверное, в этом году,<|id|>, в своей жизни, который очень долго отталкивался, много медленнее, чем набираться. Вот. И научился, например, не спать три дня и как-то быть более-менее эффективным. Ну, это я более ценю Shadow, что я вынес. А алгритм Эмеля я бы и ехал там, можно научиться. А, ну, еще вообще крутые знакомства, может быть, ничьих. Очень крутые знакомства, которые... Например, я знаком с человеком, который сейчас возглавляет всю разработку по... Знаете, есть вот, словно, Gemini, да, вот, Chagy Pt от Google, Chagy Pt от OpenAI. И есть Lama, да, от META. Вот мне преподавала девушка, которая сейчас является главой разработки в Facebook. Ладно, всем рахмет, всем доброй ночи, увидимся с вами просто завтра. Еще раз на практических кейсах посмотрим, что как этот ML development работает и с чем выведет. Кап, всем пока. Спасибо. До свидания.