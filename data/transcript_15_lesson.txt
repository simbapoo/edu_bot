 Все нормально, да? Да, спасибо, скажешь, стал лучше. А ты что-то зомблогнул и странно себя пожалуешь. Так. Так, супер. Так, всех приветствую. Сегодня у нас с вами второе и заключительное занятие по топику, как нужно делать мэль-проекты, что нужно в них знать. И также сегодня заключительное занятие по мэл-инженерии, насколько я помню. И у вас начинается сначала Project Preparation Week и потом Project Presentation Week. Супер. Сегодня мы с вами продолжим говорить то, о чем мы закончили на прошлом занятии. На прошлом занятии мы с вами погрузились примерно как мэл сейчас. И я и дошел до текущего состояния. Сегодня мы с вами поговорим о том, какое сейчас есть разделение по направлениям, из чего состоят проекты, каждый проект, какие нюансы на каждом этапе могут вас ожидать. Как этих нюансов попытаться избежать для того, чтобы проект не затянулся на какие-то долгие годы. И в конце чуть-чуть я для вас приготовил маленький пример. Вы говорили, что с автоэмейлем не работали. Я для вас подготовил маленький пример, как работать с автоэмейлем. Вот. Банк должен быть супер полезен. Так, если будут занятия, если будут вопросы, то работаем по той же самой схеме и поднимаем просто руку. Плюс если что пишем в чат, я его читаю в мониторию. Поэтому если что-то будет в чате написано, то я обязательно отреагирую. Давайте начнем сначала с самого простого. А чем вообще занимаются в отрасли AI, DS, ML? И как условно это сейчас попробовать первоначально разделить? Сейчас существует четыре основных направления в DS. Условно как в 90-х годах были только программисты или софтфер-инженеры. И сейчас у нас есть разделение над вопс, фронтенд, бэкенд и тому прочее. И так далее. То у сегодня есть такое же разделение, которое началось, наверное, в середине прошлого десятилетия внутри DS Command. То есть в целом дата сайдс это условно. Вот. И есть четыре основные направления и виды задач, которые бывают в DS. Первое направление, которое есть, оно начинается с данных. Это все, что касается анализа данных. Это подготовка данных, поиск инсайтов данных, еще можно встретить название такое когда-то, майнинг и все прочее. Также здесь могут быть промежуточные результаты. Это проверка каких-то простых гипотез насчет моделирования СМЛом либо статистическим моделированием. Здесь очень много explorative data analysis. Здесь очень много создания всяких дешбордов и в реалтами условно мониторинг. Следующее большое направление, которое есть, ну и обычно люди, если есть потребность в DS команде, то такие вакансии обычно называются либо Data Analytics. Data Analytics могут иногда называть это как Data Science, но Data Scientists могут здесь все из этого обозвать. Вот. Плюс еще могут называть Business Analytics, что тоже будет включать вектор этих знаний. Также есть направление, которое уже ближе связано с Software Engineering. Оно называется ML Engineer либо Applied Scientists. Основная здесь его задача в чем? Это в том, чтобы условно заниматься переобучением моделей, условно передеплоивать какие-то open-source решения к себе в pipeline, либо пытаться оптимизировать где-то runtime, где-то использование памяти, настройка кластеров и тому прочее. То есть это все, что касается именно в том, чтобы сама модель была лучше, лучше со стороны качества и лучше со стороны инференция, то есть чтобы она в продакшене была хорошо. Также здесь могут ожидаться задачи, связанные с мониторингом, то есть условно предотвращение дата дрифта, предотвращение поиска всяких аномальных ситуаций, когда модель перестает работать. Это все может сюда включаться. И обычно такие вакансии называются Applied Scientists либо Software Engineering. Следующее большое направление, которое есть в DS, это соответственно дата инженерия. Это все, что касается припроцессинга данных, работы с большими данными, настройка на вход в модель данных, настройка на выход в модель данных. Возможно, сюда также будут включаться какие-то pipeline для разметки данных, соответственно, чтобы это все поступало довольно качественно и во вторую очередь, чтобы это было еще все эффективно и оптимально. В том, что не было просадок, не было задержек, чтобы нам не прилетали наны, не было дропов и тому прочее. И обычно такие люди называются дата инженером. Обычно любая DS-команда всегда строится с первых двух тасков. Это дата инженерных тасков и дата аналитических тасков. Потом уже подключаются эмэльные таски. И последнее направление, которое я хочу вам сегодня рассказать, это research. Это придумывание новых архитектур неравных сетей, придумывание новых эмэльных алгоритмов, придумывание какой-то новый специфический, специфический с точки зрения математики обработки данных для того, чтобы получить какой-то новый импакт. В словном, создание Chatch GPT, наверное, самый яркий пример. В словном, вот была GPT-3, потом что сделали? Потом создали идею Instract GPT благодаря Илье Сотскеверу, который возглавлял это все направление. И после этого, как Instract GPT показал в том, что он может работать, может выполнять инструкции, уже начали ввелить полноценный сервис, потом узнали, как Chatch GPT. По сути, альфа-версия это была Instract GPT, а альфа-APOC, Instract GPT, была Chatch GPT. Chatch GPT еще на второй версии GPT, это в далеком 2019 году было. Я помню, все в эмэле играли с этим и восхищались, как он может пародировать всяких персонажей. Я помню, даже создавал бота, который может общаться, как Рик из мультика, Рик и Морти, если знаете такой мультсериал. Я создавал бота, который может так общаться. С помощью GPT-2 я фонтюнил ее по инструкции, которую он создавал, и было интересно с ним пообщаться. Вот, в общем, основные четыре направления, которые есть в DS. Сейчас существуют еще новые дробления. То есть к концу этого десятилетия мы увидим еще несколько таких направлений, которые будут основными. И так, чтобы в DS команде были какие-то роли. Из новых направлений можно еще сказать все, что касается дата разметки, где называют дата ассессоры. И это довольно мутерная работа, на самом деле, размечать данные, но при этом ее полезность неоценима. Это, наверное, самое ценное, что можно сделать в DS на первых шагах. Есть ли вопросы какие-то по направлениям? Или хотелось бы что-то прояснить, выяснить, выяснить. Окей. Если что, давайте попробуем побольше с вами общаться, потому что мне немножко скучно только самому рассказывать. Интересно много отвечать вопросов. Например, MLOps – это получается настраивать pipeline, то есть это automation с ML? Да, MLOps – это примерно… На самом деле MLOps – это то же самое, что DevOps. Просто нам надо узнать пару их нюансов. Дополнительных как модели ранятся в том, что они должны сначала быть провалидированы, то, что должна быть аппо-валидация какая-то, дрифты данных, как происходит со временем. Обычно у нас данные со временем, если в real-time какая-то ситуация, то, не условно, кто-то что-то обновит, кто-то что-то по-другому начинает делать, и поэтому происходит у нас с вами дрифт данных. Соответственно, входные данные наши в течение начинают меняться со временем, и соответственно наши предсказания тоже меняются со временем. Есть классный стартап, называется Evently.EI. Я знаком с его основателем. Это… блин, быстро хотел сказать имя, но забыл ее. Короче, это Braille. Девушка… как она? Emily Braille. Она раньше рукой делала Yandex.DataFactory. Вот, и сейчас делает стартап в Лондоне. Классная штука, которая позволяет много что об автоматизировать в плане MLOps. MLOps, наверное, самая популярная библиотека. Тоже для MLOps используется от нее. Лично знаком. Ее муж, кстати, здесь, в Казахстане, Александр Радрайль. У него есть Big Data School. Здесь он что-то преподает. Вот. Не заинтересно. MLOps тоже самый для MLOps, и это больше направление в этом, в мэйд инжинирии и все, что касается софтверной части. Можно еще чуть-чуть из даты инжинирии пересекается. Ага, спасибо. Не забываем поднимать руки. Так. И можно видеть в том, что я сказал мысль до этого, в том, что условно раньше, нулевые 90-е годы, словно был один датасинтист, который все это делал. Это обычно был какой-то PhD-шник с большим, большим бэкграундом знаний. Их было там, словно, сотни по миру. И они могли там творить магию все в одного. Сейчас, к сожалению, ситуация не такая. Можно видеть в том, что есть узкая специализация. И там, например, человек, который умеет писать архитектуру нейронных сетей, обычно не умеет Big Data. Словно оптимально эффективно писать приплайные обработки данных. Или незнаком, например, с асинхронным программированием. Или как использовать мультипоточное программирование и тому прочее. Вот. Круто, когда есть данные с других сторон. У вас есть знания в структурах данных, у вас есть знания в базах данных. Опыт в программировании, особенно в многопоточном, асинхронном. То это обычно крутые скилы, которые позволяют намного быстрее делать при процессе данных или анализ данных. И позволяет работать намного с большими масштабами. Если говорить про основное наличие, то Data Science – это условно как можно работать с данными. У него есть домен в Data Science. Это модели машинного обучения. Это один из примеров, что можно сделать с данными. На вход обработаны в каком-то виде данные. В разных доменах все по-разному, в разных задачах по-разному. Условно задачи на домены. Такое количество вариаций есть подходов к решению эмальной задачи. И оно пересекается на какую-то долю с искусственным интеллектом. Искусственный интеллект – это такое понятие, которое говорит, что давайте создадим искусственный разум, который был бы эквалиден человеку. И он создается пока более очевидными методами из науки данных. На данных надо будет это все делать и с помощью машинного обучения. Экспертные системы не работают в этом плане в прошлом веке. Много десятилетий на этапы потратили. И нет, Multi-Trading и асинхронность – это разные вещи. Multi-Trading – это подкласс Multi-Processing, Multi-Processing и асинхронность – два разных варианта. Либо у тебя параллельно обрабатывается, либо у тебя асинхронно. Искусственный интеллект, скорее всего, будет создан с помощью машинного обучения. По крайней мере, мы сейчас учимся его имитировать. И во многих задачах довольно хорошо и даже лучше человек. То есть всякие задачи типа распознавания объекта, сегментации – они решаются точно лучше, чем человек решает. Но я думаю, на процентах 30-40 тасков уже человеческие результаты машинного обучения мы побили. Плюс есть всякая популярная штука под названием AGI. Есть сейчас разделение на weak AI, слабый искусственный интеллект. Есть понятие AGI или strong AI – сильный искусственный интеллект или общий искусственный интеллект. Если мы говорим про простые мэльные модели, которые могут решить одну-две задачи, то мы обычно говорим в том, что это слабый искусственный интеллект. Например, модель распознавания речи. Например, слабый искусственный интеллект, где мы решаем какую-то одну человеку возможную задачу. Если мы говорим про… На самом деле сейчас многие задачи не просто лучше в качестве, но еще лучше в скорости. Например, человеку надо где-то 200-300 мс для того, чтобы распознать другого человека. Например, своего знакомого, что я понимаю, о этом знакомый, или его зовут так-то. То у моделям, которые работают на серверах, нужно, как вы понимаете, сильно меньше времени на это все. Потому что можно это все запараллелить, можно это все ускорить. Это есть, словно, GPU и всякое прочее. Можно использовать разные также ускорялки в типе передачи данных. Например, от CPU до e-memory, до GPU. И, словно, через всякие шины их попробовать проводить. И вот сейчас уже много всякого такого именно на железном уровне было придумано и оптимизировано в сторону вычислений ML-моделей. Там яркий пример. Например, придумали новую шину, типа XSXM. Сейчас можно видеть видеокарты, словно, с этой определенной шиной. И вот для нее очень большая пропускная способность. В том, чтобы очень быстро на видеокарты данные попадали, и потом с нее же отдавались обратно на CPU. Поэтому сейчас ускорения много происходит. И, скорее всего, мы будем видеть все с каждым годом. Все больше и больше. Все более-более лучшие железы. Потому что вон по Nvidia, если только судить, за последние пять лет, тем более за последние два года. Каждый год удвоение или учтворение, условно, усредненного качества, которое они могут предоставить. Теперь сегодняшний самый главный вопрос, который мы должны будем с вами ответить. Как делать DSML-EI-разработку эффективным? И какие советы бы я дал, когда вы начинаете новый проект? Или когда вы находитесь внутри проекта, на какие аспекты нужно в первую очередь обращать внимание? Условно, любой ML-проект можно разделить на следующие этапы. Первый этап, который зачастую новые сотрудники довольно сильно обесценивают. Потому что считают, что вся ценность в том, чтобы создать какой-нибудь крутой модель, но на самом деле нет. Ценность создать хорошее решение, которое условно улучшит какой-то процесс. Или создать какую-то новую крутую фичу. Или, условно, даст компании больше денег. И она как-то зависит от крутой модели. И обычно это не в первую очередь, а в самую последнюю очередь. Самое первое, с чего вы должны всегда начинать, это с понимания проблемы, которую решаете. Вы должны понимать не только на уровне каких-то метрик, ML-ных метрик. То есть, в основном, нам нужна вот такая точность. Вы должны понимать конечную бизнес-метрику, которая оптимизируется. Например, увеличить скорость обработки данных. Например, условно оптимизировать алгоритмические стратегии, вычислить торговлю для того, чтобы они зарабатывали на таком-то чанке больше денег. То есть, у вас должна всегда быть связка ваша ML или DS-метрика, которую вы оптимизируете с помощью AI-инженерии. Если вы работаете уже в готовой команде, где есть DS-отдел, где есть хорошая CTO, то, скорее всего, эту часть за вас закроют. То есть, бизнес-метрика будет вам предоставлена. Если вы делаете стартап, если вы работаете одним, двум, триом, даже четырем дейсерам, тем более, если ребята малоопытные, то вам придется эту ответственность брать на себя. Соответственно, нужно будет много ходить, разговаривать, для того, чтобы до конца понять, какой процесс строится или какой процесс оптимизируется, и какие конечные метрики, оценки результатов, на которые, на исходя на эту бизнес-метрику, вы должны будете накинуть ML-метрику, которую вы потом будете в Мэйле решать. Следующий момент. Следующий этап обычно занимает процентов 60 всего времени. Это все, что касается сбора данных, обработки данных, очистки данных, интерпретации данных, построения некой дата-стори на данных, которые у вас есть. И после того, как мы условно решили все проблемы с данными, где-то, может, нужна будет разметка, где-то, может, нужно будет разметка, где-то, может, мы поймем, что данных у нас мало, и, соответственно, надо идти, думать, где нужно обогатиться данными, либо купить, либо с кем-то договориться, в том, чтобы они вам предоставляли данные, потом нужно проверить, насколько хорошо они у вас мёрзнутся с вашими текущими данными. И если с этим все будет, то можно приступать ко второму этапу. Второму этапу – это мэль-моделирование. Для него вы должны понимать, какой тип задач вы решаете, какую мэльную метрику вы будете оптимизировать, и у вас должен быть какой-то лист гипотез, который вы приоритизируете и проверяете по очередности. Все. Каждая гипотеза должна обязательно быть оценена, она должна быть эффективна и оптимальна, то есть вы не должны изучаться с проблемами такие, когда-то, leakage, когда, например, я натренировал модель, и на тех же данных, что я натренировал, я делаю выводы в том, что модель там типа супер классная, потому что 100% качество. Мы понимаем, что в мэле есть ряд проблем, которые мы избегаем, например, делением данных на тренировочные тестовые и валидационные данные. На тренировочных данных мы тренируем модель, на валидационных данных мы тюнем гиперпараметры и сверяем одну модель с другой моделью, и на тестовых данных мы финально с вами замеряем наши результаты. При этом валидационные данные не должны использоваться в тренировке, тестовые данные не должны использоваться не вальдац, не в тренировке. После того, как мы с вами замерили результаты, обычно это занимает процентов 10-15 всего времени работы на проекте, то мы с вами переходим к следующему этапу. Там следующий этап называется deployment. Мы должны с вами понять, на каком железе мы будем раниться, облако это или какой-то in-house железо. Мы должны понимать, какие у нас ограничения есть на runtime, на использовании memory, есть ли у нас возможность считать на GPU и нужно ли нам считать на нем или нет. То есть мы должны задеплойть сервис, чтобы он условно работал в режиме MVP, minimal valuable product. После этого мы можем переходить к main.tasks, проверять, чтобы у нас сервис не падал, чтобы предсказания создавались, чтобы весь pipeline новых предсказаний действительно срабатывал. То есть получение данных, обработка данных, результаты модели, потом пост-обработка результатов модели, если она нужна. И если мы должны на это все накидывать тесты, мне кажется, здесь есть какое-то количество программистов. Если есть, то поднимите руки или киньте какие-нибудь маленькие, чтобы я понял, какое у вас примерно количество. Кто работал, работает с front-ом, back-ом, DevOps-ом, может с аналитиком, системным. О, супер, какое-то количество из вас есть. Да, соответственно, мы в main.tasks еще накидываем всякие MLOps-ные штуки, если они нужны. И все, условно, сделали мэльную часть, и потом можем переходить к следующей итерации и оптимизироваться. Создали MVP, и потом начинаем улучшаться. Обычно к моменту деплоймента у вас уже есть миллиард дополнительных гипотезов, что еще можно поделать для того, чтобы еще стало лучше. Но не забываем в том, что, по сути, у вас должна быть одна итерация. Вы эту одну итерацию сделали, потом приступаете к тому, чтобы, условно, дальнейшее улучшение сделать. Если предварительные вопросы на данном этапе или комментарии. А если здесь те, кто уже работают от сенсиста, email-инженера, data-аналитика, может есть кто-то email-ресеучный, тоже киньте реакции или поднимите опы. О, кто-то есть. Ну, вот так же. Документируешь? Так, я видел, что кто-то поднимал. Вроде Нур Султан был. А твой вопрос был, извините? Я просто поднял руку. Если здесь те, кто уже в DACE работают. Да-да-да, я работаю ресерш-ассистентом. Получается Prompt Engineering. Prompt Engineering. Ну, вот. Это еще одно направление, которое появилось за последние два с половиной года. Окей. Что-то есть добавить? Ну, окей. Воспрямую. Это мне кинули хайп просто. Окей. По этапам, если мы будем говорить, то первый этап – это обязательно понимание задачи, второй – это сбор, чистка, обработка, и раздель. Третий этап – это всегда все, что связано с моделированием. Не всегда вы можете на третьем этапе решать задачу с помощью email-инженера. Скорее всего, это будет в большинстве случаев так, но иногда можно обойтись чем-то другим. Оценка данных. Здесь у вас оценка данных происходит не только в вашем email-метрике, но еще и бизнес-метрике, которую вы оптимизируете. Словно на оценке данных у вас уже готовый POC, а на дипломате и мониторинге у вас уже готовый MVP есть. И потом уже дальше уже можете определять нужная версия, следующая или нет. Вот. Вот так вот должно выглядеть все в идеале. Если соблюдается качественно, качественно каждый этап выполняется, то оно действительно так работает. Но, поверьте, в ближайшее N количество проектов, у кого-то и все проекты, они будут выглядеть вот так. Потому что мы с пониманием, мы условно делаем цикл, дошли, например, пропустили понимание задачи, пошли в сбор, разметку данных, поняли, что мы ничего не понимаем про наши данные, пошли общаться словно либо с бизнесом, либо с людьми, откуда эти данные приходят. Они нам рассказали, как эти данные устроены. Мы опять прошли в сбор, разметку данных. Потом начали с вами какую-то модельку, словно с Huggenfaces качали, на ней запустились, поняли, что нифига не работают, точности плохие. Потом опять пошли общаться с людьми, еще новые дополнительные нюансы узнали про данные, опять пошли их чистить, пересобирать. Оказалось, что нам нужны еще новые данные, пошли опять с людьми общаться, где эти данные можно достичь. Потом словно опять пошли их чистить, разбирать. Потом, если качество не удовлетворило, опять пойдем с людьми разговаривать. Если нет, пошли помоделировали, увидели, что стало что-то лучшее, начали оценивать, поняли, что не работает. Пошли искать новые модели, для них начали по-другому обрабатывать данные, начали проверять гипотезы. И вот так может происходить бесконечно, если каждый этап не выполняется качественно, либо если есть ряд других проблем, о которых мы поговорим в конце. Вот это плохо, этого стараться надо избегать, но практика показывает, что первое условное, там, н-количество проектов у вас такими всегда будут. Вот. Соответственно, поэтому существуют датаменеджеры, датасайнс менеджеры, поэтому существуют уже пайплайны. Короче, маляные задачи уже, наверное, тысячи команд по миру нарешала. Часть эффективна, часть неэффективна. И от того, что эффективно получилось, есть уже вейтлайны, как и на что нужно обращать внимание. Обычно этим не занимаются люди на начальных этапах становления в профессии. Обычно эта доля падает сеньорам или дамам, либо директору вашей команды. И вот на них обычно идет ответственность по дизайну, в том, что вы за проект, который вы берете, чтобы он хотя бы мог быть решен. Вот. Первое. Не забывайте в том, что DS, ML и AI проект – это всего лишь подкласс разработки ПО. То есть мы с вами разрабатываем какой-то софтвер. И есть еще другие, условно, софтверные этапы, которые должны будут проходиться. Это планирование, это формирование требований, проектирование архитектуры. Потом все, что касается ML части – понимание задачи, сбор, разметка данных, моделирование, оценка данных. Потом тестирование и интеграция в софтверный этап, там дипломат и мониторинг. То есть после того, как ваша ML гипотеза проверится, она должна будет пойти и быть нормально оформлена в сервис и ПО. И соответственно, здесь обычно подключается продуктовая команда, либо здесь она падает, если в Soli решается проект, то на тех же самых датсантистов. Поэтому очень важно понимать, как в Computer Science все устроено и как делается софтвер инжиниринг. Поэтому классно, например, люди, которые в DS переходят после того, как поработали софтверами, потому что они не смогут довольно качественно выполнить какие-то другие части, либо как минимум пойти их обсудить, а не так на уровне, что ну вот у нас это получается, мы не знаем, сколько оно по времени у нас работает, мы не знаем, что такое профилирование сервиса, а что такое автотесты. Это F1 почитать или что-то другое. Поэтому классно, когда приходят ребята с Computer Science, которые понимают, как создается плода. Обычно, если слабоватые понимания, то все. Вот. Теперь мы должны понимать, на чем все строится успех. Первый успех, если у вас нет нижней грани, то с высокой вероятностью, когда вы будете переходить в верхние грани, у вас будет не все очень хорошо. Первая это данные типа Data First. Вы должны запомнить раз и навсегда не Model First, а Data First. На основе данных можно потом придумать какие-то модели, а не придумать какую-то модель и пытаться за уши притягивать все остальное, включая данные. Это только все усложняет и ухудшает. Надо в первую очередь смотреть, какие данные есть и как эти данные можно использовать в пользу. После того, как мы прошли к сбору данных, мы должны понимать, что у нас нормально настроено хранение данных, что у нас есть нормальный Data Lake, что у нас нормально используется шардирование, что у нас репликация существует. Да, вопрос. Да, извиняюсь. Вот такой вопрос у меня появился. Я какое-то время занимался именно CV-проектами на Computer Vision. И мне всегда казалось, что там Data Set на набор данных по тысяче штук картинок хватает. А в практике вообще сколько всего можно? Самый лучший вариант, самое лучшее количество. Это какое может быть? Может где-то достаточно 100 штук, да? Опять же, зависит от задач. Как бы есть такое понятие, как ImageNet. Если мы говорим про задачу CV, CV это Computer Vision. Это домен машинного обучения, который занимается обработкой изображений и в лучшем случае обработкой видеопотоков. И там большой спектр задач, которые может быть. И на них всех нужны разные количество Data Set. И это такое, что мы с вами уже берем все. Какое огромное количество есть с вами направлений. Да, берем Computer Vision. Есть семантическая сегментация, тематическая сегментация. Есть representation learning. Есть задача и классификация объектов на изображении. Есть задача object detection. Есть задача классификации картинок. Есть задача в 2D, 3D и огромное количество другого всего. Если мы говорим про среднюю задачу, например, классификацию изображений, то есть правило, к которому мы уже пришли. В том, что нужно миллионы картин, для того чтобы более-менее что-то начало обрабатываться. Вот, естественно, нужно очень много данных. Для этого создавался ImageNet, когда много-много студентов. Сначала со Стэнфорда, а потом много-много краудсорсинга пошло благодаря тому, что Google подключился. Собрали несколько миллионов изображений. И с 2012 года началась вся крутая движуха в Computer Vision. Только потому, что большую датцию Set собрали. Действительно. Потому что это были миллионы изображений. Поэтому после этого пошли всякие модели, типа AlexNet, VGG и прочего-прочего. То, что там условно запустило arrow deep learning, когда большие начали сетки тренировать. Но большие сетки бы нормально не работали. И мы даже гипотезу не могли проверить, они нормально работают или нет. Если бы у нас не было много данных. Поэтому я говорю Data First – это всегда. Поэтому, когда вы решаете задачу, посмотрите, что у вас по данным сейчас есть. И лучше придумайте на основе данных, что можно с ними сделать. А типа, я хочу вот такую модель сделать, такой сервис. И что мне для этого надо будет. Даже если вы идете по второму пути и таки, хочу вот эту NLP2, не важно, чего бы мне это бы стоило, то как можно это проверить? Можно пойти, поискать в статьях. Много статей есть. Включаем Google Scholar, включаем Paperware as a Code, включаем Archive.org. И начинаем смотреть, условно, кто решал подобную задачу. И на каком количестве данных, с каким качеством этих данных он, условно, получал результаты. И мы там увидим, что, например, модель была натренирована на 4 миллиона примеров, 40 классов. И благодаря этому мы сможем сформировать понимание, сколько нам данных нужно. Есть третий вариант, то, что вы будете изучать в следующие месяцы. Есть такая техника, например, Fun Tuning. В основном, мы берем какую-то готовую модель, которая решала эту же задачу, но на других данных, либо решала подобную задачу. И мы просто пользуемся тем, что она научилась вычислить закономерности, и мы ее добиваем до нашей задачи, если у нас есть маленький набор данных, например. Понятно? Или есть дешевый вопрос? Отсюда вытекает другой вопрос. Вот, к примеру, мы себе инженеры, ну, вообще инженеры по EML, и бизнес какой-нибудь, или предприятие нам дает задание на специфический датасет сделать какую-нибудь ИИшку. Но, к примеру, его нет в интернете. А сам бизнес данных про вот эти вот задания не собирает. То, что делать в этом случае, если его и в интернете, можно сказать, не особо много, и у самого бизнеса их наборов данных нет. Смотрите, вот это вот классический пример проблемы бизнеса. То есть бизнес не понимает, как это работает. Бизнес поэтому не умеет формулировать задачу, хотя бы даже в верхнем уровне. Задача в примере нам нужна какая-то Иишка. Это есть жесточайший Red Flag, который бы я бы вам посоветовал, если слышите, то убегать с этой компании и искать компанию, где уже более-менее настроенные EML процессы, по крайней мере, пока вы не доросли до какого-то синьорного уровня. Когда вы дорастете до синьорного уровня, когда у вас будет какой-то пул там, условно, из десяти успешных проектов, когда вы поработали в несколько доменах в идеале, тогда вам будет намного легче понять, какой список вопросов и требований надо составить к этому бизнесу, для того, чтобы потом сесть на листочку бумаги, почитать, сколько этапов нужно будет сделать, сколько они будут стоить, сколько времени займут, и бизнес такой поймет, а нафиг нам это не надо. Обычно проекты, которые сейчас предложил Соломжан, в 99% случаев они заканчивают ничем. В том, что просто, условно, себе инженер приобрел какой-то маленький опыт. Бизнес просто потратил деньги, ничего не получилось с этого. В частном случае, может повести в том, что есть готовая, открытая, уже натренированная модель, которую можно в сервис предпринять и там вручную подсобрать данные, и все станет получше. В противоположном случае все значит компании плохо. А можно повторить проблему Соломжан, ты вот говорил, я не расслышала. При каких случаях? Вопрос был такой, к примеру, мы с север-инженером работаем сейчас с север-инженером и пришел заказывать какое-нибудь крупное предприятие или бизнеса. У нас он нам дал задание сделать какой-нибудь CV проект, техническое зрение, где они дали какой-то специфический CV проект, где датасета в интернете мало. И о самих данных, чтобы этот датасет собрать или на основе этого датасета обучить, сам бизнес или предприятие не собирает. Это часто кейс очень плохого менеджмента в компании, который глобально выглядит в том, что компания просто не понимает как делать email, не понимает как делать DS. Здесь что можно делать? Можно на два года пойти в эту компанию качаться и через два-три года научиться ретранслировать и общаться с этими менеджерами, либо фондами этой компании. Но в большинстве случаев проблема мидл менеджмента в том, что он должен был ретранслировать запрос бизнес-заказчика в задачи для email-коменды. У тебя, скорее всего, кейс, когда ты один или вас парочку. Это довольно-таки часто, если честно, проблема бизнеса и инженеров. Это проблема. Я несколько раз уже с таким сталкивался. В софтвере, как минимум у нас в Казахстане в разработке ПО не такая проблема как с email. С email на пальцах одной руки можно пересчитать места, где более-менее научились решать email-ные проблемы. На пальцах одной руки. И не все пять пальцев будут закрыты. Поэтому просто советую искать получше компанию, где уже там и там научились, чтобы можно было качаться как инженер. Иначе придется решать email-ку. Я имею в виду, это заказ от другой компании. То есть наша компания принимает заказ от другой компании. Все равно это проблема email-менеджмента, потому что он не перекликает. Обычно общается заказчик, кроме кто? Обычно общается директор направления в компании, либо это какой-то продукт, продукт-менеджер. Если не продукт-менеджер, хотя бы project-менеджер. Если не project-менеджер, хотя бы team lead. Если не team lead, тогда уже все точно очень плохо. С каждым понижением. И соответственно вон он сверху. Должны задачи поступать. И дальше они до рядового сотрудника должны уже в готовой скрам-доске быть. Прописаны тикеты. И потом по тикетам просто бегать. На крайней мере так построено в нормальных компаниях. Поэтому должна стремиться любая компания, которая хочет делать email. Email делать супер дорого. Если в заднем деньгах, потому что надо тратить очень много на железо. Либо самим покупать, либо воплочной арендовать. Нормальный email специалист не стоит дорого. Тем более опытные, которые умеют решать задачи. Соответственно это уже огромный пост. Дороже, чем разработать в среднем по его. Плюс мы понимаем, что там большой риск прийти неудачу. Почти 90% всех проектов email они по ряду причин не докатываются даже до production. Даже до состояния MVP не докатывается, чтоб дипло что-то крутится. Поэтому это дорого. Это дорогая проверка экспериментов. И соответственно раз это дорого, то это должно быть качественно. А качественно это мало где научились решать. В Казахстане, в которой компании работают, есть ряд компаний, которые умеют делать email на проект. Поэтому лучше туда идти, чтобы набираться опыта. Как это все работает. Плюс много компаний из-за рубежа это работает. Там это уже еще больше на поток стало. Словно формирование требований и потом дальше. Отлегся. Надеюсь, ответил на вопрос. Дейта ферст. Мы должны понимать, если у нас есть данные, это зашибись. Потому что на данных можно что-то сделать. Яркий пример с иммажинетом, который я вам приверну. Сначала собрали миллион данных размеченных. Только после этого началось взрыв экспоненциальной архитектуры нейронок. Сначала на иммажинете натренировали. На сайте иммажинета натренировали в сетку тоже иммажинет. Которую назвали. У меня точность была 75. Чтобы вы понимали, в течение двух лет добили точность до 95 вроде. Сейчас она почти под 100% решается. А там было просто тысяча классов. На картинке 128-128 пикселей вроде. Что изображено. Жираф, собака, кошка, человек, байкер, машина и прочее. Понимайте, что дейта ферст. Старайтесь искать больше места, там где много данных. И желательно, чтобы они были чистые. Когда мы решили с вами проблему в том, что у нас много данных. Самый простейший случай, если мы решаем задачу классификации, как минимум для проверки первых гипотез, можно использовать по 40 примеров. Это значение найденной статистики. Много где, например, когда мы считаем дарительный интервал, мы от 30 как минимум считаем. Если до 30, то у нас другие методы подсчета. Тоже проверки статистических тестов. 40 это to be precise. В том, чтобы мы могли с вами еще на тест поделить 10 примеров. 30 условно для моделирования хотя бы статистических первых моделей. А второе, чтобы мы могли провалидировать и протестировать результаты. 40 примеров как минимум надо начинать. В реале сильно много должно быть. Но какой-то зависимость и такие-то точности, а такой-то модель можно получить при таких-то данных. Такого, конечно же, нет. Потому что мы как раз и мали дело, потому что у нас непредсказуемо. А потом мы должны решить проблемы обязательно хранения. Такие слова как реплицирование данных, шампирование. Разные сервера для вас. Или понимание штуков rate 10, rate 0 данных серверов. Если у вас эта проблема не решена и вы не знаете, что это такое, скорее всего, рано или поздно вы столкнетесь с проблем, с неприятным сюрпризом, что у вас потерялись данные. Словно сервер сгорел, словно данные нормально не сохранились, или там копии только сохранились, или они там в неправильном формате сохранились. Короче, хранение должно быть решено. Обычно, как вы понимаете, хранением занимается дата-инженер, либо датабейс-администраторы. Поэтому это их главная боль. Если у вас это не решено, то вы как минимум должны про это иметь в виду, как минимум, иметь копию данных в каком-то другом месте. В лучшем случае на флешке, в лучшем случае на другом сервере. Следующий момент. После того, как мы решили первую и вторую проблему, он называется очистка данных. То, что у вас сохраняется, желательно должно сохраняться в хорошем ценовом формате. Для начала лучше сохранять все. Потом, условно, после анализа данных мы поймем, что ценно, что не ценно, и можно будет уже выбирать, что не сохранять, но лучше все равно сохранять, потому что какой-то придумается новый метод, или какая-то новая идея будет, чтобы мы могли проверять гипотезы, и лучше сохранять как можно больше. Короче, данные – это ваше золото, и вы должны его хранить, ни в коем случае не удалять. Поэтому условно хотя бы на HDD это все сохранять, и если решаете задачу, всегда в голове имейте в виду в том, что мне надо будет столько-вот данных в хранении иметь. Этап с очисткой данных. Мы очищаем данные. После очитки данных мы уже можем строить с вами простое машинообучение. После простого машинного обучения мы можем уже посмотреть в сторону дипернинга, искусственного интеллекта и тому прочего, и так далее. Если у вас плохой фундамент, у вас никогда не получится высокого здания построить. Фундамент в DAC, в EMAIL, в AI – он выглядит вот так. То есть данные – это наш фундамент. Если какие-нибудь вопросы есть, или кто-то прокомментировать хочет. Хорошо. Готов? Нет? Ну окей. В самом простейшем случае я бы предложил вам пользоваться следующим пайплайном для решения задач. Помимо этапов, какой пайплайм вы строите? У вас есть… Ключи указка. У вас есть… Так, указка не видна. Смотрите, в усовершенненном кейсе у нас выглядит любой EMAIL, EDA, AI проект. У нас есть какие-то оффлайновые данные. Мы эти с вами данные чистим. Мы делаем с вами их EDA. После EDA мы делаем Model Design. Потом мы тренируемся и валидируем наши результаты. После этого мы начинаем строить и ваше решение. И вот это все. Мы тренируем наши результаты. После этого мы начинаем строить процесс. То есть у нас есть процесс тренировки данных. У нас есть пример, когда у нас в реал тайме данные обрабатываются, в том, что они валидируются. И потом мы с вами предсказываем и отдаем результат. Что здесь важно понимать? В решении задач не всегда нам нужно написать модель с нуля. Вот. Нам надо сделать довольно неплохое исследование. Если сейчас готовы какие-то решения, готовые модели, которые мы можем просто переиспользовать. Например, просто скачать и себе засунуть. Например, если вы решаете задачу распознавания автомобильных номеров. Есть готовые модели, их легче забрать и для MVP использовать. Для того, чтобы построить первый сервис. Если вы решаете задачу по детекцию лиц. Не по детекцию лица, а по идентификации людей по лицу. Тоже есть уже готовые сервисы, есть готовые дизайны решений. Легче взять просто их себе задеплоить. Вот. Потратить время несколько недель или несколько дней на том, чтобы условно вот это решение у вас заработало, а потом его идти и улучшать дальше. Либо, если это все не так, если нет готовых решений, надо посмотреть, а можем ли мы какое-то решение сейчас изменить. То есть, условными методами ретрейнинга, методами фантюнинга, методами федерейтет лейонинга, разными методами можем ли мы их адаптировать под себя. Если это тоже нет, тогда идем и с нуля пилим сервис, но мы должны будем понимать, что это будет не бесплатно и не быстро. Теперь, какие есть обычно проблемы, с которыми... Короче, почему провалится обычно проект? Здесь я условно выделил по процентам, по своему опыту, по опыту рассказов, которые я знаю, которые я читал от друзей, от знакомых. Первая причина, которая в большинстве случаев у нас провалится, мы проехали проект, это в том, что у нас нет нормальных данных. Вот это условно первый вопрос, когда к вам приходят новые идеи или вы придумываете какое-то новое решение. Первое, что вы должны задаться вопросом, есть ли у нас возможность получить довольно легко достаточно объем данных и достаточно качественных данных. Это могут быть уже какие-то готовые наши данные, поэтому я всегда говорю Datas First. Либо это можно где-то купить, где-то можно обменяться, где-то можно попробовать попарсить. То есть если мы понимаем, что данные получить несложно, и мы можем им это сделать, то можно начинать проект. Следующий момент, почему покапится DSML проектом. Это не соблюдение этапов, плохое планирование и отсутствие трейкинга задач. Если вы решаете в соло это, то вы должны просто понимать для себя, условно, сначала я вот этим займусь, от этого этапа у меня должны быть предсказуемые результаты. Я не должен буду в это время бегать, условно, искать какую-то другую логику или пытаться заниматься какой-то фигней, у меня должны быть предсказуемые результаты от этапа. Если вы решаете это в команде, то словно ваш менеджер, для вашим лид, вы должны ему сказать, давай нормально запланируем на неделю, условно мы делаем вот это, вот это, вот это, и по спринту будем двигаться, условно. Мы поймем, что за неделю мы хотим прийти к этому, и максимально будем думать, что нам нужно для того, чтобы к этому прийти сделать. И потом начинаем нормально трейчить задачи, начинаем с этим работать, и все с этим будет становиться. То есть второй кейс – это часто из-за менеджерских проблем или из-за того, что решают проблемы необычные люди, то часто начинается пустая трата времени, то есть делается те вещи, которые не являются важным для итогового решения. Третья проблема, почему управляли в смэльные проекты, в том, что решает, в том, что ряд проблем обычно должны решаться в определенном уровне для людей. Если вы вкатились только в мэль, с высокой вероятностью первые проекты будут не сильно хорошего качества. Со временем они будут улучшаться, улучшаться, улучшаться, и там условно через лет пять вы будете просто гуру щелкать мэльные проекты как орешки. Но некомпетентные работники условно – это одна из тоже главных причин, почему покапится проект. Либо, например, в том, что какой-то стартап смотрится только с точки зрения в том, что здесь нужно только эмельщик, при этом мы забываем в том, что здесь нужно еще и самопрограммное обеспечение разработать. А там эмельщик, он нормальный сервис скорее всего не напишет, или это будет неэффективное использование его времени и его навыков. Поэтому, к сожалению, нужен инфраграммист еще тогда. Четвертый кейс, почему покапится проект, это потому, что становится очень дорого это все делать. Опять же, дорогие сотрудники, особенно хорошие, дорого держать клаун, дорого арендовать сервера, а сейчас стало это еще дороже, потому что всем нужно, все захотели создать эмельные команды, и все начали вбухивать туда огромное количество денег. Поэтому сейчас аренда серверов, она есть, она стала супер дорогая, но еще и по этой супер дорого цене, она стала, как сказать, ее невозможно купить, потому что она уже вся в очереди на использование стоит. Поэтому надо там типа платить он демант, а он демант на ВС еще дороже стоит. Пятая причина – это отсутствие инфраструктуры и ресурсов для разработки, но это больше касается стартапов. И последнее кейс – это когда люди выиграют, или находят новую работу, или просто плюют и уходят, потому что интересные задачи. И вот это все надо иметь в голове, почему проект может в итоге ничем хорошим не закончиться. Есть ли какие-нибудь вопросы по этим пунктам? Или откликается у кого-нибудь эти проблемы, если кто-то уже пытался решать какие-то эмельные задачи проекта? Или кто-то, например, не согласен с этим? Это будет еще лучше. Так, может, чей-то тоже ничего нет. Ну ладно, окей. Следующий кейс – то, что бы я начал советовать, условно, если вы решаете в соло или если вы решаете в команде, какие методы, на что нужно точно обратить внимание, чтобы сделать лучше. Первое, чего всегда стоит начать – это понять конечную бизнес-метрику, которая оптимизируется этим сервисом. Попробовать ее замерить. Потом привязать ее к ДС-метрике. То есть у вас бизнес-метрика – это функция от ДС-метрики, ДС-метрика – функция от параметра модели, которого вы используете, данных, которые вы имеете и т.п. Я бы максимально сильно сделал эту связку. И у всех проектов, которые у нас начинаются, эта связка супер важная, на чем я трачу всегда огромное количество времени. Потому что если этого не сделать, то потом окажется, что через несколько месяцев работы у вас просто… Вы получили каст-модель, которая точно работает, но она слишком тяжелая. Она оказывается вообще не улучшает бизнес-метрику, и шаги были проделаны зря. Поэтому всегда попытайтесь разобраться, какую бизнес-проблему решается и как ее в идеале можно замерить. Прикольное использование Data Science as Service. То есть существуют уже какие-то outsourced DS-команды, которые могут решать задачи. И внутри компании лучше строить DS как отдельный департамент, который каким-то очень опытным чуваком во главе либо в штате будет решать задачи. Использование быстрых промежуточных датаметод Hypothes. То есть делать одну итерацию, замерять… Ну, условно не так, чтобы вы начинаете решать задачу, и вы хотите сразу миллиард моделей потренировать. Условно, получили первое что-то осязаемое с точностью хотя бы 70%. Попробуйте запихнуть это в сервис и как сервис оценить, бизнес-метрика улучшается или не улучшается. Если улучшается, дальше ее улучшать, улучшать, улучшать. Там дальше придумывать модификации трансформеров, куда можно воткнуть, играться с лос-функциями, использовать что-нибудь интересное. Это все уже как улучшение, но вам нужно построить baseline. А для этого вам нужно хотя бы первый маленький сервис собрать даже на самых плохих данных, на самых плохих моделях, на неоптимальной тренировке, без GPU, хотя бы так. Потом вам будет намного легче этот baseline всегда бить, и вы сможете понять в каких вещах вам надо в первую очередь побить baseline. Стандартизация методов разработки на всех этапах. То есть нужно вести документацию, нужно сохранять результаты гипотез, для этого можно email flow использовать, вести доски задачами в том, чтобы было легче. Это все менеджер Agile работает, когда мы интеративно это все работаем по поспринтам, если что, быстро меняем направление и там условно отдаем baseline CD. И не идем в email и так дальше. И самое главное, сохранить как можно больше данных, потому что если сейчас кажется очевидно, как их можно использовать, то потом станет очевидно, что не зря мы их сохраняли, и можно из них вот это сделать. Плюс, например, очень часто есть, когда вы начинаете решать одну email, один email проект, но данные, которые вы на них собрали, потом можно переиспользовать для другой задачи. Например, первоначальный email проект оказывается таким сильно интересным и прибыльным, что может повернуться на основе данных, которые вы собираетесь почистить и увидеть. Еще интересный случай, какие вопросы стоит задавать, где нужен email, e.i. или использование chat.gpt или начинание дресс-проекта. Можно смотреть на следующие моменты. Первое, где есть какой-то человеческий фактор принятия решений. Его можно заменить. Помните, например, и call center, который я вам рассказывал позавчера. Мы с вами смотрели в том, как мы можем расти от этапа автоматизации. Условно сначала люди решают, потом мы начали разрабатывать первое ПО, потом мы начали его анализировать, улучшать, потом мы начали email прикручивать, потом мы начали чат.gpt прикручивать или серьезные глубокие сетки. Везде, где есть человеческий фактор принятия решения, мы можем подумать, а если мы здесь вот так оптимизируем, станет ли лучше или нет, и опять же оценивать бизнес-метрики. Следующий кейс. Если мы видим, что в новостях кто-то начал использовать, например, dsml, где-то прикрутил и стало действительно лучше. Мы идем проверяем, действительно лучше это работает или нет, и мы можем как минимум это повторить. Или как минимум мы повторить можем это в нашей компании, чтобы мы условно уравновесили качество продуктов, которые мы делаем, чтобы они как минимум были такие же, как в комплиентах. Следующий кейс. Там, где нам нужно улучшить производительность, например, на заводах, чтобы машины стали выпускать больше стали, подключаем анализ данных, понимаем, где-то может помочь вероятностные модели, потому что любая ML и AI проект это все-таки вероятностные модели. Если они где-то помогают, то супер, мы их тогда можем прикручивать и улучшать производительность. Там, где нужна повышение какой-то точности, там точности, например, для совершения торговых сделок, либо точности, например, в определении структуры белка или там и прочего, там, где слово используется за точность, вон идеальный кейс, там, где нужен data science или ML, или DL или AI и все подобное. Следующий, где нужно улучшить предсосы принятия решений. Классный кейс, когда есть какой-то домен, где дофига данных, но при этом нет ML. Вот это прям самое вкусное, если честно. Куда можно идти и на перспективе 3-5 лет можно делать просто гигантские классные решения, которые будут менять целую индустрию. И потом так, чтобы ваше имя ассоциировалось, например, как с Ильей Сацкевером или Сэмм Уальцманом или прочего, так чтобы стать, словно, таким важным человеком, который сделает действительно важный импакт. И особенно какие-то консервативные вещи, типа медицины, телекома, государство. Там, где есть данные, там, где данных много, но там нет ML. Агрокультура, типа сельхозка, тоже как и с кейсов, которые консервативные. Нет пока еще много ML. Военная часть тоже пока там нет еще много ML. Вот это все такие отрасли, куда можно, в принципе, идти. И на перспективе какого-то времени действительно много чего интересного сделать. Потому что там есть данные, мы понимаем, что date is first. Вот. Где есть сложная и непривиальная задача. Это все, что касается ресерча, все, что касается науки. И где нужна какая-то персонализация. Соответственно, это любой сервис, сервис-услуг. Магазины, бьюти-салоны, рынок, все, что прочее. Там, где персонализация решает, там ML тоже классно встраивается. Есть ли комментарии, вопросы по этому слайду? Или кто не согласен? Или кто, чтобы хотел бы что-то добавить? Или у кого-то может в голове какой-то есть стартап, и хочет обсудить с того, что было сказано ранее? Нет? Окей. Нет, у меня есть вопрос. Да, Диана, как раз. Я хотела спросить, а есть ли, наоборот, случаи, где data science и ML лучше не нужно использовать? Что, еще раз? Есть ли случаи, например, кейсы какие-то, в которых, наоборот, как раз-таки лучше не использовать DSML? О, на это и складно в случае. Так, ну так она была не от этого. Блин. Я, может, найду тогда и чат скину. Там просто я вряд ли смогу лучше это рассказать, чем было написано. Я буквально недели три читал нашу статью, поэтому закину. Почитали. Да, Слимжан? Можете, пожалуйста, привести примеру военной структуре. Где данные только, ну есть data только в текстовом варианте. Где она может использоваться? Как она может использоваться и проявлять себя? Опять же, слишком общего вопроса. Давай пример какой-нибудь, чтобы можно было построить. Сейчас я придумаю тогда один. Ну да. Ну давай, окей. Что? Вот смотри, вот есть кейс. Например, радиопередача данных. Например, по рации они обмениваются приказами. Можно это автоматизировать протоколирование. Например, какие приказы к кому были отданы. Так чтобы мы накинули туда просто модели распознавания речи. Я считаю, есть готовый пайплан, куда это все протоколируется и доводится до главнокомандующего. Ну, да, это очень интересно. Когда это все протоколируется и доводится до главнокомандующего. Тогда так, что он знает прямо с места, условно с переговора. Между двумя солдатами. Проходит по всей лестнице и наверх. Мы туда можем уже накидывать тематический поиск. В том, чтобы искать не просто ключевым словам, но уже по какой-то символике. Например, найди мне приказы, которые отдавались. Например, не, найди мне, где было плохо. Например, участки фронта, где было плохо. Классный есть видос, я что вам покажу. Есть такая компания, называется «Полонтир». Если кто-то знает, то поймет. Она не классная презентация была. Так они свои учеботы сделали. Заведение этих боевых действий. Там, где можно было исправить учеботу, исправлять дроны. О, экрана не видно. Да, я сейчас найду этот видос и вам покажу. О, вон он. Они короче делали учеботу, а это действительно короче продается этот продукт. Я знаю семь стран, куда это продалось. Но пока нормально не работает. Короче, условно это чат GPT для введения боевых действий. То есть все как в компьютерной игре. Он пишет, скажи мне в этом районе, какие дроны есть. Условно запусти его первый, третий туда. Читайся статусе, как это произошло. Вот яркий пример, как это может работать. И здесь есть полная интеграция всех систем. Видос дронов. Сейчас он говорит, что сгенерируем мне три курса действия для таргета этого вражеского обмундирования. Ну, можно графикой видеть, как это все происходит. Вот короче, кейс как этого армия может использоваться. И это все будет больше и больше. То есть я знаю точно сейчас, как минимум. У меня есть ряд ребят, которые остались сейчас в России работать. И у них есть военные заказы. И то, что они рассказывали, немножко сюрреалистично звучит. Где полагается использовать нейронки для автономного убийства людей. Немножко даже страшно. Но это возможно, короче, как вы понимаете. И там, где более консервативный рынок, тем более можно большего крутого всего сделать. Надеюсь, нормально, например, сам джан? Да-да-да. Очень классный пример. Ну, она такая, как компьютерная игра. Конечно, это все не так работает. Но как минимум в эту сторону можно пойти. Извините, а можно попробовать применить предыдущий вопрос? Вопрос был, типа, где лучше не использовать DS? По своему опыту хочу поделиться. Если, например, у вас бизнес-решение можно решить одним человеком, условно, просто каким-то одним сотрудником, лучше туда не внедрять это. Можно просто использовать вместо персонализации какую-то статистику. Топ, продавай мы товары, вместо Lexisa. А если детекция какого-то объекта, если у вас бизнес не масштабированный, в размере страны, например, то лучше заменить это одним сотрудником, чтобы он просто мониторил и писал отчеты, писал доклад. По-моему, так будет лучше. Ладно, тогда расскажу вам в чем суть, когда обычно DS не используют. Это считать надо. Словно если в деньгах не кажется, что это как минимум несколько раз эффективнее, становится прибыльнее, там DS лучше не начинает применять, потому что DS-проект это дорискованная штука. Там нет предсказуемых 100% результатов в том, что на уме 4 людей мы проделаем вот это, и у нас будет полностью готовый сервис. Обычно так никогда не работает. Поэтому там есть большой риск овертаймов, большой риск оверстимейта, большой риск в том, что у нас точно может быть не такая хорошая, качество тоже не такое хорошее. И когда ты смотришь с верхнего уровня, условно, если вы занимаетесь финансовым анализом, либо если вы поделили проектами, то примерно понимаете, про что я сейчас сказал. Если вы просчитываете в том, чтобы стоимость разработки, условно, стоит дороже и не приносит столько потенциальной прибыли в каком-то уже текущем процессе. Да, вот как сказал Сымбат, легче поставить на человеческом уровне это все. Но есть другое. Еще можно рискнуть. Бизнес это все тоже... Незря появляются стартапы, потому что это какой-то риск, что-то условно изменить и на основе этого что-то сделать. Я знаю классный пример. Недавно у Осетинского уже вышло большое интервью с одним стартапом из США, с корнями из Армении и Москвы. И они решают проблему колл-центра, когда они полностью оптимизируют колл-центр. Это странный кейс выглядит, потому что когда мы обычно звоним на какую-то службу поддержки, то в третье случае, поскольку я занимался самой разработкой колл-центров и до сих пор порядка 50 колл-центров пользуются нашим сервисом для создания, то в третье случае у вас можно скорее пытаться переключить на менеджера, в том, чтобы условно не общаться с роботом. Но они рискуют, они видят в том, что например, то, что сейчас GPT сделала, возможно это будет как раз тем переломным моментом, когда люди не захотят так сильно переключаться на человека и общаются с ним. То есть нужно еще понимать, этот риск стоит за него взяться или нет. Это все больше стартаповая история, редко когда в компаниях новаторство придумывается. И помним как бы этапы автоматизации, мы с вами обсудили, если там решает человек и если там условно и окей, что решает человек, тогда не надо на следующий этап переходить. Если да, то переходим уже на создание ПО, на оптимизацию. Потом мы подключаем анализ, оптимизируем это все, потом начинаем с легкого email, там условно просто нейронок, и в идеале приходим к человеку GPT или к каким-то другим серьезным большим мультимодальным моделям. Но есть еще такой кейс, как случайное блуждание. Кто знает, что такое метод Монте-Карло, понимает, что можно методом тыка тоже дойти до чего-то с какой-то не очень высокой вероятностью, но можно. Соответственно, из тысяч зараненных стартапов, в начале стартап редко, когда хорошо продумывается, особенно если там человек не с большим опытом за плечами. Но все равно есть ошибка выжившего, что там может какой-то крутая история получиться. У меня было 4 раза, когда я основал свою компанию, делал стартап. В двух случаях более-менее завершилось, в двух просто какой-то кост поднес. И 90% всех стартапов заканчивали обычно в худшем моменте. А в чем идея была вашего стартапа, если можно поделиться? Там много было. Как минимум не одна. Последний, который я закрыл в 2022 году. Мы даже смогли собрать команду из 5 человек работающих. Мы делали видеоаналитику для автодилерских центров, для сети супермаркета, чтобы они могли считать почет клиентов, делать свою CRM, связывать в лицо человека с номером телефона, чтобы можно было потом пересаменизировать это все. И вот так условно на трех сетях автодилерских центров, которые, к сожалению, ушли в 2022-23 году, типа Renault и прочего. Это была не прям супервал, а полубизнес, полустартап, потому что в Казахстане такого не было. И в ближайших стран такого мало было. Поэтому этим занимались, и там была какая-то прибыль. Довольно ощутимая, чтобы этим стоило продолжать заниматься какое-то время. Были более экстралогантные стартапы. У меня был стартап, например, с трейдингом связанный. Это то, что купил один из самых больших HRT-фондов в мире, Quantum Rains. Они у нас выкупили стартап, потом полгода мы командой у них дорабатывали. Я полгода дорабатывал, двое человек, которые до сих пор там работают. Это кейс был, когда наш режим купили. Был кейс, типа дел юридического ассистента, но тогда еще не было. У меня был QGPT и QNANCE. Модели довольно были нехорошие. Это был кейс, когда у меня было не Data First, а Models First. И это погорело все из-за этого. Потому что так не было миллиардов примеров, чтобы это все хорошо обучилось. Что еще? Еще какой-то стартап был. Сейчас не смогу. Не могу точно вспомнить. Это еще дело. Два из них у меня были успешные кейсы. Когда мы какую-то часть времени зарабатывали, потом подали решение. И вторая кейс, когда мы зарабатывали. И когда компании ушли, я решил, что у меня пришло повышение по работе. И был выбор еще пойти и устроиться в другую компанию, от которой у меня был офер. И я решил пока прикрыть компанию, потому что захотел дойти еще до более... Стать еще более экспертом, чтобы следующую интерацию с большей вероятностью довести стартап на историю до кого-то. Что вы должны понимать? Есть крутой пост, имен. У меня... Блин, ну я его сейчас долго буду искать. Объясню суть. Это была реакция одного уважаемого человека в ДС комьюнити. Нового из статутного венчерного фонда. Венчерный фонд сделал пресс-релиз с фотографией участников следующего бранча. И там было прикольно. В абстракте написано, вот условно 20 юных молодых с горящими глазами фаундеров. Присоединяется к нашему новому потоку в сторону акселератора. И там фотографии все люди старше 40-50-60 лет. Ни одного молодого человека там не было. Это реальная картина, кто делает стартапы. Успешный стартап делает люди, у которых есть опыт за плечами. Есть ошибки выжившие. Но до успешности обычно доходят такие керс. Вот, например, СМАльтмадай. Это четвертый его стартап был. При этом он еще в IC поработал в какое-то количество 5 или больше лет. Еще пару моментов, которые я бы хотел отсоединять, обсудить. Еще несколько вопросов, которые бы я бы советовал вам задавать, когда вы занимаетесь маленьким проектом. И какие этапы еще особенно хотелось бы подшутокнуть, что важно. Первое, всегда на первом этапе задавайтесь вопросом, какая ценность ML-проекта или ML-решения, неважно адаптации, может, чат GPT, Prompto. Всегда выясните конечную проблему, которую вы решаете. Потому что выяснив конечную проблему, проверьте, разработка вообще по-другому будет идти, чем если вы это не выясните, вам скажут, создай мне И, вы там что-то начнете делать. И увидите немножко каменное лицо на презентации. Следующий кейс, всегда обязательно собирайте требования. Поймите, сколько данных у вас есть, поймите нужная разметка их или нет, достаточно ли там автоматических каких-то кейсов. Поймите, какая задержка модели в проде у вас будет, допустимая эта задержка или нет, как можно ее решить, сколько это будет стоить условно в серверах и в сервисах, в написаниях сервисов. Всегда понимайте, как будет происходить ваш дипломат. Это будет на каком-то железе, это будет на клауде, это будет условно с какой-то спецификацией, там будут какие-то ограничения, может закрытая сеть будет какая-то, нельзя будет накатывать обновления. Подумайте про эти все требования. Если какие-то требования конфиденциальности данных, может условно это персонально-чувствительные данные, на которых есть уже определенные законы даже в нашей стране, которые как-то ограничивают обработку их. Следующий момент. Изучите обязательно, серьезно, это может вам очень сильно спасти время. Как бы вам не хотелось запилить что-то свое с нуля, посмотрите, есть ли уже что-то готовое. Посмотрите, есть ли Open Source. Open Source очень много. Посмотрите по первому языку, посмотрите Hagenface, посмотрите GitHub, погуглите, может на Medium найдется что-то, прочитайте, изучите как минимум эти три-четыре источника по семантическому ядру задач, которые вы решаете и приблизительно похожих задач, есть ли что-то такое, что можно было пересподать. Переспользовать в e-mail можно всячески. Можно тупо скачать и запустить Inference, если у вас на вход тоже самое принимается. Можно это адаптировать, можно это фонтюнуть, можно это zero-shot, few-shot, many-shot, можно поправить, можно ретренировать и другие способы, которые мы сейчас с вами будем дальше изучать. Следующий кейс. Всегда начинайте с малого и быстро отказывайтесь от плохих идей. Пытайтесь принять это как не разработку, а как скорее всего кейс быстрой итеративной проверки гипотез. Соответственно, я POC сделал после POC MVP, после MVP, если надо, первую версию, вторую, третью, четвертую, пятую и так далее. Но никогда не идите в пятую версию, пока у вас нет POC и всего сервиса рабочего. Следующий кейс. Подготовьте, если вы выполнили предыдущие четыре шага, если вы понимаете, что проект будет развиваться дальше, подготовьте проектную документацию. Обязательно, потому что потом это может очень сильно дорого стрелять в спину, когда будут какие-то ребята приходить, уходить, или когда будет такой кейс, что новенький будет в команде появляться и не надо будет вам по 10 часов всем объяснять, почему здесь вот так было, почему здесь так. Сохраняйте обязательно историю проверки гипотез. Для этого есть отличные сервисы, типа MLflow собирает, сохраняйте версии данных. Для этого есть отличные сервисы, типа DvC, типа Mio, где это все можно автоматизировать, уже есть готовые планирования решения. Супер. Давайте, если есть предварительные вопросы, я отвечу на них и покажу последние 15 минут, как работать с автоэмэлем. Так, какой у вас был Trading Solutions? Это долго сейчас, на полчаса общаться. Если в праце, то мы использовали логику Гиберхауса, если изучали временные ряды. То есть такое понятие, как Гиберхаус, у нас экспоненциально растет ошибка предсказания с каждой новой итерацией. Но есть классный кейс, потому что она у нас на первом промежутке времени, она у нас растет линейно. Есть такие алгоритмы под названием как... Ух ты, быстро хотел сказать. Как арбитраж, условно, в одном месте дешевле стоит, в другом месте дороже. Купаем дешевле, продаем дороже. На время совершения перевода средств с одного места на другое место классно подходит ML, то есть мы на коротком сроке очень классно ML предсказываем. За это время мы совершаем в какой-то доле предсказуемой доли мы теряем деньги, в большинстве деньги приобретаем, предсказываем на будущее, как второй этап, условно изменение цены на первом бирже, на второй бирже, перекатывается деньги. Мы, короче, подсмотрели это решение, как на публичном фонде устроено. Это был далекий 17-й год. Тогда в скрипте еще много чего можно было делать, что сейчас точно не сделать. И условно мы словили идеальный момент. Он длился полтора года, когда это можно было решать. И на это можно было зарабатывать. На эти полтора года мы и позарабатывали на маленьком стаканном окне. Так, если еще купить это вопроса. И покупали скорее не наше решение, а скорее просто людей, которые могут выдвигать нетривиальные гипотезы и проверять нас, скорее как, команду купить, чем как наш продукт. Потому что когда наш продукт начали бактестить на нормальном объеме денег, там все стало понятно в том, что на тысячи баксов он пасно работает, а на миллионе баксов он вообще не работает. Сначала вопросы. Нет, супер! Вы авто-эмиле не проходили, да? Не знаете, что КПК орests�ec watercolor или Simr talked about? Cuban publisher et Allo mankind and their blog.öglich lambda.cor.uk Вы автоэмейл не проходили, да? Не знаете, что такое PyCaret? Не знаете, что такое CSRAC? Ну, нет, CSRAC как его назывался. Блин, многие вещи забываем. Не проходили. Короче, вообще, на самом деле, когда мы ждали эмейлный блок, там было очень много про автоэмейл. Жаль, что вы до него не дошли. Так. Так, на время отключу, войду и продолжу. Так, для этого я вам покажу самым знаменитым датасейте Titanic'а. Для этого нам, короче, понадобится Dataset Titanic. Кто не знает, Dataset Titanic такое является самой заезженной темой в эмейле. И это, конечно, очень многое. Но, конечно, это не все. И это, конечно, очень многое. Для этого нам, короче, понадобится Dataset Titanic. Кто не знает, Dataset Titanic такое является самой заезженной темой в эмейле. Что здесь есть? Все знают кейс с Titanic'ом. В 1911 году или в 1900-х, в начальных годах был большой такой лайнер Titanic, который сделал shipwreck. Затопился, когда встретился с айсбергом. Самое прикольное, короче, есть полная статистика по пассажирам и их атрибутам. Позависи, как я включу. Короче, есть идентификатор пассажира. Есть даже его имя. Например, мистер Томас Фрэнтис, мисс Идда Ливия и прочее. Есть известный их пол. 64% были мужчины, 36% были женщины. Известен P-класс. P-класс – это класс каюты, который у них был на билете. Один – первый класс, самый крутой. Второй – второй класс, не самый крутой. Третий – это обычно для рабочих было или тех, кто лишь бы было сбежать с Ирландии и Великобритании в новое светлое будущее в Америке. Вот. Кто смотрел фильм с Ди Каприо молодым, 97-го года вроде фильм, тот, в принципе, знает, что в Titanic'е было. Для остальных сполируйте, не буду, посмотрите фильм, классный фильм. И вот есть, короче, для каждого человека типа статус, он умер в итоге или нет. Также есть параметры возраст, сколько было человеку лет, сколько у него было актив, да, сиблингс, это сколько братьев-сестер. Сиблингс – братья-сестри. Потом вот эта вот колонка Парч, я не помню, что значит Парч. Если кто знает, напишите в чат или поднимите руку, скажите. Вот. Есть тикет, да, номер тикета. Есть QR, сколько он стоил. Есть кабин, наверное, скорее всего, Cabin Clue. А нет, это просто кабин, номер кабина. Вот. И есть Embarket, Cherbourg, Queenstown, Southampton. Это, скорее всего, там, где они типа сели. Скорее всего, он несколько точек оставлялся. Вот у нас эти данные. И мы сейчас с вами попробуем, не написав строчки кода по SQL.Learn'у, попробуем посмотреть, как можно с AutoML'ем очень быстро проверять гипотезы в классическом ML'е. Типа не решается, не решается, а уже после этого есть еще и выходить. И оптимизировать сети. Ой, оптимизировать обучение модели. Вот. Для этого придуман AutoML, да, как очень low-code-сервис, который позволяет нам с вами быстро проверить гипотезы по моделированию, проранить много сразу модели машиного обучения, так чтобы они нормально все проранились, и заменить результаты, посмотреть, что из этого лучше справляется, и оценить в целом, как, условно, этот таск решается или не решается с этими данными. Вот. Что мы будем предсказывать? Мы будем предсказывать, человек умер или жив. Так чтобы, когда, например, придумал сервис, так чтобы, когда мы продаем, короче, билеты с вами на какой-то вайнер, и человеку говорим вероятность его погибели, если вдруг что-то произойдет не так с кораблем. Ну вот. Как бы это грустно звучало. Для этого нам понадобится библиотека PyCaret, да, вот есть описание, что делает PyCaret. У нее даже свой сайт есть. Устанавливается через peep-install PyCaret, так же называется. Что она делает? Это, короче, low-code machine learning. Вот. Она делает процессы некоторые идеи, делает некоторые процессы датапроцессинга, сама такие базовые. Делает тренировку данных, делает экспонибилити данных, и можно даже сразу в продакшн попробовать воткнуть. Вот. Условно для такого базового супер-эксперимента, проверки, можно его использовать. Да, Умасбек? А этот он делает? Ну, там, графики, визуализацию, условно? Да. Это дело. Сейчас посмотрим. То есть одной строчкой кода он может все вот это сделать, да? Ну, не одной строчкой кода, а скорее всего вместо тысячи строчек кода за 100 строчек кода можно будет это все сделать. Ага. Понятно. Low-code – это не no-code. Вот если бы no-code было, тогда да. Можно было бы вообще кликом, кликом еще делать. Такие тоже есть, кстати, решения. Есть на Google Cloud, я точно знаю, есть какой-то типа этот no-code ML. Короче, знаете, надо на cloud-сервис посмотреть. Там часть есть у них, no-code ML, где по клику можно все делать. Взрослые же данные, и там сразу депло отправляешь. Ну, как вы понимаете, для проверки гипотеза – классное решение, потому что супер-быстро. Для прода потом с дальнейшими улучшениями такое все решение приписывать надо будет. Вот. На нашей задачи – типа проверки быстрых гипотез. Я включаю данные Titanic'а. Я показал, откуда с этого скагал. Импортирую библиотеки, импортирую Pandas, импортирую PyCaret Classification, потому что мы задачу классификации решаем. Он умрет, не умрет категорически на переменах. С Escaler'a забираем train-test split, для того, чтобы мы могли обучиться на тренере, проверить результаты на тесте, чтобы мы могли действительно нормально оценить, насколько хорошо у нас получилось рассказывать. Забираем classification report из Escaler'a metrics, для того, чтобы вы посмотрели на Confusion metrics, оценили метрики accuracy, precision, recall, f1 score, посмотрели в режиме micro, macro и в этот режим. Вы, наверное, это все проходили, поэтому объяснять не буду. Если не проходили, то дайте на это. Я гружу в Pandas данные Titanic'а. В итоге у меня следующие данные. Так, наверное, лучше сделать вот так, чтобы сделать чуть более красиво. А тут закрыть. Те же данные, что я показал вам на сайте и Kaggle. Теперь, что я делаю? Я дроплю ненужные колонки. Это надо будет сделать вручную. Например, мне нужен messenger ID, мне нужен name, мне не нужен тикет, мне не нужен хеббин. Для секса что я делаю? Секса переведу в категориальную переменную, полчеловека. Embark тоже переведу в категориальную переменную. Потом, для всех надо я заполню его средними значениями h. Для Embark я заполню моды, потому что это категориальная перемена. Это будет весь датапроцессинг, чтобы я не потерял лишние строчки, потому что там всего на Titanic очень мало количества троп. По 400. Так, сколько у нас троп? У нас с вами 400. Теперь я делаю train data split. Не забываем всегда, когда мы что-то с вами рандомное запускаем, никогда не забывайте фиксировать seed. Потому что иначе вы не сможете воспроизвести свои результаты или передать кому-то, а он начинает воспроизвести, и там еще другие будут метрики. Поэтому всегда фиксируйте seed. Я бы так предлагал вам обязательно первую строчку кода в любом вашей тетрадке, в любом вашем коде писать. Супер. Мы зафиксировали seed, поэтому если все, что я получил, вы у себя воспроизведете, и у вас будет то же самое. 334 строки у нас полетят на тренинг. 84 строки мы по ним будем с вами проверять провалидируем результат. Что я потом делаю? Вот здесь я из PyCaret запускаю инициализацию. Мне надо будет запустить setup. Сетап у меня находится в PyCaret classification, как метод. Я сейчас буду унициализировать этот метод. Я ему на вход подаю data frame, весь data frame, включая x и включая y, которые могут быть отправлены на тренинг. Поэтому я убирал лишние колонки, поэтому я некоторые менял тип донна. Указываю, что у меня будет таргетом, survive. Ему обязательно надо было указать, что он категориальный, иначе бы он мне его как раз и решал. Session ID – это seed, это random seed, который передается в эту модель, чтобы можно было производить варбос, чтобы поменьше принтов было. С этого момента у меня начинается построение модели, и у меня сейчас будет он принтовать все модели, которые он тренирует. Он начинает процессинг данных, он преобразует его, чтобы все модели могут пойти, и с первой модели он начинает логистическую регрессию считаться. Все по логистической регрессии почиталось, у него фантастические метрики, все по единице. Почитался наивный bias, все по единице, decision tree почитался, сам подобрал оптимальное количество параметров. Ну, не оптимальное, прям оптимально-оптимально, он скорее по этому. Насколько я помню, обычные генетические алгоритмы стоят, он как будто какой-то батч проверяет гиперпараметров, но не прям супероптимальный. Вот. Rich Classifier почитался, почитался Arena Forest Classifier, случайно лез в дереве. Почитался квадратичный дискриминант-анализ, который часто показывает, что такое. AdaBoost как boosting примитивный, Gradient Boosting Classifier как, скорее всего, как XGBoost, Extratreex Classifier, что же аналог это всего. Extreme Gradient Boosting как XGBoost, LightGBM почитался, SVM почитался, LDA почитался. Dummy Classifier это когда мы для всего просто единицу кидаем. Всегда обязательно. Он как baseline здесь используется. Самый частый кейс, самый частый класс просто предсказываем и смотрим, что по метру у него будет. И KeyInn почитался. Вот. В итоге мы с вами получили вот такое количество моделей, которые у нас 100% качество имеют. Да, все по метре, там, единица, даже ни к чему не прикопаться. Что мы делаем с вами дальше? Мы с вами сохраняем лучшую модель. Здесь она по дефолту первой выбирается логистическая регрессия. Если бы она была хуже, выбралась бы следующая по дефолту лучше. Потом, что я делаю? Я получаю предикты на трене, да, мои предикты на трене для того, чтобы ввести дополнительные метрики. И считаю предикты для теста. Вот. Для теста. Вот, за это метрики. И принтую classification record. Тоже по ним. В итоге. Логистическая регрессия. Мы видим для нее какие параметры были. Какие колонки использовались. Pclass, CxH, CIP, CIP, CIP, CIP, CIP, P, Parch, Pair, Embarked, Survived, Prediction, Label и прочее. Вот. Survived у меня соответственно на предсказах. И видим значение результатов на тесте и на трене. Видим, что на тесте у нас тоже все 100%. Сохраняем модель TitanicBest. Он сохраняет ее в Pickle. Поэтому, если что, есть несколько разных версий Pickle. Если вы, короче, с 3.11 Python сидите, то, короче, сохраните Pickle и не думайте. Но если вы на 3.8 сидите, то вам надо посмотреть, какая версия Pickle считается. Потому что старый Pickle может не поддерживаться, и надо будет скачать в библиотеку специфичного Pickle, чтобы этот Pickle открыть. Pickle это, короче, in memory к Python не то что идет. Он ее сохраняет в байд-код, и потом этот байд-код сам Python читает ее опять в переменной и переводит. Удобный класс данных, но его в текстном редакторе соответственно не посмотреть. Только из Python можно будет запускать. Я сохранил название TitanicBestModel. Он будет у меня с расширением .pql. .pql расширение создалось. Потом могу логить модель и отправлять на прод, отдавать условно Mgc, вот этим все круизном лайнером, для того, чтобы они запускали всякий сервис. Так, супер. В принципе, так. Есть, короче, те штуки, которые идеи проводят. Можете погрузить, как в PyCard делать идеи, но я советую идеи делать всегда самим, потому что нет нормального подхода. На каждой тип задач у нас с вами разные идеи нужно делать. И на разном домену специфику помножить на это. Вот столько вариаций идеи получится. А с моделями чуть проще. Модели у нас фиксированы, и модели мы на каждой задачу, если она предлана. Может так удобно посчитать. Я бы мог все это сделать с вами за где-то тысяч две строк кода. Я бы мог все это сделать с вами за где-то тысяч две строк кода. Вот, круто работает. Да, Умазбек, какой вопрос? У меня это, я примерно такое же делал, но через циклы. То есть создавал функцию, выводил там все ML, как его... Ну, практически то же самое, что и здесь, но с помощью функции и выставлял вот эти, как их называют, метрики тоже. Тоже с помощью функции все это. То есть с помощью итерации, с помощью функции и так далее и так подобное. А тут вы прямо подсказали очень классный инструмент, который в принципе все это делает. Единственный бонус, что здесь делается, то, что скорее всего у вас не было сделано, в том, что он здесь, когда считает, он не одну модель считает с дефолтными гиперпараметрами. Он делает еще по генетическому алгоритму выбирать. Ну, короче, он запускает сначала там, словно, 10 разных наборов гиперпараметров. Каждый потренировался, посмотрел, где, условно, первые производные положительные, скрестили эти ветки, потом только по ним пошло обучение. И вот так более-менее какие-то оптимальные гиперпараметры, это как-то способ он поискал. Либо через байсовскую оптимизацию он искал гиперпараметры. Вряд ли что-то третье там будет. То есть там еще чуть-чуть под капотом зашли первые приближения оптимальных гиперпараметров. Их еще можно будет докрутить. Там... Да, плюс там... Извините, что перебиваю. А там, случайно, не вшито Optune? Optune классный вариант. Скорее всего, она там используется. А можно даже проверить. Он под либо качает, сейчас быстро. И гляну. Но Optune я бы не советовал использовать. Почему? Optune он не использует. Там есть сейчас лучший аналог, чем Optune. Optune капец старая. Я еще в 2015 году, насколько помню, юзал. Там сейчас есть получший. То, что на Bias построено. Вот, на Bias. Короче, если у вас был курс по байвской оптимизации, если нет, то возьмите. Классно Ветров читает. Есть открытое на YouTube у него лекции. Вот. Супер полезно будет для того, чтобы с Джона потом в мидл переконвертироваться. Там, короче, легче самому написать, чем Optune пользоваться. Ну, по крайней мере, я в основном сам пишу. Типа греться, да? Нет, нет, греться сам примитивный. Просто по сетке проверяйте все возможные комбинации. Ну, не только комбинации, все возможные пары. Пары 3 грамма, 4 грамма и так далее. Как минимум лучше будет работать точно Random Search. Вот. Еще лучше будет работать Bias Search. И вот там от Bias Search можно уже сильно лучше копнуть сделать. Но Optune вроде она там что-то... А да, я гляну. Давно Optune не пользовался. А вроде она Bias Search по оптимизации имеет. Так. Что-то было написано. Ну, короче, Optune классный кейс. Там условно Cycle по всем параметрам, плюс Optune по всем гиперпараметрам. А с гиперпараметром это уже будет поближе, потому что здесь делалось. Понятно. Спасибо. Вот. Плюс, короче, есть еще другие AutoEmail библиотеки. Там еще можно найти, короче, и маленькие FitForward Net. Чтобы маленькие нейронки он тоже там тренировал и чтобы мы на них тоже понимали, какие точности есть. Потому что нейроны сидят в другой класс. И для какого-то количества данных, скорее всего большего количества данных, нейронки работают довольно хорошо. Там есть на разных текстурах есть нейроны, которые по тесту надо. Я не помню, был ли в каком-то AutoEmail, чтобы он условно мог разный тестить. Видел просто simple FitForward. Обогнал. Короче, если у вас есть датасет, вы сделали его идеей, сделали при процессе данных, чистку данных, то классный кейс, чтобы не сидеть, не париться с моделями. Можно закинуть его в AutoEmail и получить уже готовое что-то. Или, как минимум, проверить возможности, которые дают. И на этом курсе мы будем очень много вам давать всякого такого, то что можно быстро проверить какой-то этап в МЛ-проекте. На это он идёт точно. Есть ли ещё какие-то вопросы по сегодняшней лекции? Почему МЛ-проекты работают, не работают? Какие ещё лайфхаками я могу поделиться? Либо что-то по AutoEmail, то, что мы только что увидели? У меня вот до этого вопрос образовался. Помните, вы говорили по крипте и так далее? То есть где закупать и так далее, в какой момент лучше закупать, в какой момент скидывать и так далее? Есть ли вообще какие-то там решения по нейросетям, которые уже может предиктить именно в плане того, что надо вот в такой-то момент взять такую крипту или в такой-то момент взять такую крипту, или ещё что-то вроде? Это наверняка не только мне приходило в голову создать такое. Да, и советую туда даже не лезть. Потому что там слишком конкурентный рынок, где за большие деньги искупают очень умных математиков по всему миру, и они друг другу соревнуются. Короче, в трейдинге заработать что-то алгоритмами надо лишь в текущих моментах. И это не просто в текущих моментах. Короче, в трейдинге заработать что-то алгоритмами надо либо лет 5-10 потратить своей жизнью, либо удачно попасть, либо найти какой-то ещё мало интересный кейс. Вот три направления. Мне повезло тогда, что я не преднамеренно случайно оказался в нужное время. Ну и всё. Просто в текущий период времени попался. Сейчас криптея уже нет. Там уже занято. Короче, кривая доходности мы начали в середине 2017 года. За проект там до 2019-го крутился. Там кривая доходность вот так. 20% падала до 0% и потом в минус 6%. На корм-то тысячу долларов. А там на больших денег тем более нет. То, что будущее предсказывает плохо, как бы сейчас таймсириас это не... Короче, сейчас таймсириас, по моему мнению, это то же самое, что было с NLP лет 10 назад. То есть в таймсириасе точно будет прорыв. Точно будет foundation, time-series model. Сейчас видно огромное количество вещей. Кто туда идёт. И неплохие наработки я увидел. Точно в таймсириасе будет прорыв, на котором можно будет намного интереснее будущее предсказывать, чем сейчас. Потому что сейчас всё, что в таймсириасе крутится, это в 90% случаев базовые концепции с прошлого века. Arima, Sarima, Sarimax. Вот это условно Soto сейчас в таймсириасе в большинстве задач. Поэтому с нейронками можно в таймсириасе прикольно придумать. И всё, что касается предсказания будущей цены тоже. Если будете заниматься этим проектом, то могу вам сэкономить несколько месяцев вашей разработки. Предсказывайте не цену, а стоит покупать или нет. Если это шартовая, то сделайте. Предсказывайте классификацию на основе данных. И второе, напишите backtest-систему для обкатки прошлого, насколько бы оно хорошо работало. Вот это два места экономии времени. И из этого можно что-то интересное сделать. Как минимум, если вы хотите сделать что-то интересное, то это будет первые 5% распределения по качеству. Из всего того, что делалось в этом направлении. Две простые логики, две простые вещи. Хочешь попробовать? Но честно, есть вещи намного интереснее, и есть вещи намного более полезные для этого мира, чем пытаться предсказывать цены на акции. Или цены на какой-то актив. Условно появилось, например, где было предсказание. Ну не то чтобы предсказание. Вот, например, возьмем сайт Крыша Кейзет. Там есть, условно, цены на квартиры и так далее. И с помощью ML-моделей каких-то можно сделать код. Не то чтобы код, а вообще ML-решение такое, так сказать, которое будет показывать нам недооцененные квартиры. Соответственно, разделить по районам и так далее. То есть условно там квартира стоит 15 миллионов, а по сути он стоит должен 20 миллионов. Ну, примерно я так говорю. Это тоже все есть. Базовая списка, то, что решено. Мне казалось, этот на крыше есть такой. Но я на крыше сейчас это не увидел. У них на колесах вроде это есть. Типа, где она говорит, это ниже рынка. Колеса, Кейзет. Какое-то машину взять. Нет, тут вроде щечек у него. Вот, короче, вот этот. Ты это хотел, да? Да, да, да. Ну вот, на колесах есть. Мне казалось, на крыше тоже такое они сделали. По крайней мере, когда я в последний раз в баре сидел с их хедом, то мне казалось, я такое слышал. В российском циане есть аналог крыши. Возьмем какую-нибудь аренду, квартиру, показать на карте. Короче, в Мэлле очень много чего уже успели попробовать. Поэтому я вас и хочу побудить, и в советах я обязательно вам написал, что смотрите, изучите, есть ли уже текущая реализация. Где-то здесь было что-то типа о оценке цен, цена в объявлении. Вспекственно, она чуть дороже по оценке. Тоже в этой модели все предсказывается, условно, сколько с этими параметрами стоимость и на основе этого просто обычное сравнение. Но в колесах поинтереснее реализовали. В колесах у XUI лучше, чем в циане. Да, не знал даже, если честно. Короче, много чего в Мэлле сделано, особенно то, что в голову приходит. Надо просто это порезвечить, посмотреть. Это можно переиспользовать. Если у кого-то какие-то есть идеи на их Project Wiki, можно тоже сейчас последние 5 минут обсудить. Плюс еще, короче, если будешь ценно на квартиры идти или на какие-то дома, можно посмотреть это. Очень много крутых методов, огромное количество точек по делу, на каком-то уровне, на каком-то уровне, на каком-то уровне, на каком-то уровне, крутых методов, огромное количество точек по делу, на каком-то уровне, на каком-то уровне, можно посмотреть все методы, которые использовали, какие примерно они точности получились. Поэтому можно поизвучать. Так, нам нужны эти ноутбуки. Давай за последние 90 дней. Ну, там кто-нибудь сетки попробовал, прикрутил, хотя вот есть результаты. Кто-то вообще что-то прикрутил, тоже вещи, результаты. Поэтому я говорю, когда это новый проект занимаетесь, обязательно посмотрите на Kaggle, обязательно посмотрите на HuggyPracy, обязательно посмотрите на Archive, обязательно посмотрите на Medium. Кто-то эту же задачу решал или нет, что у него было по решению, чтобы вы могли ожидать. И посмотрите, может кто-то подобную задачу решал, если вы не решали. А если вам можно посмотреть, изучить и принять на основе этого решения. Ладно, Омазмек, давай продолжим. Ты какой хочешь проект убить? У вас есть пара советов, могут быть отдельные. Да я уже как-то честно теперь не знаю. Я думал насчет prediction house, ну вот как вы говорили. Сейчас уже честно теперь в тупике. Я специально вам написал вот идеальные вопросы, чтобы вы выбрали идеальный проект. Честно. После чего можно прям в стартап и в тузерму отправляться. Ну давайте сядем в мемокрассу. Посмотрите, где есть данные, и где ничего не делалось. Изучите. Короче, да если как и везде надо сидеть и изучать. Особенно если хочется делать какой-то стартап. Либо можно просто что-то начать делать, может там что-то крутое получится, и потом на основе этого пойти, тоже тузерму вылететь. Я такие кейсы тоже знаю. Они редкие, но есть. Поэтому тоже того стоит. Особенно когда что-то учишь Да, теперь буду сидеть думать насчет вот этих вопросов. Да, можно собрать данные по студентам, их посещая в месте их оценков. Например, предсказать какая оценка будет, если у человека такие-таки параметры, интересные таски, которые потом можно использовать. А есть, например? Можно поговорить? Можно с Солд Пером поговорить, собрать данные кейсы. На прошлом году посидеть, поработать с ними, и это будет вин-вин кейс. Может Солд Пер сделает какой-то кешбэк на основе этого, если что-то хорошее получится. А есть ли какие-то, например, сайты в Паттипа Кагл, где можно что-то вроде черпать идеи по тому, что можно реализовать? Пеппервизикод, кегл, хагинфейс, первые три. Там еще бы медиум добавил, на русском языке хаббор. Можно добавить кэпэйш, который... Там телеграм-канал, у меня там порядка 50 подписок на телеграм-канал, куда тоже можно какие-то идеи черпать. Советую идеи записывать. Я свой банк идей веду с года так, 12. У меня там уже порядка 1000 или 2000 записанных идей. Что бы интересно можно было поделать. Обычно, когда перечитываешь, там полный был счет, но процентов 5-10 более-менее что-то находится. Шикарно, круто. Спасибо. Будем думать. Супер. Лучше попробовать, короче. Серьезно говорю. Иногда получается, довольно даже часто, что чот начинается из-за нестандартного подхода, из-за горячего глаз. Там можно попробовать куда-то вырулить, куда до этого не вырулилось. Но я все-таки больше за это, за предсказуемую разработку. Какая-нибудь, то, почему мне получилось переувидеть стейколдеров в месте, где я работаю несколько лет назад, и довольно успешно сейчас идет все. А все, что как избежать многих подводных камней, я вам сегодня рассказал. Поэтому, если есть вопросы, был бы рад узнать. Либо мне каснуются лекции надо было давать в конце самого курса, когда вы все, условно, помучились уже и потом, на будущее вот это, вот это, вот это. Сейчас, как будто, мало кто смог оценить. Но поверьте, здесь сколько я в профессии, лет семь-восемь, но вот все проблемы за семь-восемь лет, которые я встретил, переобщался со многими, поэтому это точно ценная, ценная прям очень много. Я обычно эту лекцию продавал за очень большие деньги пару компаний, которые я показывал. А вам она тут достается очень очень бесплатно. Короче, советую, пересмотрите, когда будете решать какой-то эмейлный проект, пересмотрите после первого вашего картон-проекта эту лекцию, или как минимум пересмотрите слайды. Скорее всего, вы найдете дафиги, вещей, которые пересеклись. Про... Как сказать? Про... Про... Рефлексируйте над ними. И в следующий раз попытайтесь избежать многих из них. И так литеративно, от проекта до проекта, вы будете становиться все больше и лучше, как специалист, и для вас потом уже решить любую эмейлную проблему, не важно какую, уже будет казаться реальным и вполне предсказуемым. На этом, в принципе, все. Я всем хочу пожелать хорошего остатка выходных. Всем пожелаю классных идей и вдохновения при выборе проектов. Не забудьте, что проекты у вас, насколько я помню, мы сделали, что они будут идеей проекту утверждаться. Короче, бытьте готовы в том, что могут сказать, что надо будет другую идею придумать. Помните про то, что data's first. Попробуйте найти сначала данные, данные вокруг нас. Я честно скажу. Можно либо в текущем месте работать, где вы работаете, либо если вы в невере учитесь, подумайте, может, с какого-то курса можно какие-то данные пособирать, может, какой-то лабораторий можно данные забрать, чем-то позаниматься. В крайнем случае, посидеть по парсе данные в интернете. Либо найти какой-то прикольный датасет в интернете, и потом уже на основе него делать идеи, после идеи сделать обязательно заплетить какой-то датасторий. По датасторию уже будет понятно, что там имейлить. По намекам, что можно имейлить, вы уже примерно знаете, что есть регрессия, есть классификация, и вот с автоимейлем можно будет быстро прогнать эту июль. Это все. На этом все. Всем пока-пока. Спасибо, что пришли. Всем удачи. Спасибо большое. До свидания. До свидания. Я был самое главное. Накиньте лайки, если понравилось, не киньте без лайков, если не понравилось. Ну, черную мы не можем поставить, и по крайней мере, нет. Ну, окей, ладно. До свидания. Да, черную не нашел.