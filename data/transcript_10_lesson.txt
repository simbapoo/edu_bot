 Добрый день всем! Извините за опоздание Чуть-чуть не успевал Добрый вечер! Здрасьте! Ну, все зашел, раз свой играл Так, а все видно, да, экран сейчас? Да-да Охренеть Пострадал Вот тут, в десятое оно Смотрите, как получается сегодня В принципе, заключительная лекция Этого подблока И поэтому вообще Так, звук хороший, да, звук нормальный? Да, все хорошо, я слышу Хорошо Да, в принципе, заключительный подблок Поэтому, в принципе, как По многим просьбам Решил, да, акцент сделать на вопросы Больше, наверное, которые от вас будут идти То есть половина лекции пройдет Вопросы больше Q&A Далее, да, по темы как Over-sampling, under-sampling Затронем, ну и там про aginvalid поговорим Также покажу пару примеров В ноутбуках Ну и также хотел бы послушать вопросы По домашних заданиях, которые есть И, может быть, не разобрать А какой-то, может быть Ну, такой discussion session устроить По домашке Вот В целом так Давайте начнем Смотрите Так И Да, такую тему Хотел бы затронуть Как, по сути, наверное, достаточно Актуально для вашей задания Задания второму Это получается over-sampling Вообще, может быть, сказать Кто На данный момент По заданиям подвинулся Ну, или сдал Ну, хорошо, это over-sampling Не, по домашке Вообще, кто сдал или Работали, кто сейчас где Пока в работе Еще Пока что А, ну, по идее Да, я слышу, идеи был уже у кого-то Да, да, я на этом этапе Ага В самом начале И уже закончили Сейчас уже больше Построение модели Честно говоря, из-за того, что Слишком много всего надо вспомнить Обратно Довольно-таки тяжело вспомнить Ага Здесь такой момент, что В принципе, такой, может быть, небольшой спойлер Ну, помощь Что там много этих Ну, там около соотношений классов Около 1 к 10, да? К 0 классу То есть, задача, да, дисбаланса Существует также Вот, в целом, на самом деле В рамках, окей, домашки это окей Если, скажем, Вы как-то не предвидите Какие-то методики Для того, чтобы избавиться От дисбаланса, ну, лишь что-то нарастить Но в целом, как бы, вообще Это частая проблема будет, когда у нас, на самом деле Есть класс 1, но у него там Там сотни тысяч, да? Аккуренцев А у 0 класса, да? Или другого класса, у него там очень мало Соответственно, дисбаланс происходит То есть, у нас будет модель, будет Bist В сторону того, чтобы, ну, предсказывать В чаще и мере Для класса, да, наибольшей частоты Вот, поэтому Сегодняшняя тема про оверсемплинг будет Такая методика, как есть Смоте Ну, и Synthetic Minority Order Оверсемплиг техник Она, на самом деле, простая Учит в многоспособ, страшно Это такая штука Технология одна из техник Который помогает Сделать оверсемплинг То есть, грубо говоря, помогает вам Нарастить датасет до каких-то пределов Ну, чтобы выровнять что-либо Сразу скажу, что это не панацея То есть, нельзя Не бывает такого, что Обычного оверсемплинга Вы просто можете Плодить данные, да Потому что, ну, это Зачастую они, на самом деле В практике редко когда работают Прям очень хорошо Вот, потому что природу, на самом деле, данных Как-то симулировать Или мимикрировать Очень сложно В нормальном образов Вот, ну, как бы все равно Мы, думаю, мосхеф это пройти Вот, потому что Ну Методик, да, как мы генерируем данные Созначено много, в зависимости от домена Те же самые, допустим, изображения Ну, это через смотов Не сможете сделать Изображения уже будут генерировать Либо фотографировать просто новые Сэмплы собирать руками Либо проводить какие-то ауквентации Допустим, добавление шума Там разных преобразований математических Вот, либо генерировать, да Синтетически То есть, реально создавать какую-то модель, которая Будет генерировать Фейковые изображения Ну, те же диффузионки Ну, там все равно большой минус Даже они, как бы Без того, что там GAN Stable Diffusion, Stability Хорошие, да, картинки Генерировать На самом деле Если проводить какой-то реально глубокий анализ То зачастую Они, может быть, эти Картинки или данные выглядят визуально схожи В образом, да, то есть реально похожи там На людей Или похожи на какие-то предметы Которые вам нужно генерировать На самом деле, по структурной, пиксельной Более глубокой анализу Зачастую там Там есть сильные-сильные Такие Ну, то есть, такие изображения очень сильно Палятся Ну, там, может, человеческое анализ Примет, как нормально Но в целом, это по-пиксельной Там какие-то преобразования Если на растительные фильтры поставить Там отчетливо можно линейным классификатором определить Фейк от нефейка Вот, допустим, когда вы видите Генерированного человека лицо Визуально реально, ну, существует Да, существует человек такой А при, если бы там покопаться в структуре данных Распределениях, да, пиксельном RGB как раз расслабится Ну, дипфейк, как бы, типа того Вот, это к тому, к чему К тому, что, как бы, урсэмблинга На самом деле, чуть-чуть, по моему мнению, он Слегка, как бы Ну, зачастую переоценивается В использовании Вот, ну, окей, давайте возвращаемся К обличным данным, вот, к смоте В целом, концепция простая Он создает синтетические данные Да К тем, ну, как бы, такой синтетики То есть, он никак не дублирует, а синтетически создает их Да, в принципе, такой, что Как бы, создаются К, лежащих, да, каких-то Ну Да, ответ на вопрос в чате Все верно, Дмитрий На самом деле, вы можете Попробовать, поиспрементировать С тем, чтобы дать Взвешивание, да Модели На какой-то определенный класс То есть, грубо говоря, вы можете Написать, таким образом В вашу функцию Ну, или Подготовить датасет таким образом Преобразовать его таким образом Чтобы позитивные классы Имеяли, ну, одёрка Имеяла больший вес предсказаний Вот Взвешивание, это тоже один из принципов Как можно подавлять оммерсэмплинг или андресэмплинг, ну, когда дисбаланс существует. Вот, да, взвешивание можно сделать. На самом деле тоже это не... там много методик есть, все зависит от домена, все зависит от... еще можно, как бы, грубо говоря, не только взвешивать, еще можно будет делать, когда у вас, если у вас какая-то есть твоя custom null loss функция, функция потери, вы можете на примере L1 и L2 регуляризации тоже поощрять примеры, которые, условно говоря, ну, если у вас попадание в одерку есть, то поощрение сильнее дойдет. Вот. Если в ноль, то, соответственно, не так сильно, чтобы она понимала, что в одерку лучше, легче попадать. Ну, я слышал такие методики в минимум. Ну, в целом, да, все верно, взвешивать тоже можно, давать вес. Возвращаясь к смоте, то есть, по сути, пример такой, что мы должны найти, ну, мы определяем какой-то minority class, то есть, мы выбрали minority class, ну, в данном случае у нас это одерки, да, по домашке, а одерок там в отношении где-то 10% против 90% нули. Выбираем одерки, и получается, для каждой этой одерки находим в ближайших 5 соседов. Нашли 5 соседов в ближайших, и далее для каждого, получается, используя эти соседи и точку, которую мы нашли, мы находим, ну, сейчас я объясню, она, она, виспина. Потом находим, получается, какие-то средние точки между этими кластерами, соседями, путем там еще каких-то преобразований. Ну, вот, по сути, reputation steps, да, то есть, берем какой-то minority class sample, находим дальше k ближайших соседей для этой точки, из комы. Вот, потом считаем разницу, векторную разницу между соседями. Ну, векторная разница, это просто имеется в виду до фичи, словно какие-то. То есть, если у вас там 3 фичи, то у вас будет вектор из 3 разгранности, да? Вот, мы считаем, получается, разницу между ними, между этими соседями и точкой из комы. Дальше, ну, обычно, здесь методик много, обычно вожаем на рандомное число, какое-то от 0 до 1, да? Ну, да, вы просто в виду изменить вектор, радиансы вектора. И добавляем на оригинальный вектор. То есть, добавляем на вектор, на который мы собственно искали. Ну, если догадаться, да, по логике, то в целом, вот эти из комы в общем, как его? Из комы вектор получит, ну, в своем neighborhood, да? В своем районе точек, получит еще какие-то точки дополнительные, которые будут примерно рядом лежать. Рядом с ним. Ну, очень все просто, на самом деле. Есть массовый библиотек, который уже имплементирует это из-под коробки, этот алгоритм. Вот. Ну, и в целом, вот. По сути, вот, ваша точка готова. Вот. Дальше еще момент такой. Ну, да, здесь в целом advantages, да, это у нас ну, следствие идет того, что у нас больше соединения примеров. Больше, да, то есть у моделей доходит знание, примеры, как выглядят классы там с minority типа. Естественно, тем самым, как бы ну, занижаем overfitting модели. Overfitting отношения нуля, имеется ввиду. Вот. Ааа... Да, ну, тут creatorBaseDecision, Badress, понятное, в принципе, это об одном том же. Ну, и да, и мы тем самым это, видите, метод того, что это не полное дублирование, да, это, на самом деле, просто интерполяция данных. То есть мы не используем дубликаты. Потому что есть масса методик других, которые в своем оверсамплировании подразумевают полное дублирование данных. Вот. Ну, допустим, если там если вы обучаете какой-то нейронную сеть сверточную да, картинках, то... достаточно много, ну, бывает примеров, когда вы используете одних и тех же картинок, у вас одна картинка, допустим, вы делаете 100 ее копий, ну, оригинальных копий, но при этом накладываете просто какие-то фильтры на них. То есть гавосу и шум накладываете, в роты накладываете. На самом деле, это та же самая картинка. То есть здесь дублирование происходит. Но ввиду того, что вы просто какое-то преобразование проводите на ней, ну, вы, значит, вид изменили это. Вот. Ну, об этом поговорим, может, уже есть время будет. Вот. Да, в целом, ну, лимиты тоже есть вот этой вот такой техники. Как бы это первое, на самом деле, ну, бывает такое, что ну, вы не можете закодировать обычными вот этими правилами, да, Implementation Steps. На самом деле, универсальную логику построения природы данных. Ну, если бы это, если бы мы могли это делать, мы бы просто да, это сделали бы сами через Go to Fools. Но мы не можем это смоделировать, да, поведение данных. Вот. То есть вы можете генерировать, ну, нойди снайпов, то есть вы можете зашупление сделать какое-то. То есть у вас данных будет реально шум, да, потому что, ну, вы не знаете, на самом деле, насколько вот эта вот методика, да, там умножать на от 0 до 1 рандом, там, на разницу, да, векторов, и брать это как новозначение. Возможно, это в природе вообще нельзя так делать, это в этом в этом домене данных. Поэтому нойди снайпов могут появиться здесь. Потом, да, такой overgeneralization, это когда вы слишком обобщенно говорите. Ну, это получается. Мы стремимся, конечно, к реализации всегда, но overgeneralization тоже плохо. Потому что, как бы, если говорить обычным языком, то можно сказать, что там, ну, вообще ничего не сказать, условно говоря, по модели. То есть вы просто, типа, на все кейсы говорите там очень тривиальные вещи, которые в целом, ну, ничего, никакой ценности не дают задаче. Вот. Ну и в целом, еще такой момент, третий, это ignore majority cloud distribution. В целом, на самом деле это очень хороший понт, на самом деле третий. Это, кстати, тоже возможно в домашней это тоже, на самом деле, правильно подтвердить. Ну, мне очень важно будет ваш feedback почитать. Не всегда баланс классов, да, он так важен. Не всегда. Я скажу почему. Потому что, смотрите, бывают такие ситуации или задачи, которые мы решаем, где вот этот shift с Q в данных, да, в сторону какого-то либо класса majority, это натуральным образом так должно быть. Потому что, ну, вы не можете, условно говоря, работать там в данных, которые всегда на самом деле, ну, полностью сбалансированы. Ну, потому что это в природе как бы неправильно. Условно говоря, если у вас встречаются там как сказать, допустим, ну, в природе на бою, да, модель работает боевая. И если как бы на самом деле встречается там 90% таких случаев на бою, ну, допустим, фрод, не фрод, да, то есть скам, то есть какая-то афера, не афера, да, в каком в каком это, эти технологии. Ну, и понимаете, что на бою в целом, ну, если там посмотреть, не знаю, скамов разных бывают, ну, в банке бывают афроды разные, там, мошенники, да, транзакционные какие-то мошенники, которые там просят деньги перекинуть, там подобное, то в целом в 90% всех потоков данных, да, транзакции белые, они нормальные, вот, а процентов 10 меньше, это уже фродовые такие. Соответственно, вы, ну, будет не очень правильно, если... сейчас, секундочку. Да, здесь. В целом, то есть да, возвращаясь к кейсу фродом... Камера забыл почтепиться. Да, возвращаясь к кейсу фродом, это говорит о том, что, ну, это будет скорее всего неправильно, если вы будете обучать модель, условно говоря, в любом случае, я бы хотел бы выяснить, что у вас есть такой, ну, модель, условно говоря, 50 тысяч будет нормальных людей, примеров транзакций и 50 тысяч скамовых транзакций, фродовых, потому что модель на самом деле поймет, ага, типа ты мне даешь вот такие, такое распределение доравное, с точки зрения техники да, есть все изящно, все красиво, у вас там красивое распределение одинаковое, да, но в целом это неправильно, потому что модель будет тогда в данном случае полностью в плане выделения decision bound в баундере своих, да, она будет максимально, ну, отдавать и тому и тому шанс, вероятность, ну, не вероятность и шанс, а шансы выдавать и на тот и на тот исход одинаково, что неправильно, потому что, ну, по природе, там, не знаю, отсуть и домена зависит, конечно, по природе того, что фроды гораздо меньше встречаются, чем нормальные транзакции, соответственно, видите, вы должны в этот рейш все равно балантировать, потому что, ну, тогда у вас будут просто чаще нули, адёрки выпадают на фроды, и, соответственно, вы зарежете половину своей базы клиентов, говоря, что они там какие-то скамовые юзеры, а они как бы не такие, соответственно, потому что это вышло из-за того, что ваша модель была там одинаково там сбалансирована, то есть, в каких-то случаях это не всегда хорошо, на самом деле, поэтому, возможно, и в случае из-за домашнего здания, да, там, это тоже не совсем, ну, нужно проверить, в общем. Либерс, по сути, ну, я так понимаю, мы можем это все свести в разряду, что нам уже не пресиженный лирикол, ну, словно, да, к доменному пониманию, в случае с фродом, в том же случае с фродом, типа, что фроды действительно 5,5, ну, 5 процентов, по-моему, recall из-за того, что фрод может быть сильным, да, и как бы человек потеряет деньги, то есть, там, условно говоря, recall важнее иметь, чем precision, возможно, если мы для 5 еще процентов сверху людей там что-то сделаем, то ничего страшного, да, типа, забаним их там на время, вот. На самом деле, да, хабтор, да, да, да. Да, и, соответственно, почему я сказал 5 процентов, допустим, просто от этого как раз и будет зависеть, ну, вот тот параметр, да, который, сэмплирование, потому что чтобы не 50 на 50 делать, а там условно, вот я в домашке собирался сделать, там, типа, если 10 на 90, то хотя бы 20 на 80, потому что все равно у нас, ну, там, получается, что rickety огромный, но у нас спойлер f1, score, мне кажется, самый такой адекватный здесь, он, типа, не очень хороший, вот, у меня получался, поэтому я подумал, да, что нужно чуть побольше докинуть единичек, слишком мало их просто было, вот я по confusion matrix посмотрел, да, ну, там если не ошибаюсь 10 на 90, да, кажется, отношение было, ну, вроде, да, да, что-то такое, ну, вот, ну, по сути, как вы смотрите на то, что, типа, там те же 20 на 80 вместо 10 на 90 сделать, это сильно там, не знаю, то есть, надо на валидационную метрику просто, на среднюю валидационную метрику просто, то есть, если у вас валидация выросла, то значит, это нормально. Да, хороший пример, Дмитрий, отличный поинт, на самом деле, то есть, часто бывает такое, что есть trade-off между precision recall, да, то есть, между точностью и между полнотой в этой модели, то есть, я, помните, говорил, что precision recall это две метрики, которые, как бы, по своей природе они друг другу чуть-чуть, не то что противоречит, они друг другу и много, ну, противоборствуют, если так можно сказать, то есть, ну, очень редко бывает, когда на бою, когда у вас и хороший precision, и хороший recall, потому что по своей логике отсечение, да, принятие решения, precision это кейсы, когда вы, да, ну, то есть, у вас при большом precision у вас всегда будут маленькие, почти всегда, ну, в реальных работах будет маленький recall и наоборот, да, при большом recall будет маленький precision. Точесто, как и в случае, да, об этом говорили про разные доминные задачи, вам нужно иногда выбирать, да, что для вас приятнее. Вот, соответственно, да, то в случае там fraud, до того же дата нужно смотреть, да, какой fraud, если ничего страшного, там, допустим, уронить precision, да, при выдаче там этих, а при этом охватить и покрыть реальных там fraud-овых людей, то в целом это, наверное, да, recall будет выше, скорее по своей сути. Вот, ну, все смотреть надо, просто нужно опираться на здравый смысл. Вот. Так. Спасибо. Ага. Микрофон, да, нашли? Не, мне просто сейчас надо будет это дойти. А, понял, понял, хорошо. А потом не смогу, да. Так, ну да, смотрим, в целом все просто, на самом деле, честно. Также, да, и вот есть такой его партнер, да, контр-партнер, это вот Adazin, Adaptive Synthetic Synthetic Synthetic. Все то же самое, на самом деле, а как бы просто у него такой концепт другой, Дэнил вот так назвал, help those who need it most. Получается, ну, она адаптивно выбирает, кому наращивать больше примеров. То есть, грубо говоря, ну, в каких-то задачах вам не всегда нужно, вам не всегда нужно для всех точек в minority классе нужно там всем добавлять точки. Почему? Потому что бывает такое, что природа вот этого сэппла, точка, точка какая-то, примера, она там, условно говоря, слишком распространена, слишком плотная точка. Ну, имеется такая точка, встречается очень много плотностей. Ну, то есть, условно говоря, если вы спауните, генерируете хитов кейсы для транзакционной активности, юзеров каких-то, я не знаю, сервиса кинотеатров, вы там понимаете, есть какой-то средний патриот клиента, да? Там, не знаю, мужчина 300 лет любит, не знаю, какие-то боевики, да? Таких вот, таких кейсов там, ну, я не знаю, это эвтрирую пример, какой он будет. И там посмотрите, вы анализируете данные ваши, и вы смотрите в целом, у нас почти, вот клиент мужчины 300 лет любит экшен и ухожий ужасы, это такой очень сам расстроеный портрет клиента. То есть, его в наших исторических данных встречается таких людей очень много, он очень плотный. Соответственно, я понимаю, что, наверное, сэмплить еще таких кейсов, еще там пару, двадцать, десятков штук, сотен, нет смысла, потому что, ну, мы и так про него всю глубину, да, вот этого области, этого параметра данных, мы и так, в принципе, понимаем, да, как это работает. Вот. Соответственно, а давай сфокусируемся, ну, а да, сфокусируемся на более сложных кейсах, ну, вернее, не так. Соответственно, как бы, кейсы такие, как, допустим, вот, скорее всего, кейсы такие, как мужчины 300 лет, которых очень много, модель без проблем умеет различать, ну, что их очень много в целом. А есть такие, да, более сложные, которые там чаще, реже встречают на природе, да, возможно, какие-то очень редкие люди, психотипы, там, не знаю, и в целом их в ваших рекордах, ну, или не то чтобы редкие, скорее всего, возможно, редкие для вашего сервиса люди, и которых, скорее всего, будут сложнее там найти, понять, отличить, классифицировать. Ну, они там часто пересекаются с другим классом, да, их ошибают, вы ошибаетесь на них чаще, они более сложные для вас в определении, да, классификации. Ну, какие-то edge-кейсы, да. Соответственно, вот, Ада Син, что и делает? Он говорит, он focuses on difficult to learn cases. То есть, в спорте он выбирает какие-то кейсы, которые, как бы, наиболее сложны в определении, да, ну, и как это делается, делается в следующем образом. То есть, у вас, как бы, на каждый сэмпл, да, еще создается такое, условно говоря, difficulty score, то есть score сложности. И, ну да, how it works, it's fast challenging minority samples. То есть, он находит более такие сложные кейсы, а сложные кейсы здесь определяется логика того, что рядом с ними больше majority classes. То есть, у вас, как будто, minority class, да, и если рядом с ним много majority class, то есть, ну, это логично, что у вас получается какой-то класс, он, и рядом с ним очень majority другого класса. Соответственно, понимаешь, что он очень рядом, да, стоит, это очень сложный класс для дискриминации, да, какой-то. И, соответственно, они считаются более сложными кейсами. Допустим, если будет другой пример, да, где вот очень много его же классов, да, такие же minority, вы понимаете, что он очень хорошо лежит, да, в плоскости, и его там, условно, линейно разделить можно, да, сложности не будет. Вот. Ну, и да, получается, и мы генерили больше классов на difficult, hardcore, hardcore-ные кейсы и меньше для, там, для более легких классов. Ну, допустим, вот экзампл, да, у нас есть point A, да, какая точка A в пространстве есть, это все minority класс. Point A, их мало, да, этого класса, там, класс, это ноль, допустим, его мало очень. Он окружен, да, четырьмя точками из majority, то есть, одёрками, да, разными, ну, его ближайшие там какие-то соседи. Понимаем, что этот, и в точке точка B, да, ещё, она окружена только, ааа, как его, одной, одним примером, одной точкой из, из dominier, из majority класса. Мы понимаем, что, ну, соответственно, на point A нам нужно создать больше точек, потому что она, как сказать, ну, там сложнее, в случае, отдалить. Ну, как бы, такие вот моменты здесь. Ну, на самом деле, подход тоже интересный, просто, без как его знать, это очень интересная штука, но здесь тоже нюансы свои есть, скажите, какие. Зачастую бывает такое, что, ну, это интуитивно понятно, да, что если у вас точка, на самом деле, лежит очень рядом с другими точками другого класса, да, по своим признакам, определениям, мегтарам, там тем же самым, скорее всего, это может быть такое, что это какой-то шум в данных. Возможно, это просто какой-то системный шум, то есть какая-то запись, получая на тестовая запись, попало ваше табличное пространство, либо это, на самом деле, не minority класс, это на самом деле majority класс был, просто который label, ну, label кто-то ошибся, потому что, ну, по логике, если он лежит по большей степени рядом с majority классами, то, скорее всего, там какой-то есть взаимосвязь здесь. Ну, то есть это тоже надо иметь в виду, что эти методики, на самом деле, они очень примитивные в своей логике. Я опять повторюсь, они не могут выдумать универсальные правила, как вам синтезировать данные. Вот. Еще такая тема, я здесь не включил, на самом деле, слайд про сэмплирование. Ну, зачастую, да, зачастую еще очень хорошая методика, я, если не больше всего, ее придерживаю. Это вот эти, честно говоря, две методики, я сам очень, когда редко ли бы использовал в своей работе, потому что зачастую они не очень плохо работают на практике. Ну, третья, о которой я сейчас говорю, она на самом деле чуть лучше работает. Я ее хотя бы минимум на Blue применял. Это получается такое статистическое моделирование. Это когда вы пытаетесь через какое-то статистическое распределение, тоже гауссовое распределение, вы пытаетесь смоделировать какие-то данные. Что я имею в виду? Третье. Давайте пример возьму, какой интересный пример. Предположим, да, вы пытаетесь, я не знаю, пример такой, допустим, вы пытаетесь смоделировать данные, синтезировать данные там для какой-то, я не знаю, если взять там, не знаю, телекоммуникацию, какую-то дачу телекоммуникации, и вы пытаетесь смоделировать какое-то, ну, физическое распределение, свойства, да, ну, какой-то там, не знаю, 5G сигнал, допустим. Вот, 5G сигнал. Ну, 5G, кто может слышал, он работает в миллиметровом спектре радиочастот, то есть миллиметровые волны, это получается у них очень высокая частота, соответственно, высокая частота, очень низкая длина волны. Да, я просто до AML-а я в этом быстро работал, и, да, оттуда все это помню. То есть получается, когда у нас, когда волна движется там, ну, я такую воду дав, чтобы прям понимание было, когда у нас волна, электромагнитная волна, она, когда движется в пространстве, она имеет какую-то миллиметровую волну, длину волны и высокую частоту. Ответственно, такого рода сигналы они как бы, ну, это физики уредят, такие, такого рода сигналы у них есть какое-то обобщающее свойство. То есть антенна, которая гидирует такие сигналы, у нее есть только, ну, закономерность. Ну, ввиду того, что, конечно, это все-таки эти данные, взять как input сигналы, это антенны, да, это, конечно, случайные, ой, это случайные переменные, да, то есть random variables, да, там какие-то, давайте скажем, где это, да, там большая X, да, random variables. Потому что мы не можем каждый отдельный сигнал моделировать с точностью. Соответственно, когда люди как открыли это, у таких сигналов есть какая-то природа. Природа там есть какое-то распределение. Если я ошибаюсь, там, ну, она похожа на гауссова, она вот такая, ну, искривленная. То есть это получается частота сигнала, ну, это вот как бы density, да, плотность, ну, как гауссова, да, а это наш какой-то параметр там, как его, так, сейчас это была плотность, это получается, ну, можно допустим написать это как, допустим, какая-то, ну, какой-то энергетический потенциал с шкалы. Соответственно, мы понимаем, что зачастую, как сэмплы, да, сэмплы этого сигнала, они генерируются, допустим, чаще, да, на вот этом диапазоне, да, вот, то есть, ну, потому что плотнее всего, здесь больше нескольких тысяч, ну, плотной графикой, и, соответственно, там при эти, да, она чуть меньше, да, и так дальше, а здесь она тоже маленькая, допустим. И смотрите, что такое, это, допустим, не знаю, там, есть много типов распределений, да, статистических, допустим, есть гауссова, да, гауссиан, там, белькерд, да, допустим, нормальное распределение. Ещё, допустим, там, ну, на когаме, да, там, не знаю, экспоненшал, да, распределение есть, там, юниформ, бирнули, да, их очень много, на самом деле, это, это прям такие разные, уже такие зашица, хардкоденные функции какие-то сложные, но они тоже, да, империйские считались, конечно. И вот это, если я ошибаюсь, каждый для 5G моделируется там на когаме. Соответственно, как бы, это, мы говорим, да, вот, я беру за основу там на когамер распределение для того, чтобы генерирует там 100 тысяч примеров моей природы данных. И он такой, хорошо, давай по этому распределению генерирую себе данные. И он учит вот это распределение, да, ещё там какие-то параметры есть, да, там, ширины, долготы, там, всяких, я не помню, очень много параметров. Он рандомный, да, в виде того, что это рандомная величина, да, он каким-то рандомом по этому определению генерирует вам сэмплы. И, таким образом, вы генерите себе там кучу примеров, которые с точки зрения распределения природы своей, она будет прям реально совпадать. Такие сигналы очень будут близки в природе. Ну, чтобы вы понимали, допустим, вот эти диффузионные модели, да, когда вы там, далее, да, кто работал с далее, скажите, пожалуйста, или тыкал, трогал его просто. Я нет. Дали два, дали три, там, что-нибудь. Никто не работал? Работал, мы типа, типа использовали или как? Да, да, да, просто, да, в смысле, просто, я 열심히 서�дrade AI Мalts? Не все, но 90-80% большинство моделей, которые генерируют изображение на основе каких-то входных данных текстовых, да не только текстовых, они стартуют свою итерацию в генерации этого изображения с чистого галлосового шума. То есть они стартуют все свои итерации с такого сигнала. Ну, в случае нестандартизированного галлосового шума. Они берут идеального галлосового шума, с 0 до 1, на сахманении стандартном, и начинают с каждой итерации что-то выплевывать. Плюс добавляют ваш текст, контексты, напиши, нарисуй, как корову сидящему. Она какую-то информацию эту кодирует, но начинает с чистого шума. Ну, так создаётся... А генерация изображений, по сути, это тоже своего рода, по логике это тоже сэмплирование своего рода. То есть вы сэмплите, сэмплите, сэмплите. И ввиду того, что почему результаты всё время разные, почему написал сегодня корову такую-то, написал один промтом, корову что-то какая-то, так и уже промт закинул через минуту, через секунду, и выход модели друг от друга уже. Третий раз ещё другой. И соответственно вы вряд ли попадёте на такую же корову, как в первый раз. Почему? Потому что это вероятностные модели. Они строятся на вероятности. То есть вы действительно... Вероятно существует, да, от чего сортовать. И в этот раз она сортовалась с этой точки. В следующий раз будет с этой точки. Ну, зачастую чаще будет с этой точки сортовать, где-то здесь, потому что её чаще встречаются. Но бывает там и здесь попадать, и там подобное. Поэтому, условно говоря, чаджи пяти вам отвечают чуть-чуть по-разному. Потому что там тоже вероятностная модель под капотом есть. Ну, это... Окей, это мы, кажется, ушли слишком. Вот. И запросы были какие-то? Может, будут вопросы, или нет? Вопросов нет. Окей. Самый интересный. Самый интересный. Самый интересный клаваут. А, окей. Да, давайте, да, передаю ещё. Я попытаюсь сейчас, конечно, максимально, что я смогу сам вам преподнести. Но опять-таки, да, это на самом деле мне... Допустим, понять этот айген вектор, айген валис, да, потребовало, ну, там, пару недель, на самом деле. Примеров, чтение книг. То есть это вещь, которую надо прям сесть полно самому, глубоко, раз-раз с собой сесть, изучить это. Вот. Это такой-такой небольшой дисклеймер. Но в целом, попытаюсь объяснить. В целом, смотрите, ну, диффинишен простой. Нашел интернет и его, да, не буду там лукавить. А, ну, в целом, я не знаю, что вам сказать. Ну, я не знаю, что вам сказать. А, ну, в целом, смотрите, да, очень простой. Нашел интернет и его, да, не буду там лукавить. Получается, есть айген вектора и айген значения, да, валис. Соответственно, говорится, да, что очень хороший диффинишен, на самом деле. Он очень такой ёмкий, покрывает в таком коротком продолжении почти всё. Есть айген вектора, да, называется вайги вектор. На русском называется собственный вектор. Вот. Одну секунду сейчас. Есть какой-то айген векор, да, получается. Он такой специальный вектор, на самом деле не специальный, как бы, но он такой специальный вектор просто, который, ну, вы слышно, вы слышно, да, который, когда его, получается, преобразов.. Когда вы преобразовываете, линейное преобразование проводите с матрицей, да, то есть просто умножаете матрицы на него, то получается, эта матрица, она получается, только, ну, скалируется. Значение этой матрицы скалируется. Что это значит, получается, ну, сейчас объясним подробнее. Что это значит? Значит, она не меняет направление. Ну, то есть, как бы, любая матрица – это набор векторов в себе. Ну, какая структура векторов, вернее. И вектора – это величина с направлением. Естественно, получается, какая-то матрица есть. Мы умножили её на линейную трансформацию, то есть, как бы, вектор, то есть, айген вектор. Проявляет эту трансформацию линейную, да, то же самое там, линейная грести, да, это тоже преобразование линейное. Передержаете ваши х и на ваши w. И это так происходит, что в целом эта матрица видоизменяется только шкалой с ней, ну, скейлом. То есть, она не меняет направление вообще. То есть, она куда тыкала, да, в фонограмм, в ту сторону, просто она меняет магнитуду своего вектора. А этот скейл и фактор, это коэффициент, то есть, шкалы, это, по сути, айген валь. Не у всех матриц есть такие вектора и такие значения. Вот. То есть, у Бербога смотрите, что я имею в виду. Окей, звучит, да, звучит слегка странно, но постараюсь объяснить. Что я имею в виду? У нас есть такой вектор, да? Ну, скажем, так, 5, 5, 5, да? Очень симпл, но, окей, главное объяснить. Это вектор, не знаю, w. Ой, ну w, ладно. Вектор w с точками 5 на 5, 5, 5. С точкой 5 на 5, 5. То есть, получается, смотрите, ну, имеется линейный, да, его, излево так, чтобы преобразовать, но при этом не поменять его вектор. То есть, по сути, если мы там умножим на 3, да, и мы там умножим вектором, это eigenvalue, да, наш, то вектор, по сути, не изменится, верно? Просто будет там 15, 15, да? То есть, вектор вообще не изменился. Направление изменится, да. Он изменился только своей магнитудой. Вот. И, по сути, в этом, в этом, по сути, вся и соль. Eigenvector и eigenvalue. Вот. Честно, не знаю, так, да, соответственно, можно следующее сказать, ну, такой definition, а как, собственно, найти эту штуку? Ну, у нас такой assumption, 4 assumptions происходит, 3 assumptions и четвертое решение. Первое, получается, вот это наше a, да, это какая-то это какая-то матрица, ну, входящая матрица, которую мы хотим там что-то с ней да сделать. Ну, в случае, помните, PCA, да, если вы помните, PCA, в главном случае, когда PCA считается, он использует eigenvector и eigenvalue под капотом, а вот эта a, матрица a, она была квадратная матрица, корреляционная матрица была. То есть, она была там, все фичи на фичи, да, в распространении, там 200 на 200, в принципе, пример какой-то был, 200 на 200, то есть, коэффициент корреляции каждой фичи с каждой фичой. То есть, у нас есть какая-то входная матрица, которую нужно преобразовать линейно, да, и какой-то, ну, существует ве, вектор, да, вот этот агент, вектор. И мы говорим, ну, мы утверждаем, да, что вот этот v, да, будет являться, вот этот v, будет являться eigenvector, матрица a, только тогда, когда вот это перемножение матрицы a на вектор v, равно перемножение какой-то к скаляру, да, это не матрица, это скаляр, на этот же вектор v. То есть, когда мы перемножаем, ну, вы можете сами представить, да, это как это интересно, вы умножили вектор на матрицу, то есть, смотрите, у вас матрица, да, умножается на вектор, равна, блин, она умножилась матрично, и она равна, равна полностью по своему значению, скаляру умножить на этот же вектор. Ну, не знаю, если вас это, как-то, как-то, не интересно, если вы как-то тронули, да, не знаю, да, то это самое интересное. Расслужил, да, что-то говорите, да? Да-да-да, вот касательно скаляра, он же умножается, например, указали 5 и 5 вектор, давайте пример возьмем по скаляру. Не понял, еще раз, что вы хотите вот это? Именно скалярное значение вот это хочу напомнить. Ага, сейчас придем к этому, да? То есть, смотрите, здесь весь прикол в том, что получается, ну, по сути, вы умножили матрицу на вектор, это равносильно, что умножить скаляр на вектор. Ну, это значит, какой-то есть вектор и какой-то скаляр, что вот это равно, это очень интересное свойство. Лямда это получается, лямда это и есть наш скаляр, или Eigenvalue называют его лямдой. И, соответственно, обычно решается, уже математически поставить задачу, да, как это вообще найти. Решается следующим образом. То есть, мы берем, ну, там на самом деле есть еще ряд доказательств, да, как отсюда прийти вот сюда. Ну, оно достаточно большое, емкое, там много объяснений, почему это так, там, я положил сюда выживку. Если кому интересно узнать вообще, а почему, с чего это мы можем так делать, что такое identity, вот это I, это identity матрица, identity матрица это, ну, тождественная матрица, да, или единичная матрица. Это получается матрица, где а на квадратная, где у вас один, один, один, да, ну, ноль, ноль, то есть, все диагональные элементы, они равняются единице. Это такое identity матрица. А все off-diagonal, да, вне диагональных элементов матрицы, они являются нулями. Ну, матрица, тождественная матрица или identity матрица, как так называется. Вот. Соответственно, это работает таким образом. То есть, мы должны, условно говоря, ну, то есть, скажем еще матрица, еще раз, A, да, у нас, они одинакой размеры, у нас 3 на 3, ну, в два сутки 3 на 3, и 3 на 3. Ну, какая-то матрица, да, произвольная там, где какие-то числа, прям есть 3 на 3. И, видите, вы отнимаете, да, то есть, условно говоря, у вас здесь еще лямбда будет, да, соответственно, если I на лямбда, это будет лямбда, да, здесь. Вот. Соответственно, айдур, и вот, когда отнимаете, ну, получается, A, отнимаете от этой матрицы, да, допустим, тут какая-то матрица будет новая. И потом у нас, что, какие-то значения, да, существуют здесь, ну, здесь, здесь что-то там, какие-то значения, да, здесь получается будут… это значение минус лямбда, там переменная, здесь тоже будут значения лямбда минист-пермен awe, здесь тоже будут значения вот это, минус лямбда, переменная. Здесь какие-то там значения оригинальные, 찍Голем 타, мы только вышли. Такое вот, здесь нули были. И потом мы такие, а теперь, найди детерми reven вы Griffith fcream, да, этой матрице, вот. И hydrogen muse, чтобы мы видели решениеock 하는. conveyles, si. И потом, когда мы решаем эту задачу, то есть мы когда берем детериминант этого выражения, нашли ну знаете, детериминант это вообще скалер то есть по своей значении это скалер соответственно, мы находим векторат таким образом, чтобы оно работало, оно равно 0 было равно соответственно, наши лямды, получается, у нас 3 оси, у нас будут 3 лямды, 3 вектора будут и они будут таким образом подобны, чтобы детериминант этот разница равен 0 любому соответственно, у вас, словно говоря, на выходе, я сейчас не буду это читать, потому что это будет очень долго занимать даже на каких-то обычных примерах лямда будет 1, будет V, лямда 1 смотрите будет лямда 1, да какой-то там, я не знаю, 3, будет лямда 2 будет 4, допустим лямда 3 будет еще там, я не знаю, 1 и соответственно, будет такой же, да, еще и вектор 1 там, это 2, вектор 3 он там будет тоже чему-то равен, чему-то тоже равен, и чему-то тоже равен размерности 3 на 1, 3 на 1, 3 на 1 вот ну вообще, я еще скажу, что здесь немного хитрее бывает на самом деле, не всегда бывает такое, что если у вас размеры с 3 на 3, то у вас будет тоже 3 лямды вектор, лямды вальвер аген вальвер, аген вектор не всегда такое бывает, бывает иногда очень коррорная кейс, где остается только 2, допустим там много, ну, профов есть также еще хочу сказать, смотрите, кто разнимательный, заметил, да, что ну как бы сказать, более правильнее даже а, ну, есть всегда такой базовый вектор, да, в данном случае, это когда ww, а не wv, аген вектор равен 0 если вот это 0 будет равно, то это условие работы тоже будет честно, мы говорим, как бы найти вектора такие дополнительные, чтобы они не были равны 0 ну, это тривилл, на случай то есть это, это, это, это, это, это, это это тривилл кейс, он нам, он нам ничего не дает, то есть, ну, да, ок, и что, типа я знаю, что если вот вектор 0 вектор он будет для всех матриц аген вектором потому что если я наживаю это матрицами на 0 и лямбда скаляр на 0, то это будет 0 равно ну, это дурак знает, поэтому обычно эти вектора отличны от нуля еще хотел бы добавить, что так, так, так ну, а чем это офигенно, да? офигенно это бывает тем, что мы по сути, когда выстраиваем вот эти аген вектора и аген валис по своим, ну, вообще, когда мы высчитываем их кстати, а почему-то у нас так мало, 4027 человек, а это, это, это, это может быть, что-то мы высчитываем их кстати, а почему-то у нас так мало, 4027 человек можете кто-то объяснить, почему так мало, из-за чего? просто, обычно урок в понедельник идет а, точно! точно, точно, это получается но они же неформированы же вот только что вот 5 человек вышел, узнают что-то или вообще, может быть, по сейчас это домашка, да, может, у нас СП этом или как? нет, скорее всего, многие, скорее всего, по режиму там что-то кто-то уснули а, все, понял, хорошо, ну да, значит, там будет неудобно, неудобно на втором окей, понял да, хотел обещать извиниться, да, что вчера не получилось я даже был срочно, да, но это я физически не смог в ассоциативе был в это время, поэтому я никак не мог, да прошу прощения за неудобство вот смотрите, еще момент такой я хотел объяснить по этому айгенвали с айгенвлектором помните, да, в случае скажите, пожалуйста, PCA все помните, да, сейчас? ну хотя бы примерно, что это такое PCA было да, да, да ну, то есть это была такая штука, которая там помогает нам понизить размерность, да, наших данных матчных и еще там, помимо размерности понижения, она еще и какие-то создает новые новые фичи, да, то есть принципа компоненты создает новые вот помните, да, что фич новые признаки, которые преобразовывают после PCA они не не не интерпретируемые то есть она создает, а у вас был пол, не пол, там, транзакционная активность там монетарная активность, там, подобное, она просто оборотит как-то PC1, PC2, PC3 и эти эти величины будут вообще вам не понятно вам будут величины минус 0.3 вы никак не помните, что это значит но они работают теперь смотрите вообще как это в смысле, откуда это все идет во-первых, это идет потому, что мы считаем агент валеса, агент вектора то есть мы находим, понимаете чем она вообще полезна, агент вектора, агент валеса, тем, что мы находим какое-то количество вернее, мы не находим, а да, мы находим эти вектора какие-то значения из этой нашей матрицы оригинальной A которые максимальным образом описывают всю вариативность данных внутри нее то есть помните PCA, да, там, вы говорите вы там PCA когда задаете, да, там с 200 параметров уменьши размеры до 20 то есть это ваша 20, да, D20, да условно говоря это есть количество лямбда факторов которые брать помните, как PCA строите, мы там считаем посчитаем сначала считаем коллекцию на матрицу здесь наша A, матрица A и вокруг нее мы рисуем условно говоря, наш агент вектора, где валю находим но мы опять-таки, ввиду того, что мы хотим понизить задачу, да с 200 до 20 мы такие берем и говорим, ага, ну при этом возьми топ-20 агент валес и соответствующих их агент векторов что есть ваген валю еще помимо того, что это просто скаляр заточено или заложено еще своеобразная сила ну не сила, а ранг ну не ранг, тоже плохое слово ранг матриц тоже из не так другое ну в общем, какое-то ну математическое свойство, да, по величине что оно является более сильным соответственно, если вы отранжируете, да, все лямбда вектора по лямбдой валюй, да, их то вы получите наиболее сильные вектора которые обобщают в себе закладывают в себе всю вариативную данных их начальной матрицы A соответственно, если вы скалярм там D20 выбрано, то вы берете топ-20 по агент валес, агент векторов и вот эти агент вектора, если вы их стекнете обратно, да, стекнете это будет ваша какая-то трансформация линейная то есть, и потом умножить вот это умножить ваши начальные дататуры все ваши начальные датасетсы, да до, ну до, до до PC большой датасет умножите матрично-линейно преобразуйте его за вот это вот за вот этот стеков агент векторов у вас будет уже новый результат это будет уже новый датасет с новым принципом принципом компонентс с пониженной размерностью и так и образом, что эти 20 векторов сделают так чтобы вы максимум вытащили со своей со своих реагеренных данных то, без потери конечно, нереально тоже, но потеря будет небольшая, но она все равно будет максимально в этих 20 этих заложено там обычно бывает такое, что обычно если вы там условно говоря отсортируете, да по Explained Variance, да, то есть Explained Variance объясняемая, да, вариативность по там лямдо а ну, это просто лямдо то, будет такое, да, что здесь будет такое, тык дальше вот так, тык вот так, да, и все, и поехало то есть, в самом деле, вся там вся общая информация, да, от всех, всей информации, которая хранится в этой а, она заложена в первых там, нескольких значениях а в основном там, ну, уже маленькая, маленькая, маленькая и там надо, надо на 200, да, который последний ось, условно, в нашем примере 200 там вообще почти ничтожная, там ничего такого нет, информации вот, обычно, ну, поэтому мы когда ну чтобы это все быстро работало, да то есть мы, мы, мы это хотим до того, чтобы все это быстрее работало то есть, допустим, алгоритмы там и андекс музыки, да, когда вам рекомендует новый трек они считают, ну, сколько у андекс музыки клиентов, я не знаю миллионов сорок, миллионов пятьдесят, если не больше или сто даже у вас сто миллионов клиентов есть и у вас есть ну, сколько, сколько десятков, или, может быть, даже сотен миллионов аудиозаписи в их базе чтобы вот это вот огромное матричное пространство с ними адекватно работает очень быстро, очень хорошо вы по-любому должны какую-то, ну, сжатие делать, ну, компрессию создавать в данных чтобы это все быстро считалось потому что, ну, ни одна машина в мире какие бы у вас ресурсы были, даже у андекса с его ресурсами там будут проблемы с масштабированием таких решений и поэтому мы должны использовать такие, ну, векторные различные техники разложения векторов вот, еще есть такая ну, просто скажу, SVD да, singular value decomposition тоже одна методика используется для RIC-системы она тоже под капотом используется PCI расчет агент векторов агент валес в целом, это вот это все по этим хорошо окей, понял так, смотрите, что еще у нас здесь закончили, здесь закончили ага так так да, не, сейчас так сейчас кое-что поставлю, проверю я хотел просто показать вам этот VS Code тут, это практику ага, может какие-то вопросы пока я настраиваю здесь среду? просто не конец, может по домашке или по чему-либо просто не конец, может по домашке или по чему-либо просто нет, хорошо скажите, пожалуйста, пример в семье, да? да, да, да окей, смотрите, тогда спасибо так, смотрите в целом, что здесь все происходит ну, на этом примере просто хотел показать вообще сильно слышно, да? ремонт давайте постараться сильно слышно ремонт трейдера тут дурацкую? да, нет, не слышно не слышно, да? просто очень сильный ремонт а сейчас, нет? нет, вообще не слышно а, ну, значит здесь подавление какое-то есть смотрите все просто, все так же уже умные ребята за нас подумали они уже сделали очень быстрые имплементации с мотой Adasin наших образных техник SQAP-победителька IAMLearn ну, просто качаете пакет, это типа как SQLLearn, да, то же самое ImbalancedLearn такая штука, которую мы сэмплируем там, на самом деле, очень много их методика разных я просто сегодня только две охватил ну, чтобы сильно не услаждать курс все то же самое, SQLLearn, там подобное смотрите, здесь пока в этой сети есть синтетический датасет пока что вообще такое как это работает все просто создается CreateImbalancedData такие-то количества параметров, две фичи кластера вы не смотрите, это просто кластер количество фичей, ой, не фичей, признаков ой, сэмплов с тысячу и весы веса то есть я говорю, что дай вес 0.9, то есть 90% веса дай вес в пользу 0 класса, то есть в основном говоря 0.9 примерно будет 0 классов 0.1 будет, 10% будет 100 классов будет только единичного класса вот, Classification, это из SQLLearn тоже есть в самом деле, просто датасет далее мы плотим без определения плотим вот такое распиление вот вот вот ничего такого просто пополняющая функция далее смотрите здесь есть построение, то создадем потом идентифицируем наш смот, смоты ADZIN как классы для того, чтобы реимплементировать это санасмогливо получилось далее мы обучаем фитим наши 2 объекта полученных путем того, чтобы скором X и Y наши фичи и лейблы соответственно получаем разные вариации для смоты и для ADZIN соответственно ну и просто плотим 3 распиления оригинальная, нитронная плотим распиление после смоты то есть при добавлении смоты оверсэмплинга и после добавления ADZIN вот все просто здесь просто размеры для нас это и в целом все об этом сейчас вернемся в целом в целом в целом видим, что вот наш оригинальный датасет здесь просто какие-то фичи не пентфичи, просто какие-то фичи и видно, что она строилась на какой-то линейной игре просто 2 какие-то переччения от двух линий простейшая ката механика ну пометили до класса полти 0 класс то есть его majority это класс 90% соответственно и это синий класс, оранжевый это перекласс, его мало так это выглядит в общем а далее смотрите если мы посмотрим, то есть после смоты ну мы условно говоря выправили до половины до класса но 5 над 5 то есть и тех 50% в общей выбраке и тех 50% и мы видим, что смоту по своему рисунку он просто наивным образом провел интерполяцию всегда рядом с собой просто на каждой вот пример нашел его ка средний, если не ошибаюсь ка дефолтный дефолтный 5, видите? каждый точек в классе 1 то есть minority класс выбрал 5 ближайших сделал 1 сюда от них от оригинального и к пяти разным с соседем умножил на 0.1 рандомно и добавил к оригинальному вектору соответственно мы видим, что вот такое распределение вышло то есть у нас вот здесь запустившие точки вот эта точка была она здесь преобразила какие-то еще точки рядом с собой ну и в целом почему такое, да, что мы просто увеличим плотность этих данных не знаю, хорошо или плохо это сложно сказать далее смотрите, здесь стоит еще Adazine здесь мы видим, что теперь Adazine работает адаптивно, то есть он добавляет только точки там где он читает по нашему правилу, да, в сложных случаях только там где он видит по его мнению более difficult cases то есть кейсы, где очень рядом стоят там с с majority класса, да, рядом то он и больше-то болеет если вы видео смотрите, смотрите вот наш оригинальный датасет не что большинства было здесь minority класса соответственно в Adazine, видите большинство не, не большинство, а большая плотность меньшего класса здесь, она в принципе не сильно изменилась то есть их вот так и осталось мало но при этом от света этого ничего не изменилось при этом, посмотрите, посмотрите теперь внимательно здесь на оранжевой точке minority класса где наличие точек гораздо менее плотное, то есть вот здесь вот здесь, вот здесь, вот здесь, вот здесь видите, да, что он очень много ну, все как Риво, честно говоря наделал точек дополнительных именно в этих регионах, более сложных регионах здесь, вот здесь, ну, нагенериал очень много понимаем, что ну, вот так вот создалось ну, честно, объективно, видим, да, что это фигня полная в данном случае в этом примере Adazine очень фигово поступил то есть он сделал вообще он там взял нам, выделил кластер по сути, ровно половине не наплыли, но прямо на синей линии то есть с синей область здесь он просто взял туда второй клад добавил потому что, ну, логика, видите, не с ним хорошая не хорошо устроена здесь а наоборот смотря, в случае, он показал немного получше, потому что он все выстроил тренд здесь потому что природа данных такая была ну, соответственно, я думаю, если бы обучаться будем ну на вот таких данных скорее всего, чем на таких ну, равенство будет гораздо лучше вот ну, то есть все просто, на самом деле то есть вот это я просто его не доделал, здесь все хорошо пока он не смотрит сюда все просто, на самом деле все исповедется то есть у нас готовые пакеты есть мы применим, ну, плотно, смотрим визуально, как это работает а у меня вопрос получается, изначально оба эти метода... кто был? это я, я, Алан Алан, ага получается, изначально оба эти метода работают с числовыми данными, да? хорошо будут работать только с числовыми данными даже, ну, я имею ввиду с кодированиями, да? если их кодировать они с ними будут работать вообще, как бы? или, ну, как бы, не очень будут? то есть, имеете ввиду, что что будет, если на вход заходит категоральный признак? да, кодированный уже, как бы после инкодинга так скажем на самом деле, да когда речь заходит... хорош вопрос Алан когда речь заходит уже про какие-то категоральные признаки то это не исподится то есть там есть другие техники того, как кодировать уже категоральные признаки обычно, как бы, смотрите, обычно видите ну, сейчас покажу, да, вот, удобнее еще было смотрите, хорош вопрос, Савердер, очень хороший так, я выберу сейчас какой-нибудь о, видно, хорошо смотрите, вообще если посмотреть, да что такое категоральный признак? не знаю, абстрактная какая-то ось просто подыграть, чтобы мы как-то видим эту ось смотрите, категоральный признак это, как бы, не очень это просто какая-то величина категоральные признаки это, ну, они выглядят, если их как-то визуализировать на графике, их можно визуализировать таким образом, да, это будет просто вот точка просто один, да дальше как бы, ну, вот, вот, вот, вот, вот вот, вот, вот, вот, вот дальше скажем, еще одна точка, да один один один и так далее сколько у вас категорий есть, столько таких будет точек ну, дискретные точки, по сути соответственно, мы понимаем, да, что, допустим, это нольевой класс, да, ноль ну, окей, чтоб как-то наглядно было давать предположение, что это нольевой класс хотя оно как-будто здесь должно быть, но ладно то есть, нольевой класс, первый класс, второй класс третий класс и так далее, до нольевой класс смотрите, здесь здесь мы видим, что мы здесь видим следующее, что, когда мы кодируем какую-то категорию, да, там, кошки, собаки да, там, не знаю, люди, не люди деревья, не деревья, не знаю то мы, ну, мы, ну, обычно мы, да мы должны кодировать, то есть мы должны привести вид в цифровой вид который машина могла понять как сошиться, вообще, как с ними работать соответственно, когда у нас категорийные параметры то есть кошка, собака мы должны перевести тоже в цифры и обычно, ну, ну, обычно да, там, one-hot encoding но one-hot encoding мне не нравится тем самым, что она расширяет, да, ваш размеры с таблическим пространством то есть, в общем говоря, если у вас one-hot encoding на 5 категорий, то при кодировании one-hot encoding у вас вырастят таблицы в колонках на 5 колонок на 4 колонки соответственно, если у вас будет 200 тысяч примеров сэмплов, то получается вы умножаете 200 тысяч на 4, это будет 800 тысяч а если вы используете там какой-то float32A размерность для ячейки, то у вас получается очень много, много количества памяти даже сжирается за счет one-hot encoding потому я обычно использую label encoder где именно в таком виде это происходит 0, 1, 2, 3, 4, 5 и до N-о категории, то есть у вас туда одна колонка будет всего где каждая колонка 1 это значит 2 класс 0 это 1 класс смотрите, здесь вся особенность такая, что для машины видите, в машине легче увидеть эту закономенцию, что они между собой все равны но вы согласитесь, что наличие если у вас какой-то пример с этим либо с этим классом или с этим то они в своей сути они для них понимают, что это сбалансированный, равноудаленный числовое значение это понятно здесь или не всем понятно, что я имею ввиду? да, да, понятно ну смотрите, я имею ввиду допустим, смотрите какой-то sample зашел, да, 0 класса sample 1 класс зашел и sample 2 класс зашел по сути, для машины чтобы какие-то паттерны увидеть в ней для нее будет она будет видеть следующим видом для нее 0 класс вернее для нее 1 класс вот этот класс по своему категориальному признаку да, признак 1 какой-нибудь он будет в равной мере одинаково отличим от этого примера да и также от вот этого потому что они с ним стоят равно удаленно ну как вы видите здесь нет такого значения, знаете как то есть обычно поэтому мы когда там кодируем на какие-то там в нейросетях когда кодируем признаки мы обычно всегда кодируем их именно этими, да whole numbers, да, целыми числами то есть intами, да то есть без плавающей точки то есть вы не можете закодировать признак 1.3, да это будет неправильно потому что будет 1.3 пример будет другой, например, 2 а другой будет 3, да соответственно 1.3 и 2 будут с другом ближе находиться, чем 2.3 что неправильно на самом деле а мы всегда должны какого-то целочисленное, да, чтобы машина не перепутала ничего, просто бывает такое например, у вас есть какие-то категории какой-то индификатор числовой 865, да это просто, не знаю индификатор ID какой-нибудь это примеры, первый пример есть второй пример, да, у него уже 971 да, пример третий пример 1001 как бы кто-то возможно скажет в этом числовом виде в принципе, ну ладно, пускай он также заходит в нашу модель это числа они же уже в числовой плоскости находятся но на самом деле это не так, потому что кто может быть ответит здесь? кто может догадаться почему это не так? с точки зрения механики все работает это хоть и категория какая-то какой-то категорийный признак это стринги это какие-то стринги но в целом если мы их в целом чисто технически это работает это legit машина проглотит их окей, это классно числа заходят, все нормально, я могу работать с ними вопрос пишет а как, простите, вопрос прозвучал как этот? вопрос, смотрите у нас есть вот здесь, смотрите какие-то данные есть категориальные категории 2 не знаю, какие-то категории это какие-то там признак 865 у него 971 это не число это просто какой-то категоральный признак маркировка групповая маркировка групп Mark, это ваша пятая группа такая группа, такая группа в целом, в точке зрения математики я говорю если переводить ее дальше это ошибка будет, потому что ответсведающая будет но кто-то может сказать, что почему чисто технически это работает правильно потому что это numerical, это numerical, это numerical какие проблемы могут быть машины их так же можно обработать вот такой вопрос был Акжан, не совсем про скейлинг очень рядом на санде скейлинг здесь тоже присутствует, но не совсем я думаю он будет читать те, что больше по значению он будет думать, что у них есть порядок он будет из-за этого неправильно их оценивать по порядку хотя порядка, по сути, там нет отлично да, это по сути ответ, который я ожидал услышать все верно если вы там ассортируете по этому значению эти сопремеры то у вас выстрет какая-то закономерность то есть она помешает вы закладываете в величину то есть у больших значений больше сигнал у меньших меньше сигнал соответственно, вы по этому должны перевести, закодировать эти категории в интегры обычно считаются с нуля то есть label encode их это в ноль превратить, это в один, это в два и так далее тем самым мы таки оран в машине скажем подожди здесь никакого порядка как такового нет потому что у нас, когда это энкодинг на все другие тогда у нас будет много-много нулей одерок, одерок, одерок и вот этот эффект кодировки он между ними выстроит такие закономерности что вот эта с этим она будет одинаково равноводолена и эта с этим вот так вот она поймет мы даем ей правильно понять это и есть наш категорийный признак это категория просто и все вот вы говорили про label encoding, если я не ошибаюсь он же это же энкодинг для применения когда у категории есть какой-то определенный порядок же, да? типа грубо говоря оценки там 0, 1, 2, 3, 4, 5 там где 5 это хорошая оценка 0 это плохая оценка а если нам не важно что там порядок но мы не хотим применять one-hot-encoding так как это занимает добавляет лишние колонки грубо говоря какой энкодинг мы можем применить тогда в таком случае? так если я правильно вопрос понял сейчас это у нас был алмазбек, да? да да да так если я вопрос правильно понял, вопрос следует в следующем типа ну вы говорите что в случае там когда у нас в случае порядок важен мы берем label encoder и это это же это же не ваше мы берем label encoder это это не так когда у нас порядок очень важен это это не то чтобы это не то чтобы задача классификации это как бы как бы вам сказать это больше прорегистия на самом деле когда у нас есть задача классифицировать но при этом учесть какой-то порядок в основном говоря оценки 1 до 3,4,5 это в принципе классы какие-то первые классы на колке учатся на 2,3,4,5 на 4,5,5 это как бы классы но при этом вы сами говорите что на данный случай задача помимо классификации она еще увлекает в себе какой-то порядок все зависит от того как вы задачу поставите то есть я бы частно говоря задачу поставил так я бы сказал что здесь нет порядка это просто какие-то категории 0,1,2,3,4,5 она сама уже выучит закономерность классификации в ней вот то есть label-encoder ответ он не про то что там учитывает порядок это не про это когда у вас просто есть какие-то категории, признаки категории, классы, я не знаю что-либо еще ну где получается ваши признаки должны лежать в такой как здесь нарисована в плоскости что они как бы имеют одинаковую силу одерка, сигнал в вино количества это все про label-encoder например one-hot-encoding тоже тип кодировки но там получается мы просто логика такая что если у вас 6 категорий то вы создаете признак из cut1 из фича 1 из фича 2, из фича 3 это булева логика то есть если одерка значит да это она, но это не она то есть как бы такая же логика и label-encoder просто label-encoder мне нравится он изящнее обходится с кодированием то есть он не плодит в оси, а просто не д利стает информацию в увеличение сигнала вот не знаю, если ответил правильный вопрос все покажите правильный и вы их поняли то есть label-encoding можно использовать и так и так по сути как если порядок важен или как или просто скорее так когда у вас Это есть. Нужно просто категории расставить правильно. Обработали бы, классификацию обучить. Это про лабелинкодинг. Если вы говорите про порядок, скорее всего это уже не совсем классификация, это что-то гибридное. То есть вы дополнительно к лабелинкодеру должны еще учесть. То есть смотрите, вы должны в вашей модели, в тех же оценок, вы должны учесть тот параметр, что это разные категории, это разные эти субъекты, но в целом они еще, говорите они еще там, 5 это больше чем 3, да, должно быть? Ну 5 это больше чем 4, да? Я просто сейчас не могу сходу наверное ответить, как это может быть сделать, но в целом я думаю, что там нужно учитывать какую-то дополнительную логику еще в вашем объяснении. И таки ордом вы зашьете, что да, у них еще есть между собой порядок. И я просто загуглил, есть еще ординал инкодинг, вот она, которую я имею в виду, где порядок важен. Ну да, там, на самом деле, порядок, да, ордил-кодинг, да, вы можете использовать, на самом деле, хороший вопрос, хороший ответ, на самом деле, он про это тоже может сделать. Но там еще много зависит от других параметров. Там, если я ошибаюсь, ордил-кодинг еще вроде бы есть понятие, веса, то есть вы должны еще вес давать, сколько там количество в примерах есть. В целом, да, ординал-кодинг хороший ответ, по идее, можешь его пробовать. Потом могу я задать пока этот, не ходя от кассы, так сказать, вопросу проекта, ну, ДЗшки. Там есть момент такой с grid search, типа мы должны выставить grid search, там определенные параметры, а можно или использовать вместо grid search ту же библиотеку как об тюна? Я в целом не вижу проблем, потому что я, в принципе, и то и это пользовался, просто, наверное, я указал grid search, я только grid search указал, я не указал, да, обтуда. Нет, вы только grid search указали. В целом, как бы я основных проблем не вижу, просто я помню, что grid search куда чаще, лучше, успешнее устанавливался у большинства людей, чем там об тюна бывало, требовал какие-то еще, ну, зависимости дополнительные, поэтому я это выбрал. Ну, конечно, можете, можете, да, ее использовать, без разницы, а на самом деле, главное найти параметры хорошие и все. Нет, ну, для этого, для поиска этих параметров, как бы grid search, я знаю, что он намного лучше подходит, просто он очень сильно, в зависимости от датасета, он сильно перегружает систему, у меня комп не такой вот, как бы, мощный, поэтому я стараюсь использовать с обтюна, так сказать, потому что когда идут какие-то поиски гиперпараметров с помощью grid search, он, не знаю, до нескольких часов может все застрять, короче. А вы не пробовали, там, ну, вроде бы, вот это количество параметров, которые конфигурация гиперпараметров, вы же вроде сами вводите, то есть вы там можете, ну, условно говоря, я не знаю, какую имплементацию grid search вы используете, но в целом там, вроде бы, которые есть, там вроде вводите какой-то finite, конечно, количество параметров и она просто по ним бегает, то есть там, и может быть выкидать защаренные методики использования, где какой-то более сложный подбор параметров, я, условно говоря, что я делаю, я имею ввиду там learning rate, список, да, list, 0.1, 1.5, 1.1, три типа, три опции, и он как бы три вариации проверяет, с другими вариациями, комбинациях. Может быть у вас какая-то там более сложная функция подбора, где она там не три, а сотню таких learning rates смотрит, может поэтому ходит в долгие фонды. Я бы не сказал бы, у меня вот примерно, как вы и сказали, такое же выставление, извиняюсь. Ну, значит, может быть, да, с этим с компом, может быть, ну, в целом, я посмотрел датасет, это, на самом деле, не такой большой, я тоже видел там, кто писал, что проблемы есть при запуске на ноутбуке, вот, и я думал, не большой. Хорошо, попробуем. Ну, посмотри, ну, ответ на вопрос по оптюне, окей, используется и вообще без разницы, на самом деле. Главное просто, ну, скажите, что вы используете оптюна, потому что ток-ток-ток, и все. Ну, просто оптюна, я знаю, что он не подбирает самые лучшие гиперпараметры, я знаю, что он подбирает там оптимальные гиперпараметры, то есть у него нет, а вот именно для поиска наилучшей вариации гиперпараметров, вот, критерс самая классный вариант, по идее. Просто, как я уже говорил, проблемы с компом. Окей, ну, окей, просто попробуйте, ваша лучшая, это TrioBest, и все, мне кажется, там нету, на самом деле, мотивов, лучше всех набрать, там нужно просто показать, что вы владеете доставчиком знаниями, по этому топику и все. Хорошо. Вот, да. Мы, кажется, отошли очень-очень далеко, тут вопрос был, да, кажется, от чего мы нашли. Кто спрашивал вопрос про вот этот, про усэмплик на категории данных? Это тоже началось? Я спрашивал, я спрашивал. А, ладно, да-да-да, вот, мы вообще ошли от этого вопроса. В целом, да, здесь другие методики нужны, то есть вот это, я, в смысле, я могу не знать, может быть, есть какое-то смотое там for категорикал параметр, или какие-то еще там приблуда сверху на эту штучку, но в целом, я думаю, там другие методики того, как вы должны наращивать. Скорее всего, вы там будете наращивать по классу abundance, да, то есть смотреть, ну, если у вас, там условно говоря, какой-то категории, какой-то категории признака есть, у вас, там условно говоря, дисбалансы в них есть, то вы должны смотреть, вообще какие категории существуют. Допустим, если у вас, там, категории 1 есть, и там 95% по этой категории вот здесь лежат, а остальные, там, 5% делиться на вот эти, да, эти два типа категории, и они там в маленьком виде лежат. Я думаю, там какой-то, возможно, ну, какой-то идет про... Я могу поделиться тоже, sorry, я вот тут. Короче, да, есть некоторые переменные, которые коррелируют, не то что даже коррелируют, именно категориальные, которые сильно влияют. Например, да, сразу скажу, там такой момент, что люди, которые, по-моему, студенты и те, которые ретают, у них больше единички более часто встречаются. Ну, и я, как планировал сделать, я еще не тестил, как бы, в рамках pipeline, когда датасет приходит создавать колонку типа из ретает и из студент, прям отдельно. Понимаете, да? То есть именно в этих двух, то есть эти два назначения, они действительно влияют, ну, модели на то, что там 0 или 1. Вот, и как-то чтобы не прям, не все ванн ход инкодить, а чисто только те слова, которые я ищу, или те группы, которые я ищу. То есть я сначала посмотрел, что там обычно split идет, обычно split идет 90-10, да, ну, как обычно. Ну, а, например, типа у студентов идет 80 на 20, у ретаерта тоже 80 на 20. Вот, и я как раз отдельно создаю колонку, что вот когда на инференции, да, уже типа человек пришел, я смотрю его статус, и как бы переношу в колонку типа, является ли он студентом или является ли он этим ретаертом. Вот, вот так я обрабатываю. Потому что, ну, типа основная часть данных категоральных, она не особо влияет. Ну, из того, что я смотрел, типа там распределение класса везде одинаково, 90 на 10, кроме вот этих вот некоторых значений. Окей, да, интересно. Спасибо, Ломот. Надо посмотреть. Ну, скорее всего, как бы отбора там, вы можете заниматься вот этой экстраполяцией, да, то есть наращивать это сетто путем, там, ну, так сказать, заполнением, да, как обычно вы же не питете данные, да, когда у вас отсутствует данные, да, категориальные. Также, на самом деле, можете, я видел много техник, когда вот, дизбал существует, вы наращиваете там классы, minor классы, например, minor классы в категориях наполняется просто с наиболее частыми, ну, признаками. Ну, моду берете по признаку, и там, условно, нулевой класс у вас minority, в категории, или категории там 1, категория 1, вы добавляете там чаще встречающуюся там категорию этого признака. Ну, мне кажется, можно по критикам проявить, можно сделать. Ну, вот да, типа, как раз то, что вот так я попробовал сделать. Да, ну, с категориальными, да, там была как раз такая проблема, что непонятно было, как с ними работать. На самом деле, да, я опять повторюсь, в начале сказал, честно, очень редко, когда с SMP хорошо работает. Я знаю, что есть более сложные методики, как, допустим, я видел, ребята делали через ганы, то есть они обучали ган сети, она для табличек ганных, то есть обычный ган используется для каких-то более слотов еще, но они, я видел, там был кейс был, они создавали фродовые кейсы, а, короче, какой-то был фродовый кейс, очень редкий, типа, там даже среди фрода, фроды сами себе редкие случаи в данных, но этот кейс фрода был еще еще реже у других, но он был очень критично важным, то есть в зрение потери денег, потому что, ну, там была очень такая большая репутационная риска, что было. Ответственно, ну, там после каких-то там анализов, это финтех, это вещь была, после анализов они там через ганы на нанагенерили кучу признаков, которые реально по природе своей хорошо описали реальную природу таких фродов. Ну, получается, сделали классификатор на базе этих данных, и в целом они там неплохо закрыли эту проблему, то есть, то есть, можно моделями решать задачу оверсептлинга. Ну, ганы я видел, такое точно было. Кажется, это был какой-то в Тинков, что ли, что-то ребята делали. Вот. Вот. Так, ну, в целом давайте будем дальше. Смотрите, вот наш смотэ, да, и уже в разрезе наш домашнее создание, да, то есть, я взял смотэ. Да, давай как-то мне там, наделай, да, там, новых данных. По сути, что он сделал, да. Вот это оригинальное распределение, да, вот 90 на 10. Далее мы выровняли. Ну, и по сути, здесь было классифик... здесь есть классифик, конечно, репорт по модели следующей. Я брал random classifier, random forest classifier. Получается один на оригинальном, другой на, по сути, там, агументированном. Ну, и видим, да, что в целом ухудшение есть. Ну, вот мы видим, что для позитивных классов, в основном говоря, точность была реклуп такая, теперь у нас снижение по точности. То есть, мы, в основном говоря, навалидации испортили наши метрики. Ну, для того, что он как-то загенеризировал какие-то там, условно, рисунок свой сделал свой и тем самым, ну, лучше сделал. То есть, ну, это пример того, что техника не всегда работает. То есть, мы без нее бы лучше справились. Ну, это интересный кейс. Да, на самом деле, это хороший кейс. Потому что не всегда это работает правильно. Вот. Дальше еще, здесь еще был пример такой. Так, это у нас что было? Сейчас, Валечка здесь что-то недринное писал. Да, это все. А, ну, это просто я делал другую вариацию. Слушайте, у нас здесь тоже все получается. Другая модель, также обучалась на original и здесь ну вот가. Не знаю, где вот эту snapper здесь僚аades. Да, тебя мучили мы с этими работниками. другая модель также обучалась на оригинальных, так же обучалась на смотах. Тоже видим деградацию в изменениях. По однёрке мы смогли вырасти, не смогли вырасти в притяжении, по реколам мы смогли вырасти. То есть у нас получается полнота, то есть получается то как мы находим настоящие однёрки из пулы всего датасета, у нас вот этот охват однёрок вырос. Пресижение не вырос. Вот такие вкейсы бывают. То есть смотреть на данные аниметотики нужно очень грамотно подходить к тому, с чем мы работаем. По бону большую уклон делать на EDA, вы должны смотреть на графики, вы должны смотреть на статистику, вы должны смотреть вообще как это всё визуализируется и как это вообще, ну правильно ли вообще вы генерируете данные для того, чтобы это всё увеличить. Вот это кажется да, да это как раз я под bankdate под ваш датас отдел. Так сейчас секунду я проверю кое-что. Здесь был ADA SYN был ещё. Да, здесь тоже самое ADA SYN, только для ADA SYN немного поинтереснее в своем исполнении. Я решил, как говорится, сделать имплементацию. Можете посмотреть дома, здесь класс ADA SYN. Того как он по сути работает. Ну это я где-то тоже увидел. Так будет изящно красиво, я бы не смог рисовать. В целом здесь просто шаги реализации до самого лайки. То есть смотрите, мы здесь, ну, просто создаем из листов в X, в numpyer переводим. Далее мы находим просто количество классов, да распределение классов в minor и в major. Количество считаем. Далее если параметр g, ну это по сути просто наш параметр, насколько она увеличит minority класс, то minority класс. Вот и находим индексы minor класс. Мы здесь в этой чеке находим, храним все индексы наших minor классов из SATAX. Всё minority переводим, находим индексы, поставляем x minority находим. Далее до keep nearest neighbor, то есть нас пять, ищем количество получается, ищем всех людей и ищем ближайших людей и соседей для наших этих minority классов. Питимся и всё. И далее говорим, для нашего каждого класса, для каждой точки в minority находи ближайших индексы, вытаскивай лейблы этих индексов, чтобы проверить, насколько это, помните наш правило difficulty score, то есть мы должны только самые сложные примеры найти. И соответственно мы смотрим, здесь сравним, суммировать, то есть суммировать все тех, которые, в основном говоря, вернее суммировать те кейсы, которых они не равны minority кейсам. То есть в основном говоря мы суммируем количество подсчет только тех, когда мы равны majority класс. Когда у нас вот этот neighbor label равно 1, а это 0, допустим, в случае, что minority это 0, то мы это добавляем сюда, в ratio наш. И тем самым, для каждой точки из minority ищем самых ближайших точек, для каждой точки мы ищем только те точки, которые в neighborhood имеют minority класс, где это не равно, и соответственно суммируется больше и больше, ну и делится. Соответственно как бы добавляем это ratio и потом по нему, ну нормализацию проводим, это просто чтобы в scale был один тот же, и накладываем только те кейсы, то есть берем только те кейсы, которых вот этот ratio высокий был. Но это можете изучить, в принципе, это несложно. Вот. А здесь уже просто генерирование происходит, мы просто генерируем данные. То есть как я говорил, да вот видите мы генерируем данные, рандом наберем nearest neighbor, из тех пити рандом наберем, делаем получается, просто поставляем индексы и выбираем альфу, а то ли это одного, обычно это равенцы, рандома числа, и умножаем, помните, прибавляем оригинальный вектор, прибавляем альфа, то есть наш random number на разницу neighbor и x. Вот по сути вот синтетический sample. Вот он есть. То есть вся по сути линия кода, вся суть на этой линии кода. Вот. Ну и все. Так, в этом случае я здесь так же строил, что я строил здесь. Извините, можно уточнить, сегодня урок будет до 10 или 9.30? По идее уже да, заканчиваем, но в целом да, я закончил уже минуты, наверное, и все. И можем просто перейти. Да, и здесь по сути просто видим, что я тоже то же самое сделал, обучаюсь, здесь кнн я поставил для пассификации, выровняю распределение, да, ну и видим, что конечно мы и смогли нарастить, но мы видим, что у нас конфигурация матрицы, да, здесь вот наш tp упал, но при этом наш tn чуть вырос. В целом как бы ну небольшой оверсэммер к нам в принципе ничего не дал, в метриках мы как будто бы удар проиграли, да, в президии, в покрытии выиграли конечно, но потому что мы сделали bias, отсюда мы сместили суда, на равные шансы. Сейчас у нас как бы модель может определять более корректно наше покрытие. Ну все, я скину этот датасет, ой, я скину ноутбук, лекцию, материал. Давайте перейдем к вопросам, кажется, вопросы были, да, и закончим потом уже. Ну, вопросы были, может быть? Может быть не пэддей, может быть предыдущие вопросы и предыдущие материалы. У меня вроде нет вопросов. Может быть предыдущий фидбэк какой-то в целом по как вообще идет сейчас, насколько ну сложно, несложно, потом как вообще по нагрузке. Я понимаю, что здесь в принципе контингент разно собрался, учу студентов и студенты прям, которые учатся и которые работают люди, у всех разной как бы ну ответственности. Ответственно хотел понимать насколько идем, ну по успеваемости нормально успевают ли все или какой-то есть лаг, или в несколько дней по отставанию материала. Ну или общий фидбэк, я не знаю, какие-то мысли может быть. Я за всех не могу сказать, лишь честно за себя скажу, что все... Да, конечно. У меня все отлично. Восприятие материала, все понятно, все предельно ясно, вот как бы, математику сам добиваю, но это уже чисто так лично, а так все отлично для меня. Спасибо. Спасибо, Лазик. Ну у меня тоже, мне нравится, что мы более углубленно проходим концепты и у нас математические частью и конечно тоже иногда надо самому добивать, читать, но в принципе это очень классно и полезно, я считаю. Вот и у меня все отлично тоже. Спасибо, Кэн Сэрда, спасибо. Да, потому что как бы это, видите, чтобы максимально, ну это как это обучение не растит, это тоже. То, что вы сейчас говорите, вы выдаете мне какое-то направление, куда мне двигаться, да, тоже как учителя. Т.е. такое back propagation функция потери считается и я должен оптимизировать свою работу, чтобы выстроить более правильные вектор развития. Вот, поэтому это очень важно на самом деле, делиться, собрать связь. Вот. Если не ошибаюсь, кажется это, так, по-моему это вот уже последняя, кажется, лекция. Да, да последняя. Да, даже deployment, column, handle, перехватывают. Да, да. Окей, ну да там бредеплеем, продеф. А, ну в целом моды заканчиваются, значит, уже скоро. Окей, потом вы пойдете на уже NLP, да? Окей. Хорошо, NLP тогда NLP, да. А, ну получается мы сфотографируемся с вами и в NLP, да? Уже все NLP. А потом какой модуль вы будете? Да, получается 16 по 18 недель будут классификация объектов, детекция, сегментация изображений. Vision Transformer будут. GAN я буду объяснять, Transfer Stylies, Image to Text, Text to Image, возможно, не знаю, если успеем. И FIE, DTPM, Variation Autoencoders и Diffusion модели. Ну то, что по сути мы сегодня обсуждали по генерации сэмплирования, это по сути мы затронем это. Вот, в целом. Да, ну по CV, наверное, будет прям плотно, скорее всего. Очень интересная тема здесь. А, да. А, кстати, вопросы по PCA были какие-то, может быть? Оген Валис, Оген Виктора. Я, как раз, постарался максимально жатко объяснить, может кто-то мысли были. А, Агзан пишет, what to expect for the upcoming sub-model project? Ну, вообще, это шестая неделя, Project Week, сколько мы тогда ждали, с всеми тренерами, с учетом предыдущих годов, опытов, других курсов. Полчаса там будет неделька. Там, скорее всего, будем больше обсуждать с нами, с TEA, с ребятами и другими, но не знаю кто там еще помогает. Ну, там прям будет получиться работа над вашими кейсами. Там, кажется, сейчас в пул фомируются кейсы, вроде бы. Там, кажется, будет возможность выбрать самому свой кейс, либо выбрать еще кейс, который сами дадут еще. Ну, мы предоставим. И вот на этом Project Week вы, по сути, будете делать. Ну, в принципе, если в общем говорить, насколько я помню, там идет типа под модуль, вернее модуль, и проект типа по этой части. То есть там не будет такой, что там какая-то нереселька расплывет в этом модуле. Вот. А сейчас ожидайте что-то с того, что мы примерно проходили. А по поводу кейса каждому индивидуально будут или будут разделены? Вопрос решается, кажется. Там оба варианта вроде бы есть, если не три даже варианта. То есть individual based вроде бы возможен. Кажется и group based тоже возможен. Вот. Третий вариант, что-то еще как-то мы что-то говорили. Ну, это вроде бы, не знаю, кажется, я не уверен, что я сейчас знаю полностью конечный статус. Скорее всего, это уже координатор курса, IEM. Вот, у нас, скорее всего, здесь предоставят. Хорошо, спасибо. Вот. Так, ну что, в целом, вопросов есть нет? Тогда, если вопросов нет, тогда, может быть, загругляться. Так, чат пустой, это Фринк, что-нибудь пишет. Вопросов нет. Хорошо. Спасибо. До свидания. До свидания. До свидания. До свидания. До свидания. До свидания. До свидания. Всего доброго. До свидания. Спасибо большое. До свидания. До свидания. Благодарю. Благодарю. Благодарю. Спасибо. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода. С вами был Игорь Негода.