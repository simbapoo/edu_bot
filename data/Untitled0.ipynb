{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1PnY6Wx6q_vmglrTGpLnRX-ORbGACt2mt","authorship_tag":"ABX9TyPLzrp4vXdv9lkeatFKVONT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e0dbd88c8b2540dcaaa39c517c67edb8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ae15161a719474fa694170482a867ee","IPY_MODEL_8bddae61a3264039aa257fcc02657b79","IPY_MODEL_d846685c084048a4ab57cb356ad2fc2b"],"layout":"IPY_MODEL_21ab61585c104cf79552f2b5c7085cd1"}},"5ae15161a719474fa694170482a867ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d4bafbf63041c0ba15f4948323d590","placeholder":"​","style":"IPY_MODEL_183878815309402b8fadb18acbbc3736","value":"100%"}},"8bddae61a3264039aa257fcc02657b79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f584ad85344435d8b516460336eaff3","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_033cbc71884546efbf0e53637c13df41","value":15}},"d846685c084048a4ab57cb356ad2fc2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e6f3f781ae8443a9fd3595ab18b0f23","placeholder":"​","style":"IPY_MODEL_bc0ee304bcc0438c8ec67d099cbe412b","value":" 15/15 [00:05&lt;00:00,  3.11it/s]"}},"21ab61585c104cf79552f2b5c7085cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d4bafbf63041c0ba15f4948323d590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"183878815309402b8fadb18acbbc3736":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f584ad85344435d8b516460336eaff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"033cbc71884546efbf0e53637c13df41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e6f3f781ae8443a9fd3595ab18b0f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0ee304bcc0438c8ec67d099cbe412b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cCiyDqu8qy4U","executionInfo":{"status":"ok","timestamp":1737225142486,"user_tz":-360,"elapsed":309,"user":{"displayName":"Сымбат Лекерова","userId":"08390869076651441874"}}},"outputs":[],"source":["import os\n","from nbformat import read\n","from tqdm import tqdm_notebook\n","\n","\n","def load_notebook_texts(nb_dir):\n","    texts = []\n","    for root, dirs, files in os.walk(nb_dir):\n","        for file in tqdm_notebook(files):\n","            if file.endswith(\".ipynb\"):\n","                notebook_path = os.path.join(root, file)\n","                try:\n","                    # Открываем и читаем содержимое ноутбука\n","                    with open(notebook_path, 'r', encoding='utf-8') as f:\n","                        nb = read(f, as_version=4)  # Читаем ноутбук в версии 4\n","                        notebook_content = {\"source\": file, \"text\": \"\"}\n","                        # Проходим по ячейкам ноутбука\n","                        for cell in nb.cells:\n","                            if cell.cell_type == 'markdown':\n","                                # Добавляем содержимое ячейки markdown\n","                                notebook_content[\"text\"] += f\"**Markdown:** \\n{cell.source}\\n\\n\"\n","                            elif cell.cell_type == 'code':\n","                                # Добавляем содержимое ячейки кода\n","                                notebook_content[\"text\"] += f\"**Code:** \\n{cell.source}\\n\\n\"\n","                        texts.append(notebook_content)\n","                except Exception as e:\n","                    print(f\"Ошибка чтения файла {notebook_path}: {e}\")\n","                    continue  # Продолжаем обработку других файлов\n","\n","    return texts"]},{"cell_type":"code","source":["notebook_texts = load_notebook_texts(\"/content/drive/MyDrive/Colab Notebooks/notebooks\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["e0dbd88c8b2540dcaaa39c517c67edb8","5ae15161a719474fa694170482a867ee","8bddae61a3264039aa257fcc02657b79","d846685c084048a4ab57cb356ad2fc2b","21ab61585c104cf79552f2b5c7085cd1","86d4bafbf63041c0ba15f4948323d590","183878815309402b8fadb18acbbc3736","8f584ad85344435d8b516460336eaff3","033cbc71884546efbf0e53637c13df41","7e6f3f781ae8443a9fd3595ab18b0f23","bc0ee304bcc0438c8ec67d099cbe412b"]},"id":"ZoJZYPtfrSzV","executionInfo":{"status":"ok","timestamp":1737225150882,"user_tz":-360,"elapsed":5765,"user":{"displayName":"Сымбат Лекерова","userId":"08390869076651441874"}},"outputId":"8d1d17c3-02b3-442c-a04e-42698883c2db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-ee8ac8416a0a>:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for file in tqdm_notebook(files):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0dbd88c8b2540dcaaa39c517c67edb8"}},"metadata":{}}]},{"cell_type":"code","source":["notebook_texts[:2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSDYVu0JsAlw","executionInfo":{"status":"ok","timestamp":1737225202065,"user_tz":-360,"elapsed":323,"user":{"displayName":"Сымбат Лекерова","userId":"08390869076651441874"}},"outputId":"5e5e56d2-ccad-4633-93a7-5e2dcaa5b7bf"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'source': 'Untitled0.ipynb',\n","  'text': '**Code:** \\nimport os\\nfrom nbformat import read\\nfrom tqdm import tqdm_notebook\\n\\n\\ndef load_notebook_texts(nb_dir):\\n    texts = []\\n    for root, dirs, files in os.walk(nb_dir):\\n        for file in tqdm_notebook(files):\\n            if file.endswith(\".ipynb\"):\\n                notebook_path = os.path.join(root, file)\\n                try:\\n                    # Открываем и читаем содержимое ноутбука\\n                    with open(notebook_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n                        nb = read(f, as_version=4)  # Читаем ноутбук в версии 4\\n                        notebook_content = {\"source\": file, \"text\": \"\"}\\n                        # Проходим по ячейкам ноутбука\\n                        for cell in nb.cells:\\n                            if cell.cell_type == \\'markdown\\':\\n                                # Добавляем содержимое ячейки markdown\\n                                notebook_content[\"text\"] += f\"**Markdown:** \\\\n{cell.source}\\\\n\\\\n\"\\n                            elif cell.cell_type == \\'code\\':\\n                                # Добавляем содержимое ячейки кода\\n                                notebook_content[\"text\"] += f\"**Code:** \\\\n{cell.source}\\\\n\\\\n\"\\n                        texts.append(notebook_content)\\n                except Exception as e:\\n                    print(f\"Ошибка чтения файла {notebook_path}: {e}\")\\n                    continue  # Продолжаем обработку других файлов\\n\\n    return texts\\n\\n**Code:** \\nnotebook_texts = load_notebook_texts(\"/content/drive/MyDrive/Colab Notebooks/notebooks\")\\n\\n'},\n"," {'source': '5_1.ipynb',\n","  'text': '**Markdown:** \\n# Lecture 4, Classic ML, Data Engineering\\n\\n**Markdown:** \\n## Normality Tests\\n\\n**Markdown:** \\n### Shapiro-Milk Test\\n\\n**Code:** \\nimport numpy as np\\nfrom scipy import stats\\nimport matplotlib.pyplot as plt\\n\\ndef perform_shapiro_wilk_test(data, ax1, ax2):\\n    # Perform Shapiro-Wilk test\\n    statistic, p_value = stats.shapiro(data)\\n    \\n    print(f\"Shapiro-Wilk test results:\")\\n    print(f\"Statistic: {statistic}\")\\n    print(f\"p-value: {p_value}\")\\n    \\n    # Interpret the results\\n    alpha = 0.05\\n    if p_value > alpha:\\n        print(\"The sample looks Gaussian (fail to reject H0)\")\\n    else:\\n        print(\"The sample does not look Gaussian (reject H0)\")\\n\\n    # Create a Q-Q plot\\n    stats.probplot(data, dist=\"norm\", plot=ax1)\\n    ax1.set_title(\"Q-Q plot\")\\n\\n    # Create a histogram\\n    ax2.hist(data, bins=50, edgecolor=\\'k\\')\\n    ax2.set_title(\"Histogram\")\\n\\n# Example usage\\n# Generate some sample data\\nnp.random.seed(42)\\n\\nnormal_data = np.random.normal(loc=0, scale=1, size=100000)\\nnon_normal_data = np.random.exponential(scale=1, size=100000)\\n\\n# Create subplots\\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\\n\\nprint(\"Test with normally distributed data:\")\\nperform_shapiro_wilk_test(normal_data, axs[0, 0], axs[0, 1])\\n\\nprint(\"\\\\nTest with non-normally distributed data:\")\\nperform_shapiro_wilk_test(non_normal_data, axs[1, 0], axs[1, 1])\\n\\nplt.tight_layout()\\nplt.show()\\n\\n**Markdown:** \\n### Kolmogorov-Smirnov Test\\n\\n**Code:** \\nimport numpy as np\\nfrom scipy import stats\\nimport matplotlib.pyplot as plt\\n\\ndef perform_ks_test(data):\\n    # Calculate mean and standard deviation of the data\\n    mean = np.mean(data)\\n    std = np.std(data)\\n    \\n    # Perform Kolmogorov-Smirnov test\\n    statistic, p_value = stats.kstest(data, \\'norm\\', args=(mean, std))\\n    \\n    print(f\"Kolmogorov-Smirnov test results:\")\\n    print(f\"Statistic: {statistic}\")\\n    print(f\"p-value: {p_value}\")\\n    \\n    # Interpret the results\\n    alpha = 0.05\\n    if p_value > alpha:\\n        print(\"The sample looks Gaussian (fail to reject H0)\")\\n    else:\\n        print(\"The sample does not look Gaussian (reject H0)\")\\n\\n    # Create subplots\\n    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\\n    \\n    # Plot histogram with fitted normal distribution\\n    axs[0].hist(data, bins=\\'auto\\', density=True, alpha=0.7, color=\\'skyblue\\')\\n    xmin, xmax = axs[0].get_xlim()\\n    x = np.linspace(xmin, xmax, 100)\\n    p = stats.norm.pdf(x, mean, std)\\n    axs[0].plot(x, p, \\'k\\', linewidth=2)\\n    axs[0].set_title(\"Histogram with fitted normal distribution\")\\n    \\n    # Create a Q-Q plot\\n    stats.probplot(data, dist=\"norm\", plot=axs[1])\\n    axs[1].set_title(\"Q-Q plot\")\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n# Example usage\\n# Generate some sample data\\nnp.random.seed(42)\\nnormal_data = np.random.normal(loc=0, scale=1, size=1000000)\\nnon_normal_data = np.random.exponential(scale=1, size=10000)\\n\\nprint(\"Test with normally distributed data:\")\\nperform_ks_test(normal_data)\\n\\nprint(\"\\\\nTest with non-normally distributed data:\")\\nperform_ks_test(non_normal_data)\\n\\n**Markdown:** \\n### Correlation Analysis\\n\\n**Code:** \\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom scipy import stats\\n\\n# Создаем случайные данные\\nnp.random.seed(0)\\nsleep_hours = np.random.normal(7, 1, 100)\\nproductivity = 70 + 4 * sleep_hours + np.random.normal(0, 10, 100)\\n\\n# Создаем DataFrame\\ndf = pd.DataFrame({\\'Часы сна\\': sleep_hours, \\'Производительность\\': productivity})\\n\\n# Рассчитываем коэффициент корреляции Пирсона\\ncorrelation, p_value = stats.pearsonr(df[\\'Часы сна\\'], df[\\'Производительность\\'])\\n\\n# Создаем график рассеяния\\nplt.figure(figsize=(10, 6))\\nsns.scatterplot(x=\\'Часы сна\\', y=\\'Производительность\\', data=df)\\nplt.title(f\\'Корреляция между сном и производительностью\\\\nr = {correlation:.2f}, p = {p_value:.4f}\\')\\nplt.xlabel(\\'Часы сна\\')\\nplt.ylabel(\\'Производительность (%)\\')\\n\\n# Добавляем линию регрессии\\nsns.regplot(x=\\'Часы сна\\', y=\\'Производительность\\', data=df, scatter=False, color=\\'red\\')\\n\\nplt.show()\\n\\n# Выводим корреляционную матрицу\\ncorrelation_matrix = df.corr()\\nprint(\"Корреляционная матрица:\")\\nprint(correlation_matrix)\\n\\n# Визуализируем корреляционную матрицу\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\', vmin=-1, vmax=1, center=0)\\nplt.title(\\'Тепловая карта корреляции\\')\\nplt.show()\\n\\n# Интерпретация результатов\\nprint(f\"\\\\nКоэффициент корреляции Пирсона: {correlation:.2f}\")\\nprint(f\"P-значение: {p_value:.4f}\")\\n\\nif p_value < 0.05:\\n    print(\"Связь статистически значима (p < 0.05)\")\\nelse:\\n    print(\"Связь статистически не значима (p >= 0.05)\")\\n\\nif abs(correlation) < 0.3:\\n    strength = \"слабая\"\\nelif abs(correlation) < 0.7:\\n    strength = \"средняя\"\\nelse:\\n    strength = \"сильная\"\\n\\nprint(f\"Сила связи: {strength}\")\\n\\n**Code:** \\nstudy_hours = [1, 2, 3, 4, 5, 6, 7]\\ntest_scores = [65, 70, 80, 85, 85, 90, 95]\\n\\ncorrelation, p_value = stats.pearsonr(study_hours, test_scores)\\n\\nprint(f\"Correlation coefficient: {correlation:.2f}\")\\nprint(f\"P-value: {p_value:.4f}\")\\n\\n**Markdown:** \\n# Lecture 5, Classic ML, ML Intro & Supervised Learning\\n\\n**Markdown:** \\n## Lib import\\n\\n**Code:** \\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\\nfrom sklearn.metrics import mean_squared_error, accuracy_score\\nfrom sklearn.datasets import make_regression, make_classification\\n\\n**Markdown:** \\n## Linear Regression example, 1-D feature\\n\\n**Markdown:** \\nIn a simple linear regression model with one feature, the relationship between the feature \\\\( x \\\\) and the target variable \\\\( y \\\\) is modeled as a linear equation:\\n\\n$$\\ny = \\\\beta_0 + \\\\beta_1 x\\n$$\\n\\nWhere:\\n- \\\\( y \\\\) is the predicted value.\\n- \\\\( \\\\beta_0 \\\\) is the intercept of the regression line (also known as the bias term).\\n- \\\\( \\\\beta_1 \\\\) is the coefficient for the feature \\\\( x \\\\) (also known as the slope of the regression line).\\n- \\\\( x \\\\) is the input feature.\\n\\n**Code:** \\n# Generate sample data\\nX, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create and train the model\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model\\nmse = mean_squared_error(y_test, y_pred)\\nprint(f\"Mean Squared Error: {mse:.2f}\")\\n\\n# Visualize the results\\nplt.scatter(X_test, y_test, color=\\'blue\\', label=\\'Predicted\\')\\nplt.plot(X_test, y_pred, color=\\'red\\', label=\\'Predicted\\')\\nplt.xlabel(\\'X\\')\\nplt.ylabel(\\'y\\')\\nplt.title(\\'Linear Regression\\')\\nplt.legend()\\nplt.show()\\n\\n**Markdown:** \\n## Interpretation\\n\\n**Code:** \\nprint(f\\'Weights: {model.coef_}\\')\\nprint(f\\'Bias: {model.intercept_}\\')\\n\\n**Code:** \\nprint(f\\'Sample test input:\\\\n{X_test[:10].reshape(-1)}\\\\n\\')\\nprint(f\\'Predictions:\\\\n{model.predict(X_test[:10])}\\\\n\\')\\ny_pred_1 = X_test[:10]*model.coef_+model.intercept_\\nprint(f\\'Predictions, manually calculated:\\\\n{y_pred_1.reshape(-1)}\\')\\nprint(f\\'Sanity check: {np.allclose(model.predict(X_test[:10]), y_pred_1.reshape(-1))}\\')\\n\\n**Markdown:** \\n## Linear Regression, 2-D feature\\n\\n**Markdown:** \\nIn a linear regression model with two features, the relationship between the features \\\\( x_1 \\\\), \\\\( x_2 \\\\) and the target variable \\\\( y \\\\) is modeled as a linear equation:\\n\\n$$\\ny = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2\\n$$\\n\\nWhere:\\n- \\\\( y \\\\) is the predicted value.\\n- \\\\( \\\\beta_0 \\\\) is the intercept of the regression plane (also known as the bias term).\\n- \\\\( \\\\beta_1 \\\\) is the coefficient for the feature \\\\( x_1 \\\\).\\n- \\\\( \\\\beta_2 \\\\) is the coefficient for the feature \\\\( x_2 \\\\).\\n- \\\\( x_1 \\\\) and \\\\( x_2 \\\\) are the input features.\\n\\n**Code:** \\n# Generate sample data with 2 features\\nX, y = make_regression(n_samples=1000, n_features=2, noise=10, random_state=42)\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create and train the model\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model\\nmse = mean_squared_error(y_test, y_pred)\\nprint(f\"Mean Squared Error: {mse:.2f}\")\\n\\n# Visualize the results in 3D\\nfig = plt.figure(figsize=(18, 12))  # Adjust the figure size to be 2 times wider and higher\\nax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n# Scatter plot of the actual data points\\nax.scatter(X_test[:, 0], X_test[:, 1], y_test, color=\\'blue\\', label=\\'Actual\\')\\n\\n# Create a mesh grid for plotting the plane\\nx_surf, y_surf = np.meshgrid(np.linspace(X_test[:, 0].min(), X_test[:, 0].max(), 10),\\n                             np.linspace(X_test[:, 1].min(), X_test[:, 1].max(), 10))\\nz_surf = model.predict(np.c_[x_surf.ravel(), y_surf.ravel()]).reshape(x_surf.shape)\\n\\n# Plot the regression plane\\nax.plot_surface(x_surf, y_surf, z_surf, color=\\'red\\', alpha=0.5)\\n\\nax.set_xlabel(\\'Feature 1\\')\\nax.set_ylabel(\\'Feature 2\\')\\nax.set_zlabel(\\'Target\\')\\nax.set_title(\\'Linear Regression with 2D Features\\')\\nplt.legend()\\nplt.show()\\n\\n**Markdown:** \\n## MSE implementation\\n\\n**Markdown:** \\n$$\\n\\\\text{MSE} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - \\\\hat{y}_i)^2\\n$$\\n\\n**Code:** \\nimport numpy as np\\n\\ndef calculate_mse(y_true, y_pred):\\n    \"\"\"\\n    Вычисляет среднеквадратичную ошибку (MSE) между истинными и предсказанными значениями.\\n\\n    :param y_true: numpy массив истинных значений\\n    :param y_pred: numpy массив предсказанных значений\\n    :return: значение MSE\\n    \"\"\"\\n    mse = np.mean((y_true - y_pred) ** 2)\\n    return mse\\n\\n**Code:** \\nmse_manual = calculate_mse(y_test, y_pred)\\n\\nprint(f\\'MSE calculated manually: {mse_manual:.2f}\\')\\nprint(f\\'MSE calculated by the function: {mse:.2f}\\')\\n\\n**Markdown:** \\n## Logistic Regression, 2-D feature\\n\\n**Markdown:** \\nIn a logistic regression model with two features, the relationship between the features \\\\( x_1 \\\\), \\\\( x_2 \\\\) and the probability \\\\( p \\\\) of the target variable \\\\( y \\\\) being 1 is modeled using the logistic function:\\n\\n$$\\np = \\\\frac{1}{1 + e^{-(\\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2)}}\\n$$\\n\\nWhere:\\n- \\\\( p \\\\) is the probability of the target variable \\\\( y \\\\) being 1.\\n- \\\\( \\\\beta_0 \\\\) is the intercept (also known as the bias term).\\n- \\\\( \\\\beta_1 \\\\) is the coefficient for the feature \\\\( x_1 \\\\).\\n- \\\\( \\\\beta_2 \\\\) is the coefficient for the feature \\\\( x_2 \\\\).\\n- \\\\( x_1 \\\\) and \\\\( x_2 \\\\) are the input features.\\n\\n**Code:** \\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\n\\n# 4. Logistic Regression\\nprint(\"\\\\nLogistic Regression Example:\")\\n\\n# Generate sample data\\nX, y = make_classification(n_samples=100,\\n                           n_features=2,\\n                           n_informative=2,\\n                           n_redundant=0,\\n                           n_classes=2,\\n                           n_clusters_per_class=1,\\n                           random_state=42)\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create and train the model\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred)\\nrecall = recall_score(y_test, y_pred)\\nf1 = f1_score(y_test, y_pred)\\n\\nprint(f\"Accuracy: {accuracy:.2f}\")\\nprint(f\"Precision: {precision:.2f}\")\\nprint(f\"Recall: {recall:.2f}\")\\nprint(f\"F1 Score: {f1:.2f}\")\\n\\n# Visualize the decision boundary\\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\\n                     np.arange(y_min, y_max, 0.02))\\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\nplt.figure()\\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\\nplt.scatter(X[:, 0],\\n            X[:, 1],\\n            c=y,\\n            cmap=plt.cm.RdYlBu,\\n            edgecolor=\\'black\\')\\nplt.xlabel(\\'Feature 1\\')\\nplt.ylabel(\\'Feature 2\\')\\nplt.title(\\'Logistic Regression Decision Boundary\\')\\nplt.show()\\n\\n**Markdown:** \\n## Linear Discriminator\\n\\n**Markdown:** \\nThe sigmoid function is a mathematical function that produces an \"S\"-shaped curve. It is often used in machine learning, particularly in logistic regression and neural networks, to map predictions to probabilities.\\n\\nThe formula for the sigmoid function is:\\n\\n$$\\n\\\\sigma(x) = \\\\frac{1}{1 + e^{-x}}\\n$$\\n\\nWhere:\\n- \\\\( \\\\sigma(x) \\\\) is the sigmoid function.\\n- \\\\( x \\\\) is the input value.\\n- \\\\( e \\\\) is the base of the natural logarithm, approximately equal to 2.71828.\\n\\nThe sigmoid function outputs values between 0 and 1, making it useful for binary classification problems.\\n\\n\\n**Code:** \\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Generate sample data\\nX, y = make_classification(n_samples=100,\\n                           n_features=2,\\n                           n_informative=2,\\n                           n_redundant=0,\\n                           n_classes=2,\\n                           n_clusters_per_class=1,\\n                           random_state=42)\\n\\n# Split the data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create and train the model\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_test)\\n\\n# Evaluate the model\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred)\\nrecall = recall_score(y_test, y_pred)\\nf1 = f1_score(y_test, y_pred)\\n\\nprint(f\"Accuracy: {accuracy:.2f}\")\\nprint(f\"Precision: {precision:.2f}\")\\nprint(f\"Recall: {recall:.2f}\")\\nprint(f\"F1 Score: {f1:.2f}\")\\n\\n# Plot sigmoid function and model logits\\ndef sigmoid(x):\\n    return 1 / (1 + np.exp(-x))\\n\\n# Get logits (raw model outputs)\\nlogits = model.decision_function(X_test)\\n\\n# Generate values for plotting sigmoid function\\nx_values = np.linspace(logits.min() - 1, logits.max() + 1, 300)\\ny_values = sigmoid(x_values)\\n\\nplt.figure()\\nplt.plot(x_values, y_values, label=\\'Sigmoid Function\\')\\nplt.scatter(logits, y_test, c=y_test, cmap=plt.cm.RdYlBu, edgecolor=\\'black\\', label=\\'Logits\\')\\nplt.xlabel(\\'Logits\\')\\nplt.ylabel(\\'Probability\\')\\nplt.title(\\'Sigmoid Function with Model Logits\\')\\nplt.legend()\\nplt.show()\\n\\n**Code:** \\nprint(f\\'Model coefficients: {model.coef_}\\')\\nprint(f\\'Model intercept: {model.intercept_}\\')\\n\\n**Markdown:** \\n$$\\n\\\\text{logit}(p) = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 = 0\\n$$\\n\\n**Markdown:** \\n$$\\n x_2 = -\\\\frac{\\\\beta_0}{\\\\beta_2} - \\\\frac{\\\\beta_1}{\\\\beta_2} x_1 \\n$$\\n\\n**Code:** \\nb0 = model.intercept_[0]\\nb1 = model.coef_[0, 0]\\nb2 = model.coef_[0, 1]\\n\\nx1 = X_test[:, 0]\\nx2 = X_test[:, 1]\\n\\n**Code:** \\nx1\\n\\n**Code:** \\n-b0/b2 -b1/b2 * x1\\n\\n**Code:** \\nx_min, x_max = -1, 2\\ny_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\\n                     np.arange(y_min, y_max, 0.02))\\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\\nZ = Z.reshape(xx.shape)\\n\\nplt.figure(figsize=(10, 8))\\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\\nplt.scatter(X_test[:, 0],\\n            X_test[:, 1],\\n            c=y_test,\\n            cmap=plt.cm.RdYlBu,\\n            edgecolor=\\'black\\')\\nplt.xlabel(\\'Feature 1\\')\\nplt.ylabel(\\'Feature 2\\')\\nplt.title(\\'Logistic Regression Decision Boundary\\')\\n\\n# Add grid\\nplt.grid(True, linestyle=\\'--\\', alpha=0.7)\\n\\n# Ensure grid is behind the plot elements\\nplt.gca().set_axisbelow(True)\\n\\nplt.show()\\n\\n**Markdown:** \\n## Breast Cancer Example\\n\\n**Code:** \\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nfrom sklearn.model_selection import cross_val_score\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the breast cancer dataset\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# Create a DataFrame for easier data manipulation\\ndf = pd.DataFrame(X, columns=data.feature_names)\\ndf[\\'target\\'] = y\\n\\n# Data preprocessing\\n# Check for missing values\\nprint(\"Missing values:\")\\nprint(df.isnull().sum())\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale the features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Train the logistic regression model\\nmodel = LogisticRegression(random_state=42)\\nmodel.fit(X_train_scaled, y_train)\\n\\n# Make predictions on the test set\\ny_pred = model.predict(X_test_scaled)\\n\\n# Calculate accuracy\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(f\"Accuracy: {accuracy:.4f}\")\\n\\n# Display confusion matrix\\ncm = confusion_matrix(y_test, y_pred)\\nplt.figure(figsize=(8, 6))\\nsns.heatmap(cm, annot=True, fmt=\\'d\\', cmap=\\'Blues\\')\\nplt.title(\\'Confusion Matrix\\')\\nplt.xlabel(\\'Predicted\\')\\nplt.ylabel(\\'Actual\\')\\nplt.show()\\n\\n# Display classification report\\nprint(\"\\\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred))\\n\\n# Perform cross-validation\\ncv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\\nprint(f\"\\\\nCross-validation scores: {cv_scores}\")\\nprint(f\"Mean CV score: {np.mean(cv_scores):.4f}\")\\n\\n# Feature importance\\nfeature_importance = pd.DataFrame({\\'feature\\': data.feature_names, \\'importance\\': abs(model.coef_[0])})\\nfeature_importance = feature_importance.sort_values(\\'importance\\', ascending=False)\\n\\nplt.figure(figsize=(10, 6))\\nsns.barplot(x=\\'importance\\', y=\\'feature\\', data=feature_importance.head(30))\\nplt.title(\\'Top 10 Most Important Features\\')\\nplt.xlabel(\\'Importance\\')\\nplt.ylabel(\\'Feature\\')\\nplt.show()\\n\\n'}]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":[],"metadata":{"id":"CnFJUmJJsClB"},"execution_count":null,"outputs":[]}]}