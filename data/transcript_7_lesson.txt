 Доброе утро. Слышно только чуть-чуть. Сейчас лучше. Да, слышно. Спасибо. В субботом, в среднем, сколько людей бывает в один цех? От 40 до где-то 40-45. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Слышно только чуть-чуть. Да, слышно. Спасибо. Когда я думаю, можно начинать? Вы видите, да? Экран виден на всем? Да. В принципе, 32 человека. Я думаю, можно начать. В общем, всем доброе утро. Спасибо, что пришли. У нас тоже будет не всегда легко учиться. Окей, начнем. Доброе, доброе утро. Доброе, доброе утро. Делие решений. Они поговорили про строительство. Они далее поговорили про пенсифицирование деливания. Какие существуют? Вот. И я своим утро иду сюда. Окей. Ну, давайте начнем с деливания решений. То есть это такая фундаментальная штука, которая очень отлично решает. Ну, она простая вещь, но очень эффективная. Своим пониманием. Она очень похожа на то, как, в принципе, наше восприятие мира работает. Значит, для нее это деление здесь тоже по правилам как-то существует. То есть это алгоритм данного листа решает задачи. Попеи и гладче, то есть фиксификация регрессии. И в целом, что у этого следующее, да? То есть, в сущности, как бы, ну, вот древесный вид того, что мазиску-то корень, да? И как бы от этого корня, да? Как бы от этого традиционного дерева. Дальше ветвятся ствол, там ветки, листья и т.д. Ну, как и обычно, все это данного программируемого вида на дерево. Ну, то есть вот здесь написано. Здесь, как бы, если говорить, там есть три фундаментальные вещи. Нода, branch, leaf. Нод это наш сам признак, да? Ну, как бы, если мы в тему крафтоговорим, то это будет, по сути, повершено на крафт. Вот branch, вербилетление, это разделение. Это, получается, наше уже правило отделения. Ну, и leaf, соответственно, получается, да. По сути, то же самое branch, но да, только конечно, да. Слота надолго тоже нет в дальнейшем, да, это петли. Child, not native. В целом, можно аналоги привести справа. То есть, если привести на более такой натуральный язык, то, в принципе, время следующего дня. То есть, сегодня солнечно, да, это наш корень нашего дерева. И вот этот вопрос разделяющий. То есть, в данном случае, это, ну, дезеложного свеча какого-то, да? То есть, свеча, признак про погоду, да, про солнечность. И здесь сразу, да, есть вветление, да, branch, получается. То есть, на две, да, есть вветления. Есть, но, честно, как бы, по той или иной пути вветление у нас, ну, происходят другие-то события и другие правила. То есть, в случае, если мы проваливаемся въезд, да, то есть, да, сегодня, типа, солнечно. А далее еще одно вветление существует. It's humility high. Важность высокая, да, сегодня. То есть, есть yes, don't play, да, есть no autoplay. То есть, там вот эти don't play, play, don't play, play. В конце, да, это наши листья. То есть, это самый конечный исход. Вот, в целом все просто, как и в программировании обычно. В натуре данных абстракция, вообще дерево, она идет сверху. То есть, сверху мы строим наш корри. И даунтузи, даунтузи бодом мы идем по этим правилам. Ну, то есть, перерод, даунтузи. В целом вот, в принципе, очень простая аналогия, но в целом очень эффективно выглядит. Ну, так вот, при построении деревьев очень важно упомянуть следующее понятие, как... Привернулся. Да, в целом, еще очень важно упомянуть такие термины, как Термины, как интерплеи, информационные, информационные гейны. А вот, да, информационные, как какой-то айсес. Бавка, да, это перерезается. В целом, что это такое, да? Ну, тут, поди, еще нужно вставить две гейни, перерезать и так далее. В целом, а как бы... Теперь все это понятно, да? Как это строится, но вопрос заслывает. А вообще, как, собственно, это в автомате доделать? То есть, как это сделать так, чтобы машина сама понимала, какие виды они, как вообще, и когда делать? Ну, какое, да, потому что у нас бывают данные, которые в целом не интерпретируются, да? Такие понятия, как солнечно, холодно, жарко. А в целом какие-то данные непонятные. В частительных конвертах или в категориях их очень много, и такую-то конвертность проявить там очень сложно, да? А поэтому есть такое понятие, как метрики, да? У нас используются здесь две метрики при построении дирекрешений. Первая это энтеропия и генийпюрития, да? То есть, энтеропия-теропия, генийпюрития, прохладно. Не частота гений, да? Эти две метрики, на самом деле, они как бы взаимозаменяющие. То есть, мы обе используем их для того, чтобы строить наши деревья. А именно, строить наши метрики внутри дерева. То есть, мы опираясь на эти метрики, мы полагаем, на каком признаке, да? И какое правило построить на том или ином метрике. И как это делать? Давайте сейчас перейдем к алгоритму построения. Давайте поймем, что такое целая энтеропия и генийпюрития в целом. Математически это такое. А это в том же с энтеропией. Энтеропия, как бы в том ракете в Леоне, это такая мера чистоты, да? Точности, хаоса, неопределенности, а в системе, да, или в данных. Извините, вы зависаете вроде. Да-да-да, сейчас мы все зависаем все еще. У меня нормально вроде было? Бывает. Окей-окей, смотри. Да, бывает, конечно, такое. Ну, ничего говорите, да. Где я торможу, зафридался, говорите, я вернусь сразу. 10 секунд назад. Окей. В целом, энтеропия, да, это она также как бы диапазонитовая. Она варьируется от 0 до того. В самом деле, будет хорошо, если почитаете потом дополнительный сверх. Я не знаю, как это делать. В целом, можно посмотреть на таком игручном примере, да? Справа, в этой идее, да? Энтеропия экзамп. Ну, предыдущих примере, да, где мы говорили про теннисный элемент. Гармонетенис не играет теннис, да? Солнечно там не солнечно, и так далее. Предположим, да, что вот у меня тоже какие-то задачи, да, я не знаю, как это делать. Ну, я не знаю, как это делать. Ну, я не знаю, как это делать. А предположим, да, что вот у меня тоже какие-то задачи, да? Проделение такой классификации. Играть теннис, играть теннис, играть теннис сегодня. И у меня есть, да, 100 обучаемых примеров, да, примеров про сэмплов. Там какие-то там фичи у него есть, и там лейбл, да, плей, да. Скажем, таких сотня, из которых 65 из них это плей, да, то есть поиграют. И играть датасет, да, обучаем, примеров. И 35 таких примеров, да, которые, ну, переливрис тем, что не нужно играть в цивали. Вот, в целом я могу посчитать энтерпие для этого датасета. Ну, для этой системы, собственно. Вот формулы, которые указывают на десятой формулы, это подольным десятью этой формулы. h это наша энтерпия. Вот s это, как бы, наш subset of data. И оно равно минус, от subset на минус стоит, я потом объясню, почему они здесь стоят. Суммирование px на log2, 3px. Где log2 это логеритма, да, в основании 2. Потому что мы питами работаем, в первую цель. А px это, по сути, наша отнахнул. А там интерпретируется по-разному. Где-то это интерпретируется как probability density function, да, то есть цеплотности вероятности. А я тут поделюсь про функцию. Функция плотности вероятности. А в данном случае это будет интерпретироваться как odds, шансы, по сути. И соответственно, в данном случае, если мы поставим сюда, вот, первый шаг, я, например, указал. То есть p в данном случае у меня будет 65 стадо. То есть, ну, шанс того, что мы делаем, это будет 65 стадо. То есть, то, что у меня будет плейд, да, ну, это будет общий из всего, из всей кучи примеров, это будет 65 стадо. Соответственно, no play это 35 стадо. И, по сути, как бы я накладываю вот эту формулу, продолжаю ее с собой, суммирую, в неохладном тюнингстри. Что это, то есть, как это вообще можно понимать? В целом, ну, это че-то так афракно, да? Всем понятно, что с этой величины еще делать. Там тоже, на самом деле, нет такого прям определенного, правильной, такой, диффиниции универсальной терапии. Потому что в разных моментах она по-разному дает терапии. Ну, я скажу следующий, который тоже будет с этой пониманием. Также терапии часто вообще определяют как в питах информации. То есть, допустим, в данном случае, если мы делаем, например, вот, патолий, то есть, допустим, в данном случае у нас вышел 0.93, значит, у нас информация, которая хранится в этой системе, можно закодировать в среднем 0.93 битами. То есть, меньше чем один бит информации потребуется для того, чтобы в среднем закодировать эту информацию. Вот. Впрочем, когда у нас, как сказать, ну, окей, я дальше про козы терапии не буду говорить, хорошо. Ну, пока берите так. Вот такая вот величина у нас есть. Мы всегда стремимся, чтобы при построении того самого дерева, да, мы стремимся, чтобы когда мы учитывали терапию, доказывают линию этого дерева, мы хотим, чтобы вот эта терапия поддержала это сркп. Ну, в идеале, да? Вопрос есть. Да. Вот, типа, мы здесь засплитили 65 на 35 и получили энтропию 0.93. А как высчитывается значение параметра, по которому мы сплитим? Типа, перебором высчитывается, чтобы, ну, и проверяется, скажем, мы спараметру, по интерп... Ну, у какого меньше энтропии или как? Да, это... к этому мы еще не начали даже правил обредумывать. Я понял, что да. По сути, да, то, что я объяснил, это просто я показал, как пока считается энтропия. То есть пока здесь мы даже про деревень говорим еще. А здесь мы говорим просто про... вот у нас кусок данных есть, да, какой-то большой, ну, небольшой. Просто кусок данных, информации системы. Почитаем для него этот пик, ну все. Здесь пока про деревень еще ничего не говорим. Вот, вот. А... а на чем я оставился, что-то было? Окей. Сейчас посмотрим. Сейчас посмотрим. Вот. В целом, да, мы хотим при построении деревьев, да, всегда пытаться энтропию понесить. Пока так. Пока в таком степи. Вот. Давайте теперь поговорим про ту же аналогию с Дженни Пирити. Дженни Пирити, в самом деле, почти об одном том же. Она тоже показывает какую-то меру неполноты, нечистоты, да, ауса. Ауса. Какого-то там, системе или данных. Но она, так сказать, проще устроена в плане того, что она варьируется, да, оранжировался от 0 до точка 5. Соответственно, она, ну, чуть-чуть быстрее вычитывается в компьютере, так сказать. Вот для этого логарифма. То есть, Дженни, это вот 12-я формула. 12-я формула. В большая Дженни, 1 минус суммирование всех, ну, шансов, да, вероимперианности в квадрате от лежаного квадрата. Суммироваться на все классы. Ну, это, это в бинарном конце, в общем, не принес. Вот. В суть, как бы, вообще такая практика в мире, в присутствии, да, что выбирать, да, энтропию или Дженни Пирити, да, при выборе своей метрики, да, понимания, клеения, идея дерева. Тут, на самом деле, такого правила простругово нет, в принципе, они об одном и том же, в целом. Они всегда, в принципе, дают один тоже жуток. То есть, неважно берете вы энтропию как бы за слово метрику, да, или нет дерева, ибо Дженни Пирити у вас результаты, реформа с моделей, точность показания, метрик будут сложен. Вопрос, наверно, больше интерпретации, и больше, наверно, вопрос, наверное, сложность<|da|> в общем, в целом, ну, окей. Как минимум, в рамках этого курса, мы дам оно-то не так интересно. Вот. Теперь, вот, переходим, да, к информационному гейму, да, получается. Вот здесь самое интересное начинается. В принципе, мы теперь, мы теперь посчитали, да, мы знаем, что такое энтропия и Дженни Пирити. А теперь, собственно, так вот, что вопрос, я не видел ними, человеком. Собственно, как выбирать, да, вообще, и по каким правилам придерживаться? Какой рейтейн начинает делать, и так далее. Собственно, вот здесь информационный гейм помогает вам это понять. То есть, ну, это о чем спирать, инфиниция, информационный гейм, это, по сути, это, по сути, мера того, как наша энтропия, да, ну, или Дженни Пирити, понижается после того или иного, ну, сплитов, да, сплитов деревенец. То есть, когда мы сплитнули дерево, да, тогда, в данном случае, например, здесь первое сплит, у нас получальная стадия из Сани, да, надо было как-то своя энтропия в системе. Ну, допустим, на 93 было начало. После того, как мы сделали из Сани, да, то есть, делали разделение, да, на наших данных, обсудим два пути, да, а если на у нас в Сани. И в каждом теперь из них, да, из этих деревьев, ой, из этих облеглений будет своя новая энтропия. Ну, не часто так, потому что мы уже, да, чуть ли не сменили систему, мы чуть ли не сменили данные, да, которые остаются только в этом, в этом бранче. И здесь тоже такие-то данные другие остаются в этом бранче, ну, они отравили, да, просто справили. Теперь нужно нам, как бы мы пересчитали, да, энтропия здесь, здесь. Так вот, информационный гейт здесь приходит на помощь, чтобы понять, учитывая разная энтропия, а какую все-таки сплит взять лучше. Вот, а возможно вдруг, вдруг, не знаю, вдруг я хочу не из-за Сани брать, а вдруг я хочу взять первым дельм, как, как, в день, а не сани, а из-за трейдинг, допустим. Вдруг это более выгодное правило сверху, да, чем ставить из-за трейдинг, ну, где-то, где-то сейчас ставить. В этом нам помогает информационный гейт. Ну, то есть, на пистоле межропридакшен и атропия, а в этот сезон, атропия, все-таки. Аттропия, все-таки. Вот, и по сути, он считается очень легко. Вот эту формула, она, как бы, может быть, запутанная, но она не такая сложная. IG, да, равно HS, то есть энтропия родительского узла, родительской ноды, отнять суммы, взвешенные суммы всех детских нод, энтропии детских нод. Здесь понятно или чуть-чуть объясните подробнее, что это значит? А взвешенная сумма это какие числа там? Взвешенная сумма – это получится количество примерных. Ну, взвешенная сумма – это когда ты их суммируешь, но при этом еще ты учитываешь, как бы, вес каждого того или иного члена слагаемого, отстой на его количество, да, во-первых. Ну, допустим, если ты… есть просто усредение, да? Я понимаю, а вот SW, я не понимаю, что это в моменте. Ну, SW – это в векторной форме написано, S – это векторная форма, если ты SW делаешь на каждую ось вектора, понижаешь, у тебя, по сути, вес каждого члена. Ну, члена. Это амплитуда векторная, да? Ну, смотри, да. Да, допустим, у тебя есть вектор с пяти разграбностей. Ну, сейчас уже понял. Да, берешь ванитуду, внизу динаминатор, там, ванитуда будет, вектор, аскаляр. А сверху будет вектор сам. То есть ты ее поделишь, да? То есть вектор, динамит, аскаляр – это будет вектор, да? Угу. Но он уже будет нормирован, да, он помножен. Поэтому это есть вешение. Ну, понял, понял. Можно, пожалуйста, пример? Окей. Да, сейчас мы пример, сейчас объясним об этой практике, да? Прямо сейчас или попозже? А если попозже, то я подожду, да? Ну, в целом, да, я тогда объясню неформально словами, что значит. Когда мы строим правила витления нашего дерева, мы опираемся на энтропию и информационный гейн. Энтропия нам позволяет просто посчитать уровень хаоса в сплитах текущих. Вот. А информационный гейн помогает уже, так сказать, понять, насколько мы редюсимся. Насколько мы понижаем энтропию. То есть, условно говоря, у нас есть, да, ну, например, здесь, да? У нас изначально, да, когда мы еще не построили это дерево, да? Вот, забудьте, если это все в ту же дерево. Просто у нас есть фича в сани, amenity high и reining, да? Три, три, три признака есть. Три фичи. Вот. И там, не знаю, 65 play, да? И 65 не play. Ой, 35 не play. И, видите, мы... Ну, допустим, как мы поставили из-за сани первым образом? Мы сперва посчитали энтропию общую, да? 0.93. В самом начале. А потом посчитали информационный гейн, да? Для каждого случая, когда мы, значит, стартовали бы вот тут, вот, ротнотом, сперва для из-за сани, да? Посчитали до 10 энтропию, 10 энтропию. Потом, второй вариант, да? Примерно, сейчас из-за сани поставили химинити high сюда. И здесь, соответственно, будет из-за сани, здесь будет из-за тренинга. И так далее для из-за тренинга сделано. То есть мы сделали разделение на все три варианта. Посмотрели, в котором из этих трех вариантов наибольший информационный гейн. Вот. А информационный гейн считается как родница родительского, да? Энтропии. Отнять свешенную сумму всех детских энтропий. Детские энтропии, да, имеется для левый, да? Левый этот лифт и правый лифт. Вот здесь есть. Ну, на примере дальше покажем лучше. Там будет буллербан. И что мы хотим? Мы всегда хотим, чтобы наши, хотим, чтобы все наши сприты были как можно чище. Чище в плане pure. pure, чтобы каждое наше битвление дерева делило данные так, по этим правилам, разделяющим правилами решения, ну, течения рулга, так чтобы в каждом из этих лифов, листьев, которые остаются в конце, было вот, pure, класс, class pure, то есть были типа допустим, только там красный цвет остается в этом лифе, не так что там будет 5 красных, 7 зеленых остается, и 10 черных, слишком разнородный, остается лист, туда попадает типа 3 класса с разным кличицем, и вот, ну, примерно так, чтобы она еще не заорфитилась, вот, ну сейчас объясню, если не поняли, ну, в суть, вот алгорилем сам, можете его почитать, начинаем с вот этого сета, вот это рукнот, вычитываем энтропию, для всего этого сета, далее для каждой фичи, типа for loop запускаем, ну, псевдокорд такой, для каждой фичи читаем его типа информационный гейн, если split сделан на этой фиче, и потом, как мы всех посчитали для каждой фичи, мы выбираем фичу для разделения первого зрения, с наивысшим ij, информационного гейна, потому что информационный гейн нас судит, в том сколько мы информации с ценом получаем, да, системы, то есть, понимаете, как я ранее говорил, у любой системы, да, данных системы, ну, вообще, данные или системы какой-то, есть всегда какая-то непредсказуемость, да, в энтропии в системе, мы всегда хотим эту энтропию понизить, чтобы система была более, все, что предсказуемое, да, определенной была, соответственно, вот, split информационного гейна для того, чтобы делать каждое ввекление дерева таким, чтобы информации больше было у нас, чтобы uncertainty в системе не исчезало, чтобы система была предсказуемая, да, чтобы любой, как бы, исход событий входных каких-то параметров выплевывал высоковероятные, да, какие-то эти, ну, то, что мы ожидаем, да, это же estimation, да, наши ожидания всегда, чтобы оно было всегда выше, чем, чему мы ожидаем, но я опять-таки, да, возвращаюсь, то читал, да, помню, я говорил про максимум estimation, это опять по этой истории иду. А, приборы, тут можно тоже, мини-вопрос? Да, можно. Вы сказали, что пробегаемся по всем фичам, а какой именно параметр, типа, задаём для фичей, ну, там, условно, влажность больше 50 и больше 60, вот как вот это высчитывается? Берёте наивысший. Ну, по information game наивысший, да? Да, просто наивысший. А как он, ну, он, типа, внутри себя перебирает вот эти значения, ну, то есть, 50, 60, там, или 7, типа, он же split-ит, и вот как понять, типа, по какому числу он split-ит, по какому параметру? А, ну, смотри, да, да, да. Типа, почему 50, а не 60, да? Ну, я понимаю, что information game внутри лежит, но что происходит внутри? Он, типа, перебирает или как-то высчитывает, как-то там, градианно приходит, условно, к током числу? Так, я лучше, наверное, объясню в примере, это гораздо лучше, потому что, наверное, я опять объясню это, опять, кажется, мне здесь непонятно будет. Да, я нормально через линейку был. Ну, в целом, да, наверное, он по сразу ответит, split-ы, получается, почему делятся, потому что мы выбираем, допустим, пример, который будет, там будет такой параметр длина этого листа. И эта длина листа будет, правило, да, первое, будет меньше 2, 2.5 и больше. Вот, допустим, да. Вот правило это. Соответственно, она на первом делении, да, она течет какие-то кучу классов вправо, кучу классов влево. Вправо у нас тут 140 классов, 140 примеров, не класс, 40 примеров, там перемешанных классов. В другом у нас тут там 200 примеров. Окей. Вот. И вот читаем теперь, теперь читаем, вот у нас сверху была энтропия, да, общая параметр до деления. Теперь у нас еще есть энтропия правая, левая, да, под множеством классов. И считаем теперь энтропию как то, что в начале считали. Суммарно, перемножаем количество на деревное, его количество, и тогда это п, высчитаем энтропию, энтропию, энтропию, и три энтропии у нас здесь. Потом и детские, детские, ага. Ну тут вы сказали, там допустим 2,5, да, длина листья, а почему 2,5? Вот такой вопрос был. Типа, как модель поняла, что 2,5 это оптимальное, самое лучшее параметр, правило, по которому стоит делить? Почему не 3? Ну окей, хороший, хороший вопрос. В данном случае здесь задача решается как, Мария, 10,6, 30, да, они решаются так и для классификации, так и для регрессии. То есть в данном случае это было усреднение какого-то параметра, чтобы еще так и для работы. Мы взяли лист, да, вот сейчас все, все, я понял, окей, лист, и так. Допустим, если это был бы категоридный параметр какой-то, да, мы взяли. Да, извиняюсь, если это был бы какой-то категоридный параметр, типа там девочка-мальчик, да. 0,5, да, было бы условно. Нет, там был бы true-false, да. Is it girl, да, no, is male, true-false. Вот, а если так как это типа непрерывный параметр numerical, ну там, калибрица, то тут можно взять усреднение. Но это не факт, в смысле, я то, что сказал, усреднение, это, ну, иногда это не так дерется. То есть это частная практика, просто упирать усреднение. Там, кстати, есть множилка, ну, где-то ты можешь взять там, то есть... То есть, да, да, там можно какую-то там, не знаю, самую частую взять, да, подальше, или там, среднюю, взвешенную среднюю. Ну, там, как бы это, ну, это не так, в смысле, не в этом, на самом деле, еще. Не, я, да, я понял, все, супер. Но про entropy и IG понятно. Ну, в целом, да, то, что я сказал. А, ну и самое главное, что это пункт, шестой алгоритм, это то, что получается... Мы не можем бесконечно, да, наше дерево расти. Ну, в смысле, это мы можем это сделать, но это будет очень плохо. Потому что, ну, может прийти такая стойка, что мы сделаем столько ответлений, что дерево просто выучит какую-то чушь. Потому что вы все равно не сможете добиться нулевой entropy максимально никогда. То есть у вас всегда будет какая-то, ну, нединимость, донно словно, реформации, где вы не сможете очень чисто, фюр, да, делить листья так, чтобы они прям, ну, идеально. Ну, если, конечно, это с этим игрушкой какой-то, который, в общем, простой тип, полдан, да. С теста, чтобы такого не случилось, мы делаем какие-то стопинг-критерии. Вот шестой пункт. И разы поят, но я три указал. Это получается... Мы можем лимитировать глубину нашего дерева, листьев и веток. Первым это максимальной глубиной. Я хочу максимум пять уровней дерева, да, все, не глубже. Десять до семи, до восьми там нельзя. Слишком будет там много ветей. Это очень плохо. Либо там минимум number of samples, первые leaf node. То есть, то есть, то есть, нельзя, да. Я такой лимит, я констрейт ставлю такой, что в моем leaf node где-то сам он низко, да, лист, лист, листе. Должно быть там минимум пять samples приходить. То есть, как бы это минимум. Нельзя, чтобы один был. Если один приход, тогда еще туда только один пример приходит. Это капец-типо-мол. То есть, мы создали целое ветление только для одного примера. Нет, нет, нет, генерализация этого дерева. То есть, она там будет целая leaf структура работать для одного примера. Который может быть просто шумом или outlier-ом системе. То есть, там мы допустим 10 или 5. И третье, это информационный гейн below a certain threshold. Ну, типа splitted на ветки тогда, когда у нас минимум, как говорится, порог информационного повышения гейн. То есть, 20. Я не буду splitted, если у меня будет информационный гейн меньше 20. Только минимум 20 и больше. Ну и пронирование. То есть, пронирование – это такое понятие вообще очень аферное. Когда мы пытаемся обычно... После обучения дерева обычно так бывает, что мы получили дерево, оно только разрослось. А потом давайте спронируем его. То есть, уберем какие-то ненужные вещи. Чтобы генерализацию увеличить. И чтобы не было overfit. Все знают, что это overfit и underfit? Нет, нет. Что это? Окей, sorry. Ну вы еще спрашиваете, ладно, активно. Не забываем, что я помогу Ивану, вы помогаете мне. Давайте лучше тогда поднимите руки или просто скажите да-да-да. Тех, кто не знает, что это overfit. Будьте смелее, ничего страшного. Нормально. Чтобы я понимал просто насколько мы... куда мы идем сейчас. Прием? Ну да, там подняли руки вроде люди. Это в реакциях? Да, да, да, в реакциях. Мне кажется, нет причин. Ну ладно, окей. Смотрите, есть underfit понятия, да? Underfitting, model overfitting. Есть model overfitting. Ну то есть, с английского языка это получается... Ну fit это получается подходить, да? Или обучиться, да? То есть подойти к чему-то. Получиться в чем-то, да? А представка under, over это когда не добежали до чего-то или перебежали. То есть, переобучились, да? И не дообучились. И fit вы имеете в виду, может быть, перезбыток? Не в этом смысле идет feed. Не, именно вот fit. А, все, все, все, все. Не F E E D. Не по обратной связи, а вот fit. То есть получается... Обычная проблема машинного обучения это когда у нас модели либо underfitted, либо overfitted. И то это плохо. Мы никогда не хотим модель, которая underfit, потому что она не добучилась. Это значит, когда модель не получила максимальный объем потенциала, который может вытащить все эти системы данных, хотя там был до потенциал для роста. То есть она могла какие-то закономерности вытащить, она могла выучить еще правила скрытые, да? Но она это не сделала. Ну, потому что там не хватило времени, либо не хватило творности параметров, либо еще чего-то еще. То есть она как бы недообучилась, требуется еще больше обучения либо временем, либо чего-то еще. Какой параметр показывает, что модель, например, не дообучена? Это очень хороший вопрос и очень сложный вопрос, а сам для открытых. Масса причин, есть масса метрик, которые помогают нам определить, опять и не явно, то к какой мере модель недообучилась? Ну, например, это что, можно сказать сразу? Это когда... Например, это вообще, когда модель плохо работает. То есть если модель плохо работает, это один из причин, может быть, недообучения раз. Далее, это когда у нас на трейне, на обучающей выборке модель хуже работает, гораздо хуже работает, чем на валидации. Помните, валидация и трейн, это что это такое? Да, да, да. Ну, получается, только в моменте использования модели мы можем это понять. Да, да. Ну, и еще как бы обычно такое понятие есть, обычно простейшие модели, то есть модели, которые очень мало обучаемых параметров, то есть линейные регрессии. Если, допустим, вы примените линейные регрессии для сложных данных, простейших данных, кто-то, как в тот раз я показывал, где-то одна перемена существует, Александр сейчас секундочку, я отвечу. То получается, допустим, если вы примените на датасет из 500 параметров обычную линейную регрессию, то есть, если у вас нет судьбы, просто не хватит обучающих параметров, выучить закономерность, это страшно, это сложно. Это будет тоже по-любому антерфит. То есть, когда у нас очень мало обучаемых данных, когда модель не такая комплексная, это признак того, что по-любому антерфит будет. Александр, да, да? Нет, все вопрос уже закрыли. Еще раз? Что-то пропало. Вопрос закрыли, все уже понятно. А, понял. Про оверфит то же самое, вернее, диаметрально, я противоположен. Когда у нас модель слишком сложная, сложная в плане, она имеет слишком много обучаемых параметров, комплексная данная, а работает на данных, которые требуются, можно решить куда проще моделем, это хороший сигнал того, что скорее всего оверфит. А оверфит, по моей практике, по моему опыту оверфит выявить чуть-чуть легче, чем антерфит. В плане, что на оверфите, когда модель, когда она на валидации начинает хуже работать, чем на тренере. То есть, на тренере, она очень много работает, она сам все понимает, стой и ста выпивает, но мы даем данные, которые она не видела в природе, она вообще там буксовать. Она не понимает, что она слишком сильно зашилась под правила обучающей выборки. То есть, она просто не выучилась, как надо выучиться, она просто запомнила ходы какие-то, но это не есть правильные правила, которые универсальны для других кейсов. Давайте дальше потом. Просто здесь есть по перел. Давайте теперь поговорим про плюсов и недостатки и минусы. Сильвентрис. Конечно, тоже есть до сущности. Давайте с плюсов. Вообще, сильвентрис это крутая штука, хоть она очень простая, физическая считается, но без нее мало чего происходит. То есть, современные логи, мехдехи, они до сих пор используются. Даже если вам нужна простая, простейшая модель, вам не нужно нейросетку строить, вам нужно трансформер поднимать, вам нужно какую-то простую модель, но при этом она должна быть моделью. Высповеду сильвентрис. То есть, простой вред, древой решение. Потому что она очень похожа, как люди думают. Сильвентрис может работать с обеими численными данными и категорией данных. Очень классно. Не нужно никакой моделизации, еще припроцент. Процент вообще не нужно запишется. Еще, con-headed multi-output problems. То есть, если мы создаем лотикласс-санкцификацию, мы можем здесь использовать. Про концы, сговорить, просто не на татке. Можно сказать, что если неправильно их контролировать, они могут разрастаться до очень глубоких деревьев и поэтому вызывать излишнюю комплексность модели, тем самым введя понятие как overfit. Если это overfit, значит она будет плохо генерализировать новый дан. Генерализировать, я имею в виду, она не может их обобщить под реальный паттерд. Это они склонны к overfit. Потом еще, это понятие consent-strict small-resolution data. Если какой-то выброс попадает в train-roach set, шум, случайная ошибка, то это зачастую очень хорошо привлечет к адаптеру на перфоманс. Модель может выдумать витление, странное, из-за одного маленького примера. И это может угробить другие ветки. Такое тоже бывает. Потом есть bias, предъявленная с каким-то определенным печат. То есть она иногда, если что, хорошо работает с объективностью в расставлении признаков. Какие-то признаки она бывает, а иногда пушит, а какие-то не пушит. Ну и она плохо работает с высоко коррелирующими печами. Честно, я здесь вычитывал, но я пример не могу сказать. Можно вопрос про предыдущий слайд? Да, да. Сensitive to small variations. Это можно как-то митигировать с помощью прунинга? Или это сложно увидеть? В основном да. То есть в основном, как бы то, что я говорил про прунирование, это всегда делается. То есть любое дерево всегда потом, когда строится, оно полностью построилось, такое глубокое, даже в шереду. А потом мы прунируем его всегда. И вот тем самым таким образом увеличиваем шансы точности. А то, что вопрос, то есть small variations, то это как бы либо пучить и данные, то есть убрать выбросы, убрать что-то еще, аномалии, какие-то данные. Спасибо. Теперь давайте поговорим про ансамблированные обучения, то есть про ансамбли. Вот это вот прям здесь, если про 3, может быть, редко встретитесь с деревьями в чистом виде, про ансамблирование, то есть я вам говорю 10-10% шансов. Поэтому здесь максимально, слушайте, это очень крутая тема, она... Все финтехи мира, все кагл-чемпионаты, чемпионы, вот реально говорю, все финтехи, панки, крутые там всякие предсказатели временных рядов, на фондовых рынках используют ансамблированный обучение. Поэтому это прям крутая тема. В целом, ансамблирование, если в словах говорить, это по сути ансамблирование, когда мы что-то объединяем в кучу, ну, ансамбль, просто так. И в целом это простое правило. То есть у нас есть weak learners, слабые модели, которые, если они аккумулируют в себя, акумулируются, становятся едиными, или выстраивают какой-то унифицированный подход в делении информации на собой, они становятся гораздо более сильны одной модели ансамблям. Это реально так, это реально крутая штука, концепция. Даже аналоги покажу, какие-то аналоги, которые в ряду же не существуют, на основе этой идеи. В целом это говорит следующее. То есть мы подержаем ошибку через осределение для регрессии, и мы подержаем ошибку в случае голосования для классификации. То есть ансамбли они решают задачи регрессии и классификации. То есть что здесь имеется в виду? Допустим, вы знаете, я не помню какую-то теорема, математическая история, когда каждый спрашивает, вот толпы людей, какой-то один миф. Не миф, а какой-то вопрос. И там контекста вообще никого нет. И в этот момент спрашивают у людей, кару Тёка в городе спрашивает, просто вопрос без контеста. Условно говоря, сколько там тон земли, тон урожая мы добыли в прошлом году? И человека вообще. То есть никакого контекста, никакого водного не дается больше. Итак, допустим, спрашивают у двух тысяч людей. И знаете, там даже какой-то эксперимент был, что в виду того, что появляется большое количество таких quick learners людей с мнением каждому, с своим субъективным мнением, субъективным, с профессиональным читанием. Если это усредить в одно потом, в конце, то результат реально будет близок к тому, что по-настоящему было. То есть это типа мудрец толпы, да и фиг называется такой. В целом, толпа может быть и ошибаться, но в целом ее результат достаточно прибуден близок к реальной удаче. Вот. Ну, это правда averaging, когда у нас есть много, допустим, множество-множество деревьев слабых. И мы там каждого обучили, оно в целом слабо действует само по себе. Но когда мы усредляем результаты всех деревьев вместе воедино, то результаты неплохо так сходятся с стикерным результатом. То есть это про voting, голосование, да, то есть это когда majority voting, да, когда у нас классификация, то есть у нас просто 500 деревьев, так бы голосовалось за 21, 300 за 0,8. Верно, выбираем первый, потому что 500 больше, 0,5. Вот. Ну, в целом, да, бывают три типа. Беггинг, ну, типа фанцомдирования. Беггинг, так как как bootstrap aggregating, boosting и stacking. Ну, мы поговорим в этом курсе только про беггинг и boosting, про stacking в принципе, поэтому это не надо говорить, мне такое популярно. Ну и времени так много. В целом, да, фанцомдирование, и в мире задача, реальная задача, будет встречаться только в беггинг и в boosting. Давайте дальше. Давайте поговорим теперь про беггинг. Что это в общем? Вообще беггинг, по сути, я максимально жал. Вот, это уже та дифференция, когда мы обучаем множество инстанций одного и того же алгоритма, это прям ничего и слова same алгоритм, в основе, тоже алгоритм, на разных подмноженствах, учающих выборки. И потом агрегируя эти же данные. То есть мы в параллельно обучаем, условно говоря, мы просто хотим построить ансамбль из 10 литрис, через беггинг. Мы сберем, какой-то лейтенант, это одно дерево решений. Наши 1,10 литр. Одна модель. Мы обучаем 100 таких слабых деревьев на разных данных. То есть одному добавили какие-то данные, другому такие-то, ну и перемешали там. И вот так вот в параллельно они обучаются, в параллельно они дают ответы. И потом мы из результатов усоединяем. Ну как вы, мудрый стал, да, в принципе. Один хороший пример это random forest. Random forest это в частном случае беггинг с учетом, с подкапотом, когда у нас есть тижин. По сути тут у нас и 4 training bootstrap sample, organic data, jacquard, node, only random set features, то есть когда мы обучаем 1 из статых деревьев, когда приходят какие-то данные, да, туда, ну, обучаемые данные, то не все фичи, да, не все колонки заходят на все деревья. То есть, он говорит, там 20 деревьев пришли только 5 колонок, а другим 20 деревьев пришли другие 5 колонок, они не пересеклись. Поэтому называется random forest, то есть если элемент рандома чуть. И в принципе у вас на random forest вам всегда лучше работать, чем просто беггинг на season 3. Вот. Может вы тоже считаете, простой forest, да, и random forest, а простой forest, вот, он обычно лучше работает. Есть вопрос? Вроде, да. Окей. Давайте, да. А в чем разница forest и просто tree? Tree – это одно дерево, да? Угу. То есть у него просто такое дерево. А forest – это когда таких много деревьев, да, очень много-много, ему средняя значить. Ну, то есть, когда мы говорим про forest, то значит мы про беггинг говорим. Когда много-много плотим деревьев таких. И потом средняя значение, да? Ну, как бы, да, в целом, скажем так, что, как сказать, гнега скорее всего не будет в season 3, никогда не будет. Потому что это просто же и fell-способ. Понимаете? То есть, ну, как сказать, вот это есть же фаза логики, да, физики, когда простейший этот алгоритм – если свет есть, значит сегодня день. Если света нет, значит ночь. Это, ну, это есть же tree, на самом деле, да, просто и простейший правил. Вот. А forest – это когда уже мы говорим про большое-большое количество деревьев, да, лес, да, по сути, лес. И там, когда выстраивается мудрость толпы, да, мудрость каждого дерева, она усредняется, и какой-то вертык приходит неплохой. Лоранда фореста – то же самое форест, когда у нас еще такая одна деревья приходит много раз, инфичи, да, чем отреги дерева. То есть не все деревья, не все деревья видят один и те же признаки видят. Ну, это как это, например, людей, да, то есть если потом взять, да, 100 рандомов людей в Холостане, да, всех будут разные экспертизы, всех будут разные опыты, всех будут разные, да, вообще, дорогие условия, да, жизни и вообще профессиональные условия. И в целом это будет гораздо лучше, чем если взять 100 людей одинакового профиля, да, потому что разнородность данных, которые заключились перед 100 людей за всю всю жизнь в разных направлениях, будет гораздо выше, да, информации, которые они делали, чем 100 людей одной и той же профессии. Например, такая аналогия. Я хочу рано раз получше поговорить. Понял, спасибо. Давайте про boosting поговорим. Boosting, очень крутая штука, вторая тоже ансамбль. Вот, ну, он гораздо комплекснее, ну, и я бы сказал, что он даже гораздо сильнее в плане своей составляющей, как модель. Я не говорю, что если выбирать boosting и begging, то без этого boosting нет. Значит, зависит, в целом, как модель, говорю, он куда сложнее считается. Потому что, ну, так это есть, короче. Сейчас, секундочку. Есть. В целом, если говорить про boosting, то это мы строим модель на обучающей лабирике. То есть мы опять-таки берем вот льорный какой-то дерево, слабое дерево берем. Обучили раз его, получили результаты. Берем второе дерево, да, такое же. Его тоже обучаем, но с учетом ошибок, которые сделала первое дерево. Потом берем третье дерево, обучаем его, но с учетом ошибок, которые сделала второе дерево. И так далее до одного дерева. У второго дерева есть данные с валидацией первого дерева, да? Да, у каждого последующего дерева имеется информация, ошибок, которые допустила первое дерево. Вот это в хорошей аналогии, если вы аналоги, это мудрость и мудрость толпы. Я предлагаю обычной аналогии для передач информации поколения. То есть какое-то из поколения раз до деревьев людей, да. Оно жило, жило, жило, жило, сделало какие-то ошибки, да. Или все навыки не ошибки, а прогресс достигла, да, эволюцион. Знание, да, откроет какое-то. Все оно как бы переживает, вырождается в поколении. Повелится новое поколение, да, деревьев людей. Они берут все правильные знания оттуда, но также они еще учатся и делают так, чтобы ошибки предыдущего поколения не запустить. Но это не заходит о том, что второе дерево тоже не делает свои ошибки. И так далее, и так далее, поколение за поколением идет эффект датчиков в учении. Чтобы выправить предыдущее поколение, я тебе тоже что-то говорю. Но не факт, что ты тоже иногда с этим учёшь. И здесь говорится, что модели как бы так стекуются друг другу, да, стекаются, стекаются. Ну а до тех пор, пока, ну типа, reformat хорош, да, предпределить порт как становится, либо там опять-таки какой-то constraint. Какие-то параметры есть, которые пенализируют, лимитируют максимальное количество моделей, да, в этом стеке. Ну, допустим, не больше 50 поколений деревьев. Дальше, ну, опять-таки, да, повторяемся, да, core fit, да, в общем говоря, ранее, тем больше комплекса с модели, да, тем больше деревьев, да, больше обучения параметров, тем сложнее, тем модель более скорая при обучении. Точно, может выучить, к<|cs|>, выучить сложные паттерны, которые не нужно учить. Вот. Ну и два примера, это хороших, да, таких примера. Boosting, вот, модели, это AdaBoost, Adaptive Boosting, да, и Gradient Boosting. Ну, в целом, в основном используется GB, Gradient Boosting, AdaBoost, как бы, я в самой жизни никогда не использовал, я в этой жизни никогда не использовал, а Gradient Boosting, да, я использовал почти каждый. А в целом, definition такой, что AdaBoost, когда работа идет с завешиванием всех наблюдений, то есть мы завесили все наши наблюдения, да, в нашем дотесете, 10-20, да, дали разговорку, и тем самым в таком образом, что мы даем больший вес таким классам, да, или таким примерам, которые сложно предсказать. Not putting more weight on difficult to classify instances, and less on those already had it go. Ну, то есть акцент, на что смотрит больше, то есть, давай тем примерам, которые были сложнее предсказывать, то есть, я хочу сказать, те, которые хорошо, меньше платили. Ну, адаптивный, да, Boosting, поэтому AdaBoosting называется. Вот. Далее про Gradient Boosting. В целом, то же самое, только здесь такая вещь замечательная, как Gradient, да? А с Gradient все знакомы? Только кто не знаком, скажите, пожалуйста. Самопонятие или именно Gradient Boosting? Самопонятие Gradient, оно, в принципе, понятно. Да, да, вот именно Gradient, что-то, короче. Ну, вообще достанется. Если стесняйтесь, пожалуйста. Если стесняйтесь, пожалуйста. А вы меня слышно? Да, слышно. Gradient, получается, показывает slope его learning rate. То есть, чем больше Gradient, тем быстрее модель учится, как я понимаю. То есть, Gradient Boosting, это когда мы используем несколько моделей, у которых learning rate маленький, но мы, получается, совмещая их, улучшаем скорость обучения. Я так понял. А, спасибо. Кабинарно, да, я, да, я, да. Сэвер спрашивал, я проспрашивал про Gradient, да, что это было Gradient вообще? Патематики, физики, да? А, ну Gradient, slope, одно и то же. Delta Y на Delta X. Хорошо. Да, все верно. Это просто производная, на самом деле. В целом, Gradient Boosting называется Gradient Boosting, а потом, а, там пришли, что-то. Можно сказать Go? Не знаю. Окей. Ну, это я тебе объясню. Ну да, Gradient, это вообще понятия много, да, у него есть. Ну давайте, в общем, на ML. Вообще, Gradient это скорость изменения от чего-либо. То есть то, насколько что-то отсидно чего-то быстро меняется, растет. Допустим, если вы возьмете физическое понятие скорости, да, тела, то это по сути, мы можем сказать, что это производная его функция расстояния. То есть то, насколько человек, да, меняется в свою последнюю расстоянию, да, в пространстве, относительно своего времени, тем изменения этого, это есть скорость для нашего, ну физическая скорость. То есть, здесь тоже самое, что Gradient считывается обычно относительно функции потери к каким-то гиперпараметрам нашей модели. Ну параметрам, обучаемым параметрам. То есть то, насколько у нас, на наш Lost Function, меняется относительно изменений, изменениях обучаемых параметров. Это и есть Gradient. Это и есть Gradient. Скорость изменения нашей Lost Function, отсидный параметр. Потому что мы можем воздействовать на эти параметры таким образом, чтобы это изменение Лоса, ну, нашей функции тоже изменялось сильнее. Если мы хотим сделать так, чтобы этот Лос остался к нулю, и тем самым мы таким образом найдем параметры, чтобы это было так. Ну здесь, совсем не в этом, в том, что Gradient и Boost используют Gradient, прощет, вот здесь, в производах, для того, чтобы оптимизировать себя. То есть он под капотом напрямую воздействует на изменение своего Gradient. Ну, изменение своих параметров через Gradient. Поэтому это используется. Ну, если говорить про Definition, то строится Additive Model, да? Все так же строится, стеклуется модель, друг за другом, да? In the forward stage, white fashion, да? Ну, перед другим, как каскадным образом, да? И это allows the optimization of the arbitrary different differentiable loss functions. То есть, как я говорил, что Gradient и Boost, он позволяет, ну, differential late is gradient. То есть, это позволяет, ну, деференциал late is gradient. Рассчитывать, да, в своей оптимизации, использовать в своей оптимизации деференциал, который у нас вытесняет. И более того, здесь еще говорится, что, видите, arbitrary. То есть, то может работать с задачами и ражирования, да? Есть такие задачи, да? Где не всегда можно взять, вернее, есть такие функции, где не всегда можно взять производку этих функций. То есть, это позволяет, да, в этом случае, вернее, есть такие функции, где не всегда можно взять производку этих функций, да? То есть, некая функция может взять производку этих. Потому что, ну, она должна быть перерывной, да? Ну, и там много других предыдущих. Вот. Поэтому, градент и Boost, он может как раз оптимизацию делать даже разных любых функций. Поэтому это очень хороший плюс. Ну, удачи и ражирования, допустим, это важно, чтобы отсутствовала градент и Boost. Ну, это ладно. Так, ну давайте теперь про... Давайте теперь про SVM быстренько пройдем. SVM это еще короче, это вообще король всех классических моделей. Напор некоторые машины, да? Это метод напорных патеров на русском. Получается задача такая, что я потом выписал самые легкие, жвыжимкие модели. Это мы всегда хотим найти в SVM-е, да? Какую-то оптимальную плоскость, гиперплоскость, линию, либо плоскость, либо гиперплоскость, да, в зависимости от пространства, то есть, по той ли мы работаем. Таким образом, чтобы она разграничивала, да? Разные классы, да? Два класса, три класса, четыре класса, не знаю. С максимальным отступом. То есть, у нас, в основном, грейс точки, да, в пространстве. Классы, да? Примеры. Разных классов. И мы хотим найти такую разделяющую плоскость, чтобы вот этот вот отступ от этой плоскости к ближайшим классам, примерам этого класса и другим, другим примерам класса, другого класса, было максимально. То есть, мы должны найти прям посредине, да, что-то линии вертолеющей, чтобы и это было хорошо, и это хорошо было. То, что мы в предыдущий раз объясняли по клической репрессии, это почти то же самое, но здесь гораздо сложнее. Вот. Это в целом посыл такой. Вот, у margin говорится, вот margin, это distance between the highly-play и ближайшими датапоинтами какого-то потомых классов. И support-ректора, да, то есть, support-ректора. Это, по сути, это по сути те же датапоинты, да, то есть, примеры этих классов, которые являются ближайшими для этой плоскости репрессии. Потому что мы на основе этих точек, опорно, мы как бы опираемся на эти точки, строим эту плоскость, поэтому называется support-ректора. Ну, как бы, точки, вектора, это, по сути, одно и то же почти понятие, просто любая точка по драту, это есть вектор. Она имеет вектор, если она тянется сначала, да? Вот. Здесь понятно всё? Привет, привет. Ну, вроде всё понятно, да. Да. И я хотел ещё объяснить понятие как kernel-трикосвен. Вообще, очень важное понятие. Kernel-трук с ядром. Данные бывают нелинейнероделимы. То есть, почти всегда данные нелинейнероделятся обочинами. Потому что, ну, такое, так и бывает. Вряд ли такое не бывает. Нет, это не класс. Поэтому, чтобы, ну, если простые задачи у нас есть, то да, там, это линии, это линии, это линии, это линии, это линии, это линии. Если простые задачи у нас есть, то да, там, это линии можно разделить, но это очень легко бывает. А если, а зачастую такого не бывает, поэтому нам нужно придумать так, чтобы у нас данные, ну, можно было как-то нелинейнероделить. Потому что трук данного это очень сложно, да. Просто линии разберёшь. Поэтому исполнил kernel-трик. То есть, теперь, что мы делаем? Мы трансформируем наши данные, да, наше какое-то входное пространство данных, преобразовывало в такое, ну, high dimensional feature space, то есть, какое-то высоторазмерное пространство признаков, где эти теперь фичи, которые ранее не были линии разделимые, становятся линии разделимыми в этот пространство. Ну, вот такой like-heart-trick. Вот как это, по сути, делается? Что же самагея такая? Здесь помогают нам kernel-function. То есть, функции, ядерные функции, функции ядра. Ну, они, по сути, нам помогают вот это преобразование, формацию сделать в этих линии и данных. Вот они разные бывают, их очень много, я четыре вымил. Линейный kernel, polynomial kernel, то есть, линейное ядро, полиномяное ядро, полином, да, и RBF это радиальная базисная функция. Ну, там экспонент, в основании стоит. И еще segue-я ядро. Это остальные такие четыре функции, которые используют для того, чтобы преобразовать разные данные в более линейный вид. Вот, пока про это. Также хотел сказать про soft-margin classification. То есть, иногда, как сказать, ввиду различных аномалий, да не только в природе данных, мы не всегда хотим быть такими строгими в плане наших разделяющих линий. Потому что, ну, бывает, как бы, я могу подрадовать один sample неправильным образом, то есть, классифицировать его неправильно, да, по какую-то неправильную категорию. При этом у меня там будут другие две стаданы, они будут правильными. Иногда нужно какое-то дело, так сказать, ослаблять нашу зону разграничения. Аппорт интервал. Честно, это есть soft-margin classification. То есть, когда мы вносим в наше уравнение все параметры, да, все параметры, которые вот controls the trade-off between maximizing margin and minimizing classification error. То есть, это такой промисс, ввиду того, чтобы послабить чуть-чуть наш margin между максимизацией, да, мы изначально говорили, что мы всегда хотим максимизировать отступ от нас, да. Но мы говорим, ну, не снято это хорошо, поддаваем, возьмем все параметры, как регулизация, да, как это. И найдем корпорацию между тем, чтобы максимизировать, конечно, это отступ, но при этом и минимизировать такой сферационной ошибку. А, да, я немного не всем правильно сказал до этого, не 1 класс, может быть, 3, 2, 3 выиграет, а ввиду того, что мы максимизируем, да, цель у нас стоит, максимизирует margin, мы их, да, из-за того, что цель это стоит первое, мы их классификационно ошибку, да, у нас растет, что добавит, нам не нужен такой хороший отступ, максимальный margin, да, но при этом классификация будет, как бы, это будет гораздо ниже. То есть мы будем больше правильных классов встречать, ну, направлять, определять. В всякой практике есть large C и small C, то есть когда у нас большое C, у нас такой очень узкий narrow margin, то есть отступ получен, узкий становится. Раз он маленький, то есть не таким широким, то есть классы они мягко ближе становятся. У нас есть... А, нет, наоборот, наоборот. Когда у нас наоборот, у нас гораздо меньше этих перчей мидс классов. А когда у нас наоборот small c, то есть очень маленький c, он становится шире margin, и у нас... we can more than allow разрешенных мискансификаций. В целом, покажу на примере, как работает. Сейчас на слово будет, конечно. А, ну всё, теория покончена. А теперь делаем... Максик, ты, Максик, лезь. Пожалуйста, Увит, да, играл? Да. Да, да, да. Всё, хорошо. Так, в целом... Да, вопрос бы теперь начали. Вы посмотрели на предыдущие вообще ноутбуки мои? Смотрели? Да, посмотрели. А верхов? Окей. Со средой проблем не было, да? Макеты подняли, питоновские. Среду подняли. Да, нормально. А лично у меня нет. Что еще раз лично у тебя? У вас что? Лично у меня проблем не возникло. А, понял. Шанул, Эйпл, да. Что говорите? Да, в целом хотел бы начать вот с действия 3, да? То есть древи решений. Эпи, кинни, пиди, джинни, пиди, твей, информацион, гейм. В целом, ну запуткаем просто. Скалерд, также скалердом и вытаскиваем. Лоу и Арис, да? Арис это гуляем датасет. Цвета, если я ошибаюсь. Ну цветы, стенки. Там около 100 с лишним примеров. 3 класса. Это настоящий датасет, который был собран лет 50-х дат. Иптобиологами. И там по сути, разграничение типов растения по ширине его лепестка. Ширине, толщине там лепестка, стебель, что-то такое. То есть посмотрите, допустим. Ну, по-любому, наверное, слышали, кто вот. Вот, все же, да? Вот это по сути, видите, вот один пример, да? 6,1 это какая-то ширина чего-то, длина чего-то, ширина чего-то, длина чего-то. То ли стебеля, то ли лепестка. Ну, неважно. По сути, по этим признакам мы можем посетировать их как равный масс. Допустим, да? Лепест. Вот. Вост. Вост. Вост. Первый класс, это, кажется, Сетоса называется. Он имеет вот такую, ну, размерность его, смотри, пометили своего вот, растения. Вот. Ну, и давайте лепест попробуем, да? Вост надать до того, что мы делали, да? Да, лепест. Так что у нас есть терапия, да? Вот формула, когда я говорил ранее. Я как-то не буду это снимать, потому что мне творились все уже. Просто чтобы под рукой было, я здесь все отписал. Окей. Давайте теперь, я хотел бы здесь вот построить собственно через скейлер, да? Само такое, наше decision tree classifier. Это же классификатор, да? На основе решения. Опустил. Ну, вот, по сути, так интеллигируется, да? То есть я создаю два дерева. Для интереса, да? Одно дерево на основе критерии Gini-Purity, да? Как я говорил ранее. А другое дерево на основе Этерапии, да? Вот. Вот такой у нас уже класс, decision tree classifier, да? Скейлер готов. И в целом, смотрите, я помимо критериона, да? Критерия нечистоты, да? Не полноты, да? Не частоты. Я добавляю максимальную глубину дерева. То есть, вам я говорил, да, что мы можем контролировать, сколько дерева может остаться в глубину. Я говорю, три. С максимальной на три. И ранос 42, но, по-моему, тебя что-то было. Окей, я их обучаю. Вот fit-fit. Это p-fit, Gini-fit. Делал. Далее. Вот plot-che просто. И еще здесь вот такая сфутка, я говорю, важно, plot-tree. Это такая вшитая формула. Вот эта вот сфотография, которая тут есть, ну, диаграмма. Которая, мое дерево, симатично, диаграммно, да? Исписывает вот такое дерево, по сути, называется. Добавляет здесь фичи имена, таргеты имена, да, ну и всякую вот неважную вещь. Вот, ну, давайте посмотрим. Первая диаграмма, это у нас все видно, да? Нормально видно? Да, видно. Да, видно. Отлично видно. Ага. Окей. Надо в общем-то использовать стены. Да. Во. В целом, смотрите, вот, как я говорил ранее, у нас есть вот root, то есть коринга нашего дерева сверху, и мы в ней мы не садимся, да, поняв? Ну и вот, да, видите, получается его изначальная тропия, да, здесь? Вот до деления, да? 1.585. Да, это пентропия дерева. То есть вот этот вот, вот этот досад, который я скормил, да, у него, он до деления имеет один пентропии, да, и полноту, да, разброса, да, внутри себя. 1.585. Соответственно, мы говорим, что вот, и так говоришь, что в этом участке, да, до деления у нас 120 120 сэмплов. Вот, например, 1-го, 0-го класса, 41-го класса, 3-е, 4-е, количество 2-го класса. И большинство класса это версии палор, ну, это тип растения, это кажется, вот этот, ну, его максимально выглядит. Соответственно, моделька нашла лучший информационный гейн, и она определила, что лучший информационный гейн это будет делить на petal lengths. Petal lengths это, ну, кажется, там из тех четырех параметров, это какой-то из них параметров. То есть, видите, есть petal lengths, есть petal widths, есть ааа... что еще здесь? Ну, и а вот здесь пишется. А еще сэппл, ну, окей, они варят. И смотрите, мы разделили на 245, то есть мы говорим, если, типа, пример, да, имеет petal lengths меньше или равно 245, то кидаем его сюда, в это место. Ну, не дерево это уже есть стало. Иначе, да, то сюда. Вот посмотрите, он говорит, да, меньше или равно 245, petal lengths, цвета, то мы кидаем его сюда. И, видите, сюда, смотрите, в этом вранче, да, интерпьер на нулю. Это значит, что у нас все, кто фильтровался сюда, они все одного паса. Видите, здесь 40 примеров, классы все и ноль других. Ноль прести-калора и ноль гидженики. Класс. Здесь мы понимаем, что у 40 сэпплов, из них все 41 вида, мы понимаем, что это дерево максимально чистое. Ну, pure, да. Понимаете, да, теперь что такое pure? Вот. И это круто, потому что, ну, почему это так? Потому что, видите, ну, я это посфактом показываю, потому что она под капотом там все поковыряла, пэтлленс, пэтлленс, там, пэтлвитс, там, сэпплленс. Это я такое, да, 4 признака разные поковыряла, и это она лучше показывала уже. Весно, в этой конфигурации, да, вот здесь информационный гейн гарантированно выше, да, вот в этом отделении, да, чем если бы здесь был другой правило, там, вместо пэтлленс было пэтлвитс или, понял, сэпплленс или сэпплвитс. У пэтлленс он дает гораздо лучше информационный гейн, потому что, видите, уже на первом же отделении уже интерпет 0. А вы помните, да, что информационный гейн он, ну, если интерпет маленький, информационный гейн как он называется, ну, значимый, значит, мы больше добрали, да, мы, видите, мы его торпили по неделю. В этом отделении на интерпет 0. А оно само поняло, по какому, типа, пэтлленс или пэтлвитс? Ну, да, это вот здесь вот в капоту фидна был, фидна жали. Ну, как бы, то, что я показывал, помните, вот, вот здесь, да? Вот алгоритм. Ну, он, по сути, вот используется. Он все четыре варианта, все четыре фичи посмотрел, прогнал, информационный гейн максимально взял. В соответствии, в данном случае я вижу, что информационный гейн максимально в этом ретиллере, когда пэтлленс выбрали его как фичу, а это просто в среде. То есть, вот это тоже не был комбинат, это было в среде, в<|tl|>, в<|tl|>. То есть, он просто взял среднюю пэтлленс по всем примерам и взял его как основу. И увидите, что, видите, вот уже на первом разделении, да, мы уже грамотно отделили целый класс. То есть, по сути, всего лишь одним правилом, да, прикиньте, мы 120 примеров взяли, и мы уже 40 правильных классов полностью определили одним только правилом. И поэтому это, то есть, всего лишь одним разделением, да, мы могли сделать. Ну, это круто. Давайте дальше пойдем. Окей, что вот если false, да, ну окей, будет больше, чем 245. Им придумал еще одно правило. И то же самое опять интеративно мы повторяли, как мы здесь повторяли. Мы для всех оставшиеся правила, которые остались, да, вот это уже выскучается. А, нет, это не исключение, это оно тоже выходит. Я ошибся. Мы опять прогадаем тоже алгоритм. Опять ищем максимальное правило такое, чтобы информационный гейн был максимальным, да. Вот. Ну, вот это почитание трапеи, да, и сравнивали, и отняли, да, ее от вот этого суммира, этого суммированного, взвешенного. И вышла вот эта информационная гейн. Вот. И видим, что здесь трапея вот в этом, вот там, от диадерка. Здесь уже видите, сэмс 80, да, ноди, 40 сюда, а следующий 40, вот здесь, сюда дошли. Далее, спрашиваем, да, ну, здесь мы стоим и спрашиваем, если, ну, если у нас есть правило, то что ну, максимально информационный гейн. Если оно меньше, чем 475, опять, видите, повторяем правило, ну, видимо, оно лучше разделяет, просто другой мер взял. Уже с средненными значениями, да, с этими средненными значениями. Потому что эти ушли, видите, она увеличилась, ну, в среде нет другой стали. Мы пошли дальше, true, меньше, да, примеры меньше, чем 475. Вот, зашли сюда и видим здесь тоже, видите, смотрите, здесь тоже энтропия упала, да, она маленькая стала вообще, но она еще составляет 9. И мы видим, да, что по сути здесь тоже, да, энтропия маленькая, потому что у нас, смотрите, у нас классов 36 версий color, да, и один всего единика. То есть, по сути, почти вот этот вот нода, да, она почти чистая вся. То есть, мы вот этими двумя правилами сделали так, чтобы сюда определить версий color почти полностью правильно, ну, сюда ложно, ложно попало, да, как бы новый, в единик. Но это не проблема, потому что мы для этого случая еще придумали еще, да, чтобы это разделить, еще чище сделать. Мы разделили сюда еще по этому признанку, 165, по этому width уже, да. Если меньше или равно, то сюда, и здесь уже, видите, число 36 остается, и полностью версий color один. Нолевой энтропии, вообще ничего нет лишнего. Ну, вот, соответственно, сюда заходит в единика, да, как иначе просто false. Ну, и как бы такой alt-layer, вот этот вот пример в единике, видимо, какой-то был слегка аномальный. То есть, в этом растении именно вот этого случая, она была какие-то, видимо, ну, обнормальна в своей своем виде. Возможно, это просто ошибка кто, тех, кто собирал датасет. Ну, никто не знает. Ну, потому что, видите, здесь повредено, что сюда открывают 34, да, их штук, а сюда отвергают, странно. Ну, окей, с этим можно жить, ладно. Возвращаемся сюда, да, дальше, на apparent выше. Возвращаемся к тех, кто false, то есть, больше чем 4.75, заходит сюда, история энтропии понижается, как-то, видим, что здесь в основном в единике отцелилось. Вот эти пять какие-то, они в версии color одну странно зашли. Возможно, не аномальный. Придумаю правила, меньше чем 1.75, если да, то сюда заходит, если больше, сюда заходит. И, соответственно, все у нас, по сути, раз, два, три, четыре и пять. У нас пять лестников остаются в станции. По которым мы можем с точностью там, сколько, с аккуратностью вводить 100% всех адмитровых правил. Вот, а здесь то же самое, только через коэффициент, ой, через Gini Empirity, да, видите, здесь стоит тропия, стоит Gini, и то же самое. Ну, она также работает, просто будет, или информация где-то вычитается, но Gini подголовно считает, и все. Вопросы есть? Вопрос, извиняюсь. Да. У нас же это depth speed было, да? Глубина 3? Да, 3, 3 было. И мы остановились там, там, где где-то, тропия была 1.0 и 0.187, да, на правой стороне. Если бы мы дали еще больше глубину, мы могли бы еще меньше тропию получить. Да, да, да, да, все верно. Overfitting, да, кажется, произошло вот так. Никто не знает, потому что этот сет очень маленький, понимаете, если у нас всего 120 примеров, если бы еще было какие-то, еще проверить этот сет, то там было понятно. Ну, как бы да, в целом все верно. Здесь вот этот, мы стопанули, да, на не нулевые тропии, потому что дальше есть потенциал, видите, двигаться. Но мы не сделали, потому что у нас 3, да, max depth size, 3 стейки. Все верно. То есть я могу предполагать, что вот эти, которые оставшиеся, 4 сэмпла там осталось, да, и вот здесь один сэмпл не подходит куда-то вверх в руку, они какие-то outliers опять, типа обязательно outliers. Можно, можно, так можно сказать, да, это можно сказать. Скорее всего они, да, либо либо они outliers, да, либо они аномалии, либо это просто когда ошибка в данных. То есть кто-то неправильно разметил. Возможно, это вот этот один пример, это не версиклор, да, ой, это не да, это не версиклор, а в самом деле это вертиник, который и 35 был, по идее, идеален. Ну, тут ошибся и поставил лейбл, да, версиклор. Спасибо. В целом понятно, да, как будто бы? Я думаю. Да. Хорошо. Можно у тебя что-то. Смотрите, у нас еще не пример такой хороший, но он такой длинный, ну, чуть-чуть такой трикки, но постараюсь объяснить. Просто смотрите, я когда аплодую этот ноутбук, пожалуйста, постарайтесь прям его под лупой, да, не знаю, изучить. И просто тогда, да, да, да, понял, а вот каждый момент в коде понять, а почему, собственно, он это написал, от чего это он сделал. То есть все, что я писал, это понимаете, любую вещь, да, мы хорошо обучите, понять, если вы будете сразу на ракурс, скорее всего, это поднимете на нее. Я пытался привести какие-то примеры, да, экзампы, чтобы максимально сразу ракурсов раскрыть эту тему, нелегкую, нелегкие тему, чтобы получить максимально встание по этим. Поэтому рекомендую прям хорошо сесть, посмотреть, что это. Окей, держусь дальше. Смотрите, запускаю это сел, давайте посмотрим. То же самое, load.edit, asset, ну, здесь еще, видите, помимо того, что я сам написал функции, теперь просто вам следа, что нужно пытаться самим все так писать. Можете посмотреть и согласиться, что это правильная реализация формулы сферы, для энтропии и для гениапилотики. Это просто метод для оптимизации, ну, для регуляции создания датасета, подгрузка на таргет, разделение x, y. Далее первый пример. Давайте посчитаем энтропию и гениапилотики для всего датасета, сам в начале, до сплита еще. Вот посчитали, давайте посмотрим. 1.58 на 6.67. Видите, да, это? Давайте вернем, это мы вручную посчитали, верно? Согласны? Я вручную посчитал. Вот. Давайте теперь вернемся к тому, что автоматически было посчитано, да, вот здесь, смотрите. Вот это вот устроилось уже в том числе, это уже считал. И видим, да, что для энтропии, энтропия в начале сам, она равна 1.585. Смотрим сюда, то, что вручную посчитано, 1.585. Все корректно, это начально посчитано, до сплита, да, еще. Теперь про гени. Гени ручным образом был 0.6667, и смотрим автоматом, да, 0.6667. Ну, что-то так оруглядились они. Все верно. Значит, мы энтропии посчитали верно. Формула работает верно. Первый-перем понятие. Давайте второй пример смотрим. Для каждого сабсета, да, определенного, 50 саблов, в которых все ситуации, да. То есть, видите, датасет устроен, ну, давайте покажу здесь, чтобы понятно было. Датасет, вот это iris, да, датасет, он устроен в кеове, потому что, смотрите, y возьму, да, это все наши лейблы, да, нулевой это какой-то лейбл, лейбл-это какой-то лейбл, и второй это еще лейбл. Они устроены так, что они просто по порядку состоят. То есть они перемешаны, они просто, то, что первые 50 или 40, это чисто там, нулевой класс дальше дотуда, первые там, до 50 это до 100, еще какой-то класс, и до конца это второй класс. То есть, то, что я делаю в другом примере, я беру, смотрите, я беру все 51, то есть все ситуации, классы, все нули, и считаю для него это пиле. Кто может сказать, какое будет, думаете, результат? Ноль должен быть, если они все одного класса. Да, ноль будет. Все верно. Да, ноль. Ну, вот. Все верно, да, смотрите, ноль уже. Ну, здесь минус, потому что нам видимо какая-то чистотип, какие-то оптимизации на компьютере, не смог вот зайти. Ну, это ноль, если его срезать. Ну, правильно, окей. Ну, неплохо. Давайте посмотрим третий пример. Уже, смотрите, два 2, теперь 25 класса по 2 классам. То есть, я смешал 25 2 и 25 2. И посчитал. Есть понимание, какое будет примерно? 0,5 1 0,5 что, а 1 что? Энтропия. Энтропия 1. Энтропия 1, а GenieMap 0,5 должен быть. Все верно, отлично. А знаете еще, как можно интерпретировать это? Все отлично, классно. По формуле, да, ориентировать или по другому? По предыдущему примеру. Это кто говорит? Рослан Есина. Ага, Рослан. По предыдущему примеру. А что именно? По предыдущему, имеется в виду, где более нарисована полностью. Ее по классу раскидывали. Энтропия. А, все, все, запомнил хорошо. Вообще еще, может, где догадаться еще вообще. Во-первых, я вначале говорю, что энтропия это количество битов, через которые можно выразить систему. Посмотрите, так как у нас получается равное количество и того и того, то есть это как подбрасывать монетку, верно же? Опасть на то и на то. Шансов 50%. То есть, смотрите, ну, типа орел или решка, да? То есть, естественно, я могу всего лишь 1-бит информацию, да? Вот это состояние передать, либо 0, либо 1. Вот. Ну, это вот, поэтому 1-бит информация. А в одном бите, да, это 2 комбинации, да, могут либо 0, либо 1. А это типа получается 2 степени 0, 2 степени и минус 1, 2 степени и минус 2, если с гитами. Ну, типа того, да. Ну, как бы здесь вся суть в том, что это как аналогия подбрасывая монетки, да. У нас монетка тоже, да, у нас вот одна монетка есть, у нас 2 исхода быть, да, одна сторона такая, одна сторона такая. Если в энтропии посчитается монетки, да, подбрасывается система монетки, то и аддегатерка тоже будет. Почему? Потому что вы можете, а всего одним битом информации, 1-бит формации это 2 стейли, верно, либо 1, либо 1. Вы можете одним битом описать систему полностью. Верно? Получается, если энтропия 2, то 4 варианта. Энтропия 2 не бывает. Да, да. Ну, уловили суть, как бы окей, я рад, что вот здесь есть понимание, это круто. Просто в страсте всегда какой-то еще параллель до полностью выстраиваю, потому что вот эти термины, их можно, я даже вам могу сказать, что эти вещи можно еще там с других фрагментов объяснить. С научным, с физикой какой-то там, с химией, я не знаю, и они будут в принципе биться между собой. То есть есть какая-то параллель прокладывается, прокладывается идея. Так, давайте четвертый пример. Информационный гейн и инфоквейтн. Да, ну вот, да, здесь просто я реализовал информационный гейн на то, как считать. Ну, как видите, мы теперь считаем количество left child, right child, parent child примеров, sample, пока остается там. Дальше считаем через импьюрити функцию, определенную импьюрити функцию или эторапия, либо джинни. И считаем. И все, вот weighted child, impurity, то есть количество этих примеров делить на общее количество, делить на общее количество нужно его суммировать. И далее parent, impurity отнять weighted child, impurity. И все, это есть несложненький. Ничего сложного. Помните, парень на твое твое твое твое просто давал, как формула будет считается? Вот понятно сейчас? И все, здесь, конечно, еще. Понял. Я посмотрю потом. Окей. Ну вот, вот, в принципе. Теперь давайте посчитаем, собственно, информационный гейн, да, в системе. Вот на примере того, что мы тоже считаем, помните, да? Вот, 245, да? Petal length 245. Оттечение правил оттечения. Вот. Ну, посчитали, да? Задали вопрос, вот, типа, Petal length, это те индексы, которые Petal length меньше равно 245, и write индексы это те, которые больше 245. Наложили на эти y, вытащили их реактические значения, лейблы этих y'ов, и дальше пишем информационный гейн. Видите, что информационный гейн равен 0.9183 и 0.333. Вот, я показал бы здесь, но я здесь не вывел его, здесь не так выводите, здесь показывают терапию информационный гейн этого сплита. Вот, ну, здесь он будет информационный, типа, гейн, стоит там, сколько мы там посчитаем. И вот его. Так, мы закончили дирею, давайте в ансамбле дальше, и потом в SV. Вопросы какие-то есть? Чем больше информационный гейн, тем лучше, ведь, вот так или? Да-да, чем больше информационный гейн на каком-то сплите, тем лучше сплит, и мы его берем дальше. Давайте поговорим теперь про ансамбль. Ага. А края нет у информационного гейна максимального значения или есть? Ну, так информационный гейн зависит от энтропии не гени, а гени и энтропии, они, их есть рейдш, скорее всего, информационный гейн тоже есть рейдш. Но, в принципе, там, как я понимаю, там зависит от цепи уже количество. Поэтому, скорее всего, его нет, его нет. Вот. Вот, ансамблирование. Тут тоже формулы, посмотрите, такие сложные они, в принципе, легкие. Вот. Давайте быстренько пойдем в бэггинг, да, random forest. Вот, все по классике, NumPy, SkyLearn, Pandas. Дальше я использую предыдущий для последнего, помню, 40 грудей. XY, target data, splitch на train test, с 02 для теста и 80% для трейна. Вот. Строю дерево. Строю дерево, random forest pacifier, количество 100, 100. И состояние срогла, ну, random. Вот. Дальше я обучаю, статую объект, потом дальше обучаю, дальше делаю предикты, калитацию, ну, то есть, и вычитываю accuracy score. Дальше почитываю классифика<|tr|>, что репорт, а не покорю позже, вот, страю щеледину, наверное. Дальше строю кафешу матрицы, я уже снял. Вот. Ну, пока посмотрим. Только с навариванием. Ага. Да, accuracy, смотрите, отличная наваривация, 6,5% классифика, что репорт. Вы посмотрели, да, поняли, да? Помните, да, с предыдущей Precision Recall метрики? Поняли? Да, да. Ага. Здесь просто не будет, наверное, да? Пока на это можете забить, на эти метрики, в принципе, пока что не нужны, на саппорту всякие, пока только утренки. Окей. Ну, и смотрим, что у нас там, как же лакость, мы понимаем, что она хорошо работает, это высокая, это низкая, низкая, ну, нормально. Давайте посмотрим теперь про кросс-валидацию. Вот, помните, да, я говорил про кросс-валидацию, да, что это такая штука, которая очень эффективно делает 5 типов вариантов разных сплитов дедосетов, и обучает 5 разных моделей на каждой из их 5 кросс-валида... дедосетов. Вот, она их обучила, то есть она 5 раз обучила 5 разных рандофоресов, разных сплитов вариантов дедосетов. Потом усредили, да, и вывели. Допустим, вот смотрите, вот это строка, это вот такое. Вот. Косс-валидацию. Видите, вот раз модель, да, score, accuracy, 2 модель score, 3 модель, видите, хорошо вышла. 4 модель и 5 модель. Теперь вы понимаете, да, почему кросс-валидация очень интересная штука, потому что, видите, если я сплитнул бы вот так вот, да, у меня было 92% всего, да, я дуло 92%. А в другой день я мог сплитнуть вот так, 98%. Потому что какой-то кейс в палдавбидж. И, видите, когда мы это усредили, мы приходим к какому-то нормальному облечению, объективу, да, 25 и 6. Поэтому, да, это важно, да, кросс-валидацию всегда. Точно, точно. Честно, если я сделаю не 5, а 20 фолдов, да, оно будет почти таким же, но будет еще точнее, да, как у больших кисел. Далее, смотрите, про research. А, в той research я набрал еще feature importance. В второй раз сделал feature importance, то есть я получил модель, сделал feature importance, запихал в Panda Series, да, и просто записал, ну, типа, после того, как я в рандофорс выйду, вывел фичи. И, видите, вот эти фичи, они очень сильные, это слабые. То есть вот так вот, Panda Force тоже может помогать в определении признаков. Это очень классная штука, видите, ну, то есть я взял в будущем только эти модели, фичи взял для моделей, все эти даже не использовался, они вообще не вычисляли. Далее говорим про tuning hyperparameter. Смотрите, видите, вообще модели, да, логического мало, они часто будут творчаться, то есть перебирают разные параметры, потому что, ну, нужно количество деревьев, да, почти это разное, 100-200 деревьев, 300 деревьев, потом глубину, да, разую, 3 в подфокус, 10-20-30, потом минимальный sample split, да, сколько должно быть, да, минимум сплетения, а нет, это было лифт 1 на 4, и целый раз это я не помню, 2,5, что было, а, количество сплетения, потом минимально, да. Видите, комбинации у нас сколько? 3 на 4, 12, 12 на 3, 3,6, 3,6 на 3, это у нас 108, да, видите, у нас в этом param grid у нас 108 разных комбинаций случайного леса. Леса мы, ну, берем, вот такую штуку, randomizer search, напускаем наш классификатор, тазусовываем, передаем параметры наши, и стартуем. Договорим еще, что там будет 5 cross-parameter, то есть 108 на 5 сколько это будет у нас? 540, да, получается. То есть мы 540 раз проинтерьируемся по этому датасету самым с запрещением, сразу и варианта. Делаем обучение, и находим лучшие параметры с этого списка комбинаций. Ну, и смотрите, что выходит. Извиняюсь, откуда было число 540? Это 108? Потом, здесь 5, 5 моделей, да? 108 на 5 это 500? Если не ошибаюсь. Понятно? То есть у нас здесь 108 комбинаций разных моделей, а тут еще 5 cross-validation. То есть мы на каждой из этих моделей даже сделаем 5 cross-validation. Здесь 108 разных моделей, а здесь еще 100. С этим 540 у нас в совокупности разных моделей с учетом разных порций датасепы, комбинаций датасепы. Да, понял, спасибо. Обучаем, это на моем комплекте задел немного, возможно, на других может быть компакт чуть-чуть, пробности будут, ну, с пресетером. И она обучилась и вывела best-параметры, да? Вот. Вот best-параметры found. 300 короче деревьев, прикиньте, вместо стада, который в начале стоял. 2 сприт максимальные, мецеплит 30 и спанк 30. Губокое дерево. Вот. Ну, то есть неплохо. Видите, 96, 49 было, да? Стало на best. 95. То есть, нравится есть. Мы нашли хорошую разницу. Вот. По сути, что делаем дальше? А, ну, все, теперь плотим learning curve. Такая тоже штука интересная, по сути. Ну, это просто, ничего такого не происходит. Мы просто передаем нашу лучшую модель bestRF random forest learning curve метод, который нам по красоте, видите, по теку, вырисовывает наш динамический, вырисовывает то, как наша модель имеет потенциал для обучения. Что это значит, это значит следующая. Вот видите, график, да? Она показала нашу лучшую модель, да? Как она обучается. Пока она красно заметили, это тренировочный score. Вот тренировочный аккаунт. Смотрите на cross-valdation score. То есть, она говорит, когда твоя модель видит x количество samples, то есть, я посмотрел 100, у нее, типа, accuracy 0.9. 300, да, вот столько. 400 вот столько. То есть, сколько она, например, посмотрела во время обучения, плотила, она вот так вот улучшается. То есть, то есть, смотрите, мы видим здесь, на этом графике, да, какой потенциал. Это очень-очень важный самый график, полезный для инсайта, да? То есть, вы можете видеть потенциал данных для обучения. То есть, возможно, да, будет такое, чтобы данных было больше, там, до 2000, да? Возможно, мне не нужно 2000 данных, потому что у меня она 400, исполнение мгакерства такое же, как на 5-6000 штук. Видим, да, уже сатурация, да? Она затухает. Все, вот так тихо. Еще хотел бы сказать, видите, вот этот вид, да, такой, слабо-зеленый, да, видный, вот чтоб, да, в долей линии, да, идет такой ангелоп. Это наши, наши, вот это четкая линия, да, темная, это наша усредненная кросс-валидационная скорость, да, средняя. Из тех пяти, да, кросс-валидацией. А вот эти все точки, это наш максимальный разброс, да, стандартных пленей для наших этих скоров. А же пяти, вот в этой позиции, да, на 200 примеров, среди 5 accuracy, да, у нас такой разброс. Вот. Понятно здесь? Да, признательно. Окей. Существует то, что из под капотов, из календарных функции существует. Просто вот. Если я вот у меня это описал, что зашел, значит. Окей. Мы, кажется, закончили с этими, да? Да, с этими. Байкингом спереди бустик. То есть формулы. Давайте посмотрим. Ада буст. Так. Так, так, так. Все то же самое. Брест, катор, добасет. Рей. Создаем. Создаем. Да. Помните, я говорил, что ада буст, это бустиковый алгоритм, который использует в себе слабые лернеры. Эти слабые лерны выступают как вот бейз-естиматоры. Это есть обычная decision tree, обычно. Губяной один. Вообще простой, босой астиматор. Пословом сюда. Ада буст и говорим, пусть таких, что будут 50. Вот это играет один, то есть степ обучения один. И все, было рано встану. Обучаем. Предиктим. Это prediction, это предикт-пропот. Это уже как бы вероятности. Вот. Вернее, из вероятности в лейблевые министры. Теперь validation происходит. Аккуратный репорт. Это фига, репорт. Видим, что ада буст обучился на 97.37. Видим, что он просто еще участвовал. Потому что мы на 97.37 не били ни разу по отбросам агентства. Мы все же. Вот это вот. Примерно классический, как это работает. Еще хотел attention поймать на Rocko-ooker. Про ей подробно мы поговорим позже. Пока просто в голове видите, что Rocko-ooker это очень крутая. Это вообще требуется отдельный урок, на самом деле. Рассказать максимально, что такое Rocko-ooker. Это тоже одна из метрик для классификации. Которая помогает нам понять в совокупности, как модель исполняет вне зависимости от какого-либо порога. От течения. Помните, что вот этот classification-репорт он строится на рейтинге. Как он считается? Он считается просто, если скору выше чем 0.5, это одерка, если скору ниже, то это 0. Здесь мы видим только с учетом порога 0.5. Это как бы прикольно, но в целом это и так хорошо, потому что вы всегда понимаете, какую порогу вам нужен. 0.5 это просто default. Возможно вам нужен порог до 0.7. Вы не знаете, как это работает. Рока-лук показывает совокупности все пороги. Поэтому Рока-лук очень классно раскрывает большие сути своей репетации. Он вне зависимости от порога может усредить на все пороги. И вот так это выглядит. Так строится через рапкер готового функции. Называется Ytest. Тут значение Y, Yпредикты вашей модели. Возвращается в floatPositFORAIDs и intPositFORAIDs. И в сути, с помощью TP-RFQR мы строим ареи от рапкера футографика. Торючей это функция. Дальше ее плотим. И по сути, мы видим что здесь очень важно. То, что мы сейчас видим, это Рока-лук-кер. Это квадрат единичный квадрат. У нас по оси XY 1 развертость. Вся площадь этого квадрата равна от дырки. Максимальная. Мы всегда хотим, чтобы Рока-лук не был такой как это. Рок-кер, оранжевый линей, который стремится отсюда нижнего квадрата вверх и направо. Мы сейчас сходим, чтобы она была вот так вырисована, чтобы она покрывала все. Почему это так? Потому что, смотрите, если вы посмотрите на X-оси, у нас лежит наш falsePositFORAID. То есть рейт того, как рейт – это положительный рейт наш. Скорость – это положительная. Мы часто говорим о дёргах от этой инфекции. А по Y стоит TMPR. TMPR – это хорошая метрика, которую мы хотим подняться. То есть насколько мы всегда дёргаем, я не оказываю правильными дёргами от класса. Мы всегда хотим, чтобы это было максимально, а это было на 0. Но тут погрешность, здесь какие-то есть косяки классы, которые не полностью открываются. В один случай будет только квадрат. Обычно это бывает вот такая линия, она вот так вот идет криво-криво, классы вот так вот. Такого никогда не бывает. А вот эта пунктирная линия, синяя, если идейка, что это такое? Что это может быть? Рандом yes. Кто сказал? Нима. Нима сказал классно. Правильно. Это линия родного. Опять-таки, мы говорим про бинарную классификацию. Это по сути случай, когда мы делаем классификацию, и мы делаем классификацию, в случае бинарной классификации опять-таки в этой линии парадон. По сути, если наша линия, не дай бог, идет в то или иное, или по нему, по этой линии пунктирной, то значит, наша модель работает как парадон на этой. Она ничего не знает. Она просто говорит 0.1, 0.1, 0, без райдера, ей пофиг. Она равномерна до этой 0.1, она немножко вся подряд. Просто равномерна. Интересно. Второе вопрос. Может, Зайди и Диме, и другим тоже. А что, если у нас график вот так идет? Вот так. Что это значит? Все время ошибается, нет? То, что научится. Смотри, еще раз, еще раз вот так вот стрелка видна стрелку она вот так вот так что это значит экспоненциально ну как вот так поэкспоненциально что значит эти больше чем true позитив или больше да типа будет как бы как сейчас почти такой же только на бразилька на сюда представь что у меня вот так наверное потому что это либо не до обучения показывает либо просто модель подобрана вот так вот это в принципе реальный вор это риел вор сценария если низу с позитив позитив растет при этом true позитив практически не растет то есть типа мы говорим вместо единички всегда нулик сейчас полс не наоборот мы говорим единичку там где нулик ну типа это что же ну типа самое плохое так это 05 если мы типа имеем accuracy 0 можно просто обратно делать будет accuracy 1 такого нет у модели что-то получилось просто она видимо просто зеркалит она видимо ошибка в коде это вы перепутали label и вы просто там то есть модель она что-то понимает она и рандомно делать она ряночка дев просто вам нужно нулики на единиц представить тогда она вот эта штука обратиться вот эту штуку ты самый худший сценарий это это не доделать это вот это снаряд самочи хуже него нет короче а вот это значит у вас модель получает если у вас все корректно конечно если у вас ошибок это в коде значит у вас модель наоборот делается то есть она на нолики намеренно говорит ну на истинной нолики она говорит намеренно единиц она истинная дёргая она говорит нолики то есть она полностью паранапросы делает вот возможно какой-то какой-то здесь по двух есть хорошо так но то же самое что и та да да мы зафиговали у нас времени мало остается тоже и мы Давайте пойдем теперь к Gradient Boosting по сути. В целом тоже Gradient Boosting теперь. Breast Cancer, Take a Pay, Extrain, Extest. Gradient Boosting классифар строим. Дефолтные параметры строим. Ну и выводим, вернее, в дефолтные параметры. Так, эту штуку можно убрать. Вот. Да, как бы. Обучили модель, делали предикты, посчитали accuracy score, классификационный репорт посчитали, построили coefficient matrix вот так что. Делали для грейд-черча параметры, делали грейд-черч, обучили грейд-черчам, виталировали 5 параметров. Ну, берем лучший модель, предикты и показываем. Все то же самое. По сути, это что мы поговорили просто, теперь на Gradient Boosting. Вот. Да. Вот, да, пойдем. That's where, потому что, мне кажется, лучше на этом нанеслоксируйся. Здесь тоже самое, чтобы это было обсуждать. Вот, Models.Reformats. SkillBit.Use.TheModels. То есть сколько обучалось по времени и по многократии. Здесь, в принципе, можно посмотреть сами. Это не сложно. Так, смотрите еще. Давайте посмотрим, как выглядит Станина. Мне кажется, он тоже не просто убил. А, ну это окей. Это тоже можно дома посмотреть. Здесь очень много примеров придумал. Это может быть дома. Давайте рассмотрим. Здесь, мне кажется, интереснее будет. Вот, смотрите. Знаете, я с вами формулы. Все верно. Все просто. Не так уж и сложно. Вот, почитать. Это идеальный. Это кернальный ядронет для перестройки линейного пространства. Будет высшего порядка. И все. Давайте рассмотрим. Теперь я допустим. Буду вам советовать, что собственно такое. Смотрите, я здесь для примера сделал такой хороший пример, интересный. Я не делался, я подвели в интернете. Создал датасет. Какой искусственный датасет создал. Да, MakeMode. Идеально, знаете? Знаете, типа. Вот. Ну, достаточно сложно, да, Суркуля, смотрите. Да? Такая типа лунная, типа, да? Идеально. Очень сложно, да? Как бы ее как-то разделить. То есть, я не знаю, логически регрессия как бы справится. Эсвен справится. Бутик, кажется, справится. Здесь просто MeshGrid это функция для графика. Это просто... Доверьтесь мне. Вот видите, просто это для дефолта. Здесь не надо много париться. Под контуре тоже для плотинга. То есть, это просто помогательная функция. С<|el|>.тоддую датасет. Сманичу. Скалирую. С.тоддую скейлер свой. Для того, чтобы... Я вообще-то почему? Для того, чтобы заскалироваться. Вот. С.тоддую, видите, на экстрене. С.тоддую скейлер. То есть, я просто заскалировал. С.тоддую, значит, новую версию. Тоже самое для теста. Напомню? Хорошо. С.тоддую свои конфиги. Для ядер линейный, полинов и rbf. Радио-бейдер. Базисовый банк. Сигнотик, что бы. Я не сигнотик. И еще сивалист. Сивалист, помните, это тесьмина с порога. Не терпит, правда, это? Компромисс между тем, чтобы... Маргин, да, свой не завышать максимально. Отстоп между классами. И еще конфигурационная шипка. И нерадец, если спросили. От 0 до 10. Это просто отлот для этого. Посмотрите, вот здесь самое особенное. В этом for-word. Смотрите. Я беру для каждой конфигурации ядра функции. Вот этой. Пробегаюсь. И для каждой еще конфигурации сивалист сделаю следующее. У меня это вот сколько? 3 на 3? 9 проходов парагонов. Я создаю классификатор. Svm. И там support-vector. Свм.svc. Указываю ядро. Указываю конфиг параметра. Ну, вот, soft-margin классификатор. И свой. Feature на скодированном. Внимание, смотрите, на скодированном. Все feature. Додицепт мой оригинальный. Просто скодированный. Вот, потом я делаю предик. Опять-таки на test-scale это тоже. Считаю accuracy-score. Precision-score и obscure. Вот. Далее. Смотрите, я накладываю. Создаю токлопеременные. Передаю вот сюда переменные для mesh-grid, чтобы построить плоскость. Мешек из творца, кто знает, передаю свои скаляры. Вот. Что дальше? Дальше идет следующее. Я их просто bloat'ю. Почему же я так делаю? А, это следующий пример. Здесь мы проще. Здесь просто пример. Смотрите, мы увидели следующий. У нас есть две идеи графиков. Первая строка, это у нас три колонки. Это у нас линейная. Линейная с 0.1, линейная с C1, линейная с C10. Accuracy, precision, recall. Видим, что линейная не исправляется вовсе. Это очевидно, что мы видим, что линейная разделяющая не хватает ей мозгов. Комплексности у модели для того, чтобы нормально разделить для класса эти. Видим, что линейное ядро не помогает. Смотрим по лином. По лином тоже лучше, но по лином тоже не исправляется. Пытается обогнать эту штуку, но не получается. Смотрим теперь RBIF. Смотрите. Вышло чудесно, по-моему. Он смог какую-то выучить сложную функцию, экспедиенциальную функцию, которая могла плохо обогнуть данный. При этом, значит, хорошо так разделить на класс, на пространство. Вот. Ну, это и такой пример интересный. Мне нравится следующий. Он похожий. Просто теперь это и на сететики. Это сететические данные. Это уже, опять, пресс-канцер. Пример. Он чуть посложнее, но интересно. Показываю. Пока начнем. Все то же самое. Мэйшгриды, блокконтеры, все то же самое. Это новый, это другой. Сплити. Скальер создаем. Для скодирования наших данных. Потом смотрите в системе такую штуку, как PCA. Я не знаю, вы в каждой штуке PCA не проходили, да? Или кто-то может знает про PCA что-то? Трансформация. Трансформация. Ага, один, двух, амер, да, да. Все верно. Трансформация. А что им надо было, если подробнее? Это я уже подзабыл. В первом курсе изучал. А где учились? Эксетер. Такой универ в Англии. А, эксетер, да? Да, только на второй курс сейчас перешел. Понял, понял. Ну да, все в целом PCA это principal component analysis. Такая не совсем модель, это больше инструмент для того, чтобы редюсс размерность секторов для данных на целевые ураганцы. То есть в данном случае, видите, я создаю PCA модель, потому что редюссовать мой data set на до двух размерностей. То есть сжать. То есть у меня было на входе 10 фичей, 10 векторов, 10 размерностей. Я делаю это, загоняя в PCA, PCA делаю таким образом, чтобы у меня вышел новый sample этого data set, но при этом не на 10 осях, а на 2 осях всего лишь. При этом не потеряв информации почти. Ну, потеря информации невозможно полностью, но мы можем минимум это сделать так, чтобы PCA находят новые фичи для нелинейной комбинации новых фичей, principal components новые, которые могут создать предыдущую данную типу данных с 2 осяй. Это больше для реализации или ускорения обучения. Я создаю, видите, тоже на скалярах, обучаю, ловить, финиш, трансформ делаю, и создаю экстрен PCA. А если попозже, почему я создаю это? Здесь просто у меня оригинальная data set скалирована, а здесь уже data set троейновый, но уже в PC, то есть в RSS2. Это немного другие фичи, они не бьются друг другу, они не играют друг с другом, но они как бы не фрамптино походят. Общую информацию они показывают. Дальше смотрите, тоже самая комбинация для конфигурации SVM я строю, тоже самое, тоже самое, обучаю SVM в внутри, предиктую, замеряю скорые метрики наши, потом еще помимо, видите, вот это FIT, CLF FIT, я еще делаю здесь другой CLF PCA. То есть это такой же дубликат классификатора CV только для PCA данных. И засовываю для PCA данные, жаткие данные. Для чего это делаю? Для того, что для визуализации. Помните, что SVM работает в таком уровне, что он нелинейные данные возводит высоко высоко размерные трансформировать данные для того, чтобы оно в себе лучше работало. Для этого CVM PCA здесь тоже. Но это для того, чтобы увидеть какие-то фичи ног. Сейчас вопрос. Дмитрий, да, вопрос. Да, я только сейчас видел. Но в целом, для дерева, может быть, да. С правой. Но для бустинга и... Смотрите, просто... Видите? Для дерева, да, это работает. Деревья, в принципе, наверное, в обработке не нужны. Но понимаете, бэйгинг и бустинг это ее ранее. Он строится на... Ну, это отсаблирование моделей слабых. А никто не говорил, что обязательные условия отсаблирования бэйгинг или бустинг это дерево. То есть, вы можете вместо дерева брать линию для регрессии. Какую пленку? Это не запрещает. Поэтому, отвечая на вопрос, скорее всего, для бустинга и отсаблирования, скалирование всегда нужно. Потому что они всегда выделяются. Смотрите. Хорошо. Спасибо. Спасибо. Окей. Да, здесь построили. Давайте побежим туда-напосмотрим. Здесь тоже фичи и порно мы делаем и лейяуты встраиваем. Ничего такого, просто графики. Я здесь принесу для каждой комбинации SFM его метрики. Линейный С01, длиннейный С1, но тогда на графике будем смотреться. Смотрите, это у нас уже breast cancer. Смотрите. Здесь у нас как его? А, ну да. Линя, да, строка это линия, длинейная, вторая это полино, третья строка это полотенцер БФ. И видите, что у нас в целом линия неплохо справляется. Но наверное, есть потенциал того, что проверит более сложную функцию. Мы видим, что полино может неплохо вымещать. Интересный случай. Конечно, если это плохо, это хуже сделано, но он может выставить какую-то сложную линию более для того, чтобы разленить. Но это у нас очень плохо вышло. Мне, честно говоря, нравится, как работает РБФ. Потому что я вижу здесь, он смог, кажется, этот вариант основался в 98-889. Да-да-да. Смотрите, РБФ, Кернель и C10, да? Могла наиболее, в лучшем образом, по метрикам вытащить и отделить данные по ДОФСЕЛ. Ну, вот таков. Видите, вся мощность по сути СВМа, СВМ модели, по плану, что он очень хорошо работает с высокоразмерными данными. СВМ на пальцах щелкает на размеры 300-400 фичей. Ему вообще плевать. И он хорошо с ними работает. Если вы закинете в K-meets примеры для кластеризации 400 размеров, тогда он просто сломается, потому что он не спорит столько кластеров, которые выделит для одного реверса. Потребует меньше энергии в любом случае. СВМ это плевает все равно. Бусинг тоже так и нравится, на своем поле. Также фичи, фичи ипотезы, и все в целом. Вот. Так, ну, это будет все. Прикольное время заканчивается. Заканчивается у нас, что-то поздали, что-то было значимым. Давайте по вопросам. И вопросы может есть у вас. И туда я отключу, в принципе. Вот насчет... Можете вы же просто привозить здесь, где СВМ паттерн у него, почти все охватил, да? Самый последний, тут, ниже правый угол. У нас сколько сэмплов было в целом тут? Так, в целом тут... я скажу. Сейчас я<|tl|>. Это, кажется, датасет, да? Да, да. Вопрос в чем? Вопрос в том, что насколько это правильно так делать, то что модель может в переобучении, то есть, просеквационный bias, да? Или это... Ой, то есть, регрессионный bias может случиться. Да, все верно. Я понял, почему. Потому что, типа, насколько такой экстремальный рисунок выстраивать, это хорошо, да, будет? Такую прям дугу дорисовать. Да, да, да. На самом деле, все верно. Просто, видите, датасет был создан ситетически, да? То есть, это прям такой рисунок, да? По такой идее, да? То есть, если в слове говоря на расчет датасета за миллион, он такой же и будет. Потому что это такой статистический датасет, да, в таком рисунке устроится. Он просто будет плотнее, да, датасет, чем больше. В данном случае это будет корректно. Но в целом я понял, почему. Да, на самом деле, если у нас опять-таки case-by-case, мы всегда должны смотреть на сколько у нас функций сложно. То есть, в этом случае, да, вот этот RBF10, он очень сложный. То есть, у него какая-то там, знаете, у человека сложная функция, и понятно, как она вообще депрессируется. Который такой странный паттерн смог бы суммельно выучить, да? Соглашусь, что возможно, такой пример будет плохо масштабироваться, скенеризировать другие примеры, которые не были видны в реаблучении. Но в данном случае, в этом контексте это окей. Вопрос можно тоже. Что-то я прослушал, либо не понял. Вот параметр C, что влияет на SVM? Параметр C, да, влияет на... Да, получается, Мария Демитриевна, получается, основная задача SVM, это найти оптимальную плоскость Марган, Марган-плоскость, да? Марган-плоскость. Таким образом, чтобы она была максимально... Ну... Расстретилась вот так. Да, максимально отстойно для обоих классов. Ну, или для троих классов. Чтобы и для того был максимум отстой, и для того отстой. Соответственно, надо посередине делать. Это как бы первая цель его. Но иногда, такие случаи, когда вот такая задача стоит, это не целевает задачу подержания классификационной ошибки, ошибки классификации, это не целевает задачу. Тесно, я чувствую такое, что когда мы посетим цель максимальной отступа, найти максимальную плоскость отступа, иногда будет классификация не существует. То есть, чаще мисс классификация происходит. Из-за такой вот логики. Поэтому, чтобы послабить вот это правило, найти максимальную марген, мы добавляем вот такой регулизатор цель. Я понял. Такой регулизатор. Такой образом, чтобы ослабить это давление, ка мон, давай. Ты находи марген, но не такой струй, как раньше. Все, понял, понял. И еще такое по СВ. Получается, это только binary classification, там cross multiple classes Не будет работать? Конечно, будет. Просто на картинке вонит. На картинке, да. Опять-таки, да. Все куда срадеть, потому что, видите, это все визуализация. Смотрите. Видите, это визуализация 2D просто. В самом деле, это не так, что СВ работает только в 2D плоскости. Он строит киты плоскости. Понимаете? Он может работать с развертостью 100 damage. Это не реально представить. Соответственно, он может там и разделяться. И он также может разделять. То есть, представьте, гиперплоскость. Если, допустим, в 2D, чтобы в 2D отделить, нужно линия. В 3D нужно плоскость. А в 100D нужно 99D плоскость какая-то. Гиперплоскость. Ага. Это не реально представить, но он так работает. Он может придумать такую плоскость, которая состоит из 99 развертости и разделяет вот так вот классы. Не только два, а мультиклассы. Понял. То есть, мы просто приводим я для визуализации перевожу его на такой примитивный уровень. 2D плоскости линии просто. Потому что, видите, допустим, в одном примере ниже, я здесь указывал, PC еще. PC опять, потому что, видите, breast cancer, он... Датасета у него, смотрите, здесь посмотри. Водируется, да? Да, смотрите, у него вот такая шейта. Видите, 30 размеров. Немного, но, в принципе, немало. Поэтому, видите, я здесь... Видите, посмотрите, оси. First, second principal components. Это не оригинальные фичи. Здесь были оригинальные фичи 2D, потому что вот этот датасет, видите, фича 1, фича 2. Этот датасет был вначале 2D. Не, я понял, да, хорошо. Тут еще есть третий вопрос. Ты мне вырос. А вот kernel... Вы вспомните, вы говорили про kernel trick в Swam? Это не то... Ну, я так понял, что как раз для нелинейных распределений подходит. Мы его не применяли, да, тут? Не, применяли, применяли. Вот здесь вот есть kernel trick. У здесь kernel trick используется, у здесь кернел trick используется, это все kernel trick. Почти всегда кернел trick. То есть вы не понимали... Это просто параметр, да, при объявлении модели. Да, да, да, это параметр. Все, я все понял. Всем спасибо. Еще какие-то вопросы, может быть? Если сядься. Может быть, не по элекции, а просто вообще, куда мы движемся, что еще будет, что еще не будет, что лучше сделать, что не сделать, и т.д. и т.п. Можно по... Домашнее задание, наверное, примерно такое же, да, будет применение уже? Да, да. Да, да, примерно такое будет. По хору. А можно подтянуться к начальному ходу, где был cross validation на 5 минут? CV5 это было. Ага, оно почти везде было. Деревья, да, или boosting? Да, это деревья были, да. И мы сказали best model, да. Best model сказали. Ага. В начале, в начале, да. Cross validation мы его shuffle делаем, и мы можем это как-то мониторить, чтобы найти best best shuffle, чтобы использовать тренинги модели в плане... Я как-то понял, что вы кинематизируете? Вы, наверное, чуть-чуть перемешали понятия cross validation с great search. Cross validation, да, это всё правильно, мы шахлив датасет по-разному, но это не для того, чтобы найти лучший сплит датасета, да. Это для того, чтобы просто для нас удивиться, что в целом по окончании обучения нашей модели у нас нет шанса того, что мы просто накнулись на очень хороший датасет в выборке, снизился. А то, что вы говорите, как бы, лучшие параметры найти, это уже про great search, когда мы уже в root force запускаем полноваштабную такую итерацию разных комбинаций моделей для поиска и выяснения лучшего перфоманса. То есть при каких конфигурациях и перфомансах она стреляет лучше. Всё, всё, всё, я понял. Это две разные вещи. А извиняюсь за вопрос, можно ещё раз эти материалы до урока загружать? Что может быть самим ещё пора смотреть? А, окей, хороший момент для меня. Окей, буду фонворты кидать. Зачастую бывает, что материалы готовы за час, за час бывает. Ну, хотя бы за полчаса тогда скидывать, что можно было скачать. Хорошо, хорошо, я буду скидывать. Спасибо большое. Ну, в целом, ещё вопросы. И если нет, то а как вообще вопрос вовлет к вам теперь? Можно получить оправданный связь? Как идём? Как курс? Как материал? Ну, понимаю, это два урока было со мной, но в целом, чтобы я тоже как-то конфигурировал, корректировал ведение курса и преподавание по ток-мейдам. Я классно. Мне нравится, что мы достаточно быстро идём, потому что у меня вот знакомая учится параллельно на датс-санте. Вот, что-то там ребята очень медленно идут. А, в смысле, в вот перетор? Да, да, да, да. А так по объяснению, по детализации всё будет. Спасибо. Ну да, в целом, наверное, потому что AI-инженер курс больше на вот современные сети, и на применение больше, поэтому, наверное, чуть-чуть быстрее. Здесь, в принципе, мы когда вначале моделировали курс, мы его вообще довольно классически не брали сюда. Потом решили, ладно, ладно. Ну, мне нравится, что всё подробно объединяется и как бы пытаемся за выделенное время всё покрыть, даже если это дополнительное время занимать и не оставляется что-то в будущее. Мне очень нравится, что всё покрывается. Спасибо. А примерно какая домашняя практика у нас будет? Вот чтобы, когда готовится к урокам? В целом, я просто, видите, я сам трейдер первый раз, как бы я первый раз видел этот курс. В целом, я ещё не все тоже понимаю, как здесь домарная работа устроена, но, насколько я понял, одна домашняя в неделе на один блок. Соответственно, у нас с вами неделя в понедельник началась запярться, а нет. Ну да, да, в понедельник у нас завершение этого блока, подблока. Я пять кидаю домашку на неделю, все это чуть-чуть, а можно начинать всё другое, да я его уже Model Evolution блока. А в целом, по составляющей, скорее всего, это будет тоже такой ноутбук, вот как я показываю вам сейчас. Ну, плюс-пинус такие же темы, скорее всего, я не буду вообще что-то из меня задавать. Наверное, будет практическая такая агрегация всего, что мы прошли. По типу код задачи, дополни, вот тебе в кустое место, дополни код, пиши своё мнение, додучи, додумай, придумай, и что-нибудь, да, как его... Я не знаю, я еще не начинал домашку делать. Я вообще придумал, короче. Вопрос был, да? Сейчас вопрос был, да? Ага, понял, спасибо. Ну, уже примерно более-менее понятия есть, что будет практическая, спасибо. Окей. Да, вопрос о совпад первого. Какова фамилия? Поскорейте, вы будете практицировать какие практические коды? Так. Хороший вопрос. В целом, с током прям напрямую не работал, но, ну, мне вернее, я сам строил модели, но ребята, ну, строили в команде у меня. Ну, это все просто, то есть, смотрите, в основном интернете все полно много информации, то есть задача тока, она очень не тривиальная, она требует, на самом деле, очень хорошего качественного обработки данных, потому что, к примеру, модели там простые, через кредитные бусики хорошие управляются. Вместественно, там больше, наверное, нужно вам собрать этот пасет качественно. То есть, что такое отток? Это отток — это когда клиент уходит, но вы не знаете, когда он уходит, соответственно, вы должны понимать по признакам, по какой-то активности сервиса вашего, понимать, что есть вообще отток. Потому что человек может просто раз в год заходить в приложение, для кого-то он будет отточен, для кого-то он не будет отточен, потому что для каких-то сервисов они раз в год заходят — это нормальная динамика активности пользователь, а для каких-то сервисов неделя не появится — это уже отток считается. Вам нужно, мне кажется, понять, что такое диффиниция оттока в вашем сегменте, домена, и уже на основе этого выставлять какую-то логику. Потому что это очень важно. По примеру каких-то best practice, скорее всего, просто бинарная классификация, как простейший вариант. Имеется по логичи кредитная бусика, банляра классификация, где 1 — это отток, 0 — это не отток. То есть, собираете какие-то состояния человека, признаки его во время обучения, на момент до оттока состояния, берете target label, как в моменте оттока, допустим, берете состояние декабрь, январь, февраль, марта, его активность какой-то, как признаки. И в марте он ушел, да соскакивает, отверг. Это будет набор сэмпла для человека, который был отточен вописом. То есть, вы должны какую-то составить временную ось, потому что отток — это же про время, то есть, должна быть какая-то временная ось в виде агрегатов, типа там, среди инвафер, в раммар, за время, жадство, и так далее. Это как логический подход. Есть подходы там, типа прям временные ряды, прям настоящие временные ряды, где есть состояния по времени разные, аримы, сарима модели. Может быть, не рестивы, но теперь все модели, рекуррентные, все эти. Там на самом деле полно информации, можете посчитать. И что у нас? Надеюсь, это уже все. Еще что-то? … Ладно, тогда, я думаю, всё. Об facile будет, когда мы разрешим. Спасибо, очень приятная лекция была. Спасибо. До свидания. Хорошего аппетита!