 Добрый вечер, друзья. Добрый вечер. Здравствуйте всем. Добрый вечер. Давайте еще дождемся 35 частников. Добрый вечер. Добрый вечер. Добрый вечер. Видите, да, монитор? Да, все видно. Да, все видно. Давайте начнем. Не подключится больше. Ставлю время интернет-ресторж. Так, всем приветствую. Спасибо, что пришли. Сегодня наседьмая лекция по классике ML. В целом, получается, это заключительная лекция в этом подблоке классиков ML. Вот здесь поговорим сегодня про много расширения для процепов, которые в прошлый раз отрадовались. У меня вот такие слаймы. Вот здесь чуть больше процессов по форму, и больше деталей захватим. На это я иначально хотел в одну тему. На прошлый урок я закончил потом. У нас есть запас передиэпиги. Давайте или позже. Извините, можно вас перевернуть? Да. А можете, пожалуйста, с микрофоном немного его настроить? Нечетко. Сейчас лучше, да? Сейчас нам лучше немножко, но все равно. Ухо режет. Ухо режет. Да, да, да. Давайте я подумаю. Так, сейчас. А вот так лучше? У вас микрофон слэпки как будто идет. Да, типа 10 этого. Спайка. Потом хотел узнать еще, у нас какая-то практика будет или будет сегодня лекция? Да, всегда практика будет. Обычно. А вот так лучше? Да, намного лучше. Да, намного лучше. Да, видимо. Видимо, да, у меня спэп. Сейчас я. Возможно, у вас нервные. Лучше, да, сейчас? Да, намного лучше. Отлично. Я тоже в тот раз смотрел видео, какие-то отрывки жесткие были. Окей. Ладно, давай начнем. Вопрос был какой-то по практику, да? Парень сказал какой-то, кто-то спросил. Этот вопрос. Да, будет как в привычном формате. То есть, сначала начнем с лекции. Какой концептов, потом перейдем уже в ноутбук, да, и там, ну, как обычно, в общем. В это начнем. В целом, да, я хотел еще отметить следующую, следующий концепт. В целом, я, наверное, чуть-чуть, наверное, не сактонтировал внимание на такой теме, как регулирование, на самом деле. Почему-то она как-то мимо прошла. Вообще, мы должны были, наверное, ее в пятой лекции обойтись. В целом, главное сейчас охватить. Но вообще она как домашняя будет у вас. Просто дамы интро общие. Что это такое. И в целом, как ее использовать, да? Вот. В целом, да. Можете, пожалуйста, сказать, вернее, те, кто не знает, что такое регулирование, там, отписать либо просто озвучить, я не знаю. Я не знаю. Я не знаю. Не знаю тоже. Я не знаю тоже регулировать. Тоже не знаю. Окей. В целом, давайте объясним это, да? Подробней. Регулировать, да, это такой концепт, который помогает нам вообще в задачах машины, в алгоритмах машинного обучения сделать алгоритмы таким образом, чтобы они имели более лучшую обобщающую способность. То есть, что такая обобщающая способность? Это генерализация, да? То есть, это когда алгоритм умеет обобщать данные, да, либо хорошо работать на тех данных, на которых он не обучался вовсе. То есть, это чем более, как бы, генерализируемый алгоритм, тем лучше он работает, тем более универсально становится, и тем лучше он работает вообще, ну, всегда по жизни. То есть, мы всегда хотим построить такие алгоритмы. Вот. Но зачастую, да, чтобы такой алгоритм построить, это очень сложно, и зачастую алгоритмы становятся переобученными, да, то есть, когда мы слишком сильно заорфитили, да, алгоритм. То есть, мы переобучили на данных, на тренировочных данных таким образом, что модель, она выучила, конечно, что-то, но она, на самом деле, выучила слишком хорошо это, что начала просто копировать, да, или подстраивать свои коэффициенты внутренние, да, вот W коэффициенты параметра модели таким образом, что просто наилучшим образом снизить ошибку в рамках тренировочного обучения, ну, в рамках обучения. Как бы, это не всем хорошо, потому что ваши данные, которые на тренировке, да, появляются, они не всегда являются так и таковыми же во время валидации, да, или в боевой стрельде. Общий паттерн подходит, но в целом, вариации, как бы, ну, есть, конечно, присутствует. Впрестано, тем самым, модель переобучила и до сфера. Вот. Вот. Наверное, да, здесь стоит, да? Ну, да. Так, а что это у нас? Что-то висит, да? Да, висит, окей. И в целом, смотрите, получается, ну, чтобы дабы теперь бороться нам с тем, с этим самым переобучением, да, мы должны какие-то техники применять. Одна из таких техник это регулизация, да? Регулизация, на самом деле, это тоже общее понятие. То есть, не такое, что регулизация это вот о чем-то, о чем-то, да, там, определение одно. Это тоже общее семейство подходов того, как делать так, чтобы модель не оверфитилась сильно. Ну, что я, в целом, вот, definition, да, простая регулизация, эта техника получается. Техники, которые позволяют предотвратить оверфитинг путем, не обязательно путем, только этим, добавление каких-то, да, штрафных понятий, да, или термов, ну, или чтенов к функции потери. Я и сказал, да, что не всегда так, потому что видите, опять-таки, это definition регулизация, он был, опять-таки, построен здесь в контексте классического, да, эмали, но, в целом, это не всегда так, потому что есть масса других техник регулизации, да, в глубинном обучении, которые также являются регулизацией, но при этом они не вводят такое понятие, как добавлять, добавить достаточно штраф в функцию потери. Допустим, dropout до слои в нейронах сетях, это тоже регулизация, но там вообще не по этой идет история. Вот. Ответственно, это как бы, ну, и здесь такое продолжение, да, discouraging the complex models. То есть, да, тем самым, как бы, ну, это же слово, не знаю, переводится на русском, это на русском, не знаю, на русском, не знаю, не понимаю его. То есть, когда мы пытаемся, наши модели, чутка, как бы, ухудшить, да, на время обучения, именно. И, таким образом, когда таким образом мы пытаемся обучить что-то, да, обычно, вообще, в принципе, генерализация, да, модели улучшается. То есть, словно говоря, мы пытаемся внести какие-то дополнительные такие штрафы, да, таким образом, чтобы модель лучше работала потом на unseen data. Сейчас мы поговорим про эти, да, штрафы. Ну, в целом, это как домашний, конечно, как бы, уйти, но в целом, поговорим. В общем, в линии регрессии, да, да, и логической регрессии, да, обычно для, для, для вот этих двух моделей, используются L1, L2 и Elastic Planet, три типа регулизации. L1, да, ну, здесь, как бы, кто математикой, из мира математики, больше они, наверное, поймут, почему L1, L2 вообще к чему это значит. Это больше про нормы, да, норма — это длина вектора, нормаль вектора. Магнитуда, длина вектора, ну, какая там влитуда вектора. То есть, есть два типа, ну, два типа длины векторов — это L1, L2. То есть, ну, получается, L1 — нормализация, это когда мы вектор, условно говоря, так, если не ошибаюсь, сейчас, сейчас, сейчас вспомню, если не вспомню эту формулу, но давайте попробуем написать. То есть, вот L1, да, это когда мы пытаемся нормализовать наш какой-то вектор, таким образом преобразовать его, да, чтобы X1, да, ну, это разы, да, это размерность вектора, плюс X3, да, и так далее, да, под корнем, да, должны равняться единицы. Если не ошибаюсь, кажется, вот так вот было. Могу еще, ну, сейчас подожду еще. То есть, это, грубо говоря, у нас вектор был в таком-то виде, да, тоже в каком-то там, не знаю, X1, да, там X2, в таком-то виде. Мы такие, давай-ка его сделаем так, чтобы его сумма, кажется, без корня, я сейчас уточню. Таким образом, чтобы все его задеть, да... Я вот сейчас, сейчас могу наобщиваться, сейчас проверю, да, ну, это просто, как бы, пока напишу. Таким образом, чтобы, да, его там, сумма сходящих, да, ну, в сумме равня равно одёрки была. Вот L2, соответственно, это то же самое, только когда у нас мы... Вот L2 я точно знаю. X1, в квадрате, да, 2, в квадрате, да. Так, далее-далее-далее, до N-ого члена в квадрате. Взять, если под корень, это все, равно 1. Как бы это, это называется нормирование векторов. Нормирование, то есть, это когда мы их нормализуем. Когда у них магнитуда становится единицой равна, да. Почему это хорошо? Ну, как бы, вообще, из математики, да, мы делаем вектора, ну, в рейндже, да, магнитуда, в 0,1, ну, в одёрке, да, типа. Сейчас скажу это вам точнее. Чтобы просто вам сразу хорошо за это дошла, ну, посиделась эта мысль. Да, всё верно было, это просто без корня будет, да, просто. Когда суммарная, когда сумм, ну, суммировать вектора, они равно в 1, магнитуду должны равна единицей. Вот. Ну, это отсюда как бы идёт понятие L1-L2. В целом, их также, да, чтобы более применить, их назвали просто Lassa Rich. Ну, это уже, видимо, какие-то авторы подхода. И в целом, когда мы говорим про L1, регулируем длину и длину игре, мы говорим, что мы добавляем, опять-таки не забывайте, да, что мы добавляем к функции потери, да, loss function. Видите, this penalty turn to loss function. Мы добавляем сумму абсолютных значений, коэффициентов весов в нашей модели. То есть мы добавляем, помните, да, была наша loss функция, да, какая-то там, Lx, там, mse, да, в данном случае. Там, какой-то, да, поднорился, да, этих, слагаемых. Мы добавляем туда вот эти термы. Видите, географом, ну, как бы, я сейчас объясню попозже, почему это работает вообще и почему это даёт нам эффект. Вот то же самое проходит и для L2, да, только в L2 добавляется уже сумма этих коэффициентов, да, значений, не знаю, в квадрате. Ну, по сути, вот, да, об этом говорится. Вот. Так же, новый ElasticNet. Я, честно, на самом деле, не с тем вижу смысла его объяснять, потому что, в целом, это просто объединение этих двух, обеих типов регулизации, и они, в принципе, вроде по дефолту, когда мы эскалерно импортируем библиотеку, это всё уже учитывается там. Как бы, вообще, в целом, наверное, вы сами никогда не будете прописывать это в жизни, L1, L2, там, потому что в целом, под капотом уже, когда вы эскалерно импортируете что-то, линейной грейсию, ту же самую, линейно-логичеческую, она уже это учитывает. То есть, она уже делает так, что это внутри модельки вашей учитывается. Просто, как бы, для общего понимания, это вы должны знать. Вот. В целом, да, также хотел бы ещё вот сейчас эту штуку показать. Просто... Вся вот момент важный очень. Возможно, у вас есть какие-то какие-то интересные вопросы? А можно вопрос? Да-да-да, можно вопрос как раз. Вроде мы отдельно импортируем вот эти регулировки, типа, как LASSO, RIDGE, из эскалера. Нет, вроде бы там какая-то, сколько я знаю, они по-дефолту уже используются. То есть, там уже, как бы, возможно, там ещё какой-то как его... Уверен, это первородная регулировка. На самом деле, там есть очень много модификаций. Возможно, они уже используются под капотом. Но, сколько я помню, я, честно, ну, не так много, на самом деле, работает с эскалерным в последнее время, но в момент, когда я работал с ними, там это уже было прописано. Вот. Можно тоже вопрос, если есть возможность? Да-да, конечно. Я, сорри, может, забегаю вперёд. Во-первых, первое просто выточнение, вот LASSO и RIDGE. Получается, от KMSE добавляются там суммы коэффициентов для LASSO. Обучаемых параметров. Да-да-да. То есть, у нас там три фичи, допустим, и у них там 3, 4, 5, да, и вот сумму 3, ну, там, сколько? 12 будет. Вот эту сумму добавляет KMSE, правильно? Ну, для L1. Ну, для L2 там соответственно сумму квадратов. Да. Окей. И тут такой вопрос, что же я, конечно, мог бы загуглить. Сорри, сорри, что ты... Это кто говорит, что ты пропадаешь? Это кто говорит? Дима Ли. А, да-да, чуть-чуть пропадаешь. Ещё раз повтори, пожалуйста. Сорри, сорри, да. И второй ещё вопрос такой, я, конечно, мог бы загуглить, говорю, но что-то я, ну, типа, легче так понять. У decision tree и у RunaForest тоже есть параметр регуляризации, на самом деле. Вот. И хотел бы спросить, как именно не в линейных моделях, а вот в таких древовидных моделях будет работать регуляризация. Если вдруг, знаете, да. Да-да, конечно. Вообще, да, хороший вопрос. В таком вот именно... Да, это сперва отвечаем на первый вопрос, да. Показательно, вот именно подробнее, да, объяснить. Окей, я объясню. И по древесам тоже решим. Вообще, сейчас я покажу формулу, да. Как раз я так углю просто. Так. Так, по чуть-чуть у меня почему-то сильно лагает этот. Сильно лагает. Вообще, да, я бы хотел, наверное, ещё протопливать это. Ставить. Для вашего понимания, чтобы просто легче было. На самом деле, я не захотел без формул, потому что курс не про формулы, но всё же лагает. Давайте поставим. Раз мы затронули глубже это. Так, и давайте сделаем это, да. Отношение, да. Так, clear, да. Где там было? Смотрите. Вот, как игре урагни, да, мы сейчас говорим про L1. L1, да, это наша вот формула, да. То есть, видите, вот это будет регрессия наша, да. Все видно, да, то, что я говорю, сейчас показываю. Вот наша функция. Это наша функция кастово, да, ну, или loss function без разницы, как именуете. Их поразом называют. МСЕ, допустим. Вот это прям МСЕ. Здесь у нас есть y-target, наш, да. Отнимается наше произведение линии трансформации наших входных параметров x на наши веса, да, обученные. V-quadrat. И это, в принципе, есть наш МСЕ. Видите, вообще в переворотном виде, да, без регулировки, да, в стандартном виде она вот так выглядит, да, просто. Но мы добавляем, как здесь говорилось, да, выше. Прям что мы добавляем сумму, да, наших новых параметров. Тех же обучаемых весов. В случае L1 регулировки, то есть, да, loss. Добавляем вот просто сумму, ну, сумму наших весов, да, в абсолютной магнитуде, то есть просто в бод модулях. Умноженными еще на такой коэффициент лямда. Это просто такой регулировкационный параметр, чтобы усиливает эффект сжатия. Теперь давайте поговорим здесь подробнее. Ну, лямда бывает там, она маленькая, типа там от нуля, там от нуля, больше нуля, там от нуля там до 2, до 3. Там, на самом деле, очень много вопросов по тому, какое именно валюм. Обычно это там от нуля до 2, до 3, это такое небольшое значение. Он просто сжимает, да, увеличивает, либо сжимает, да, вот этот регулировкационный эффект. Смотрите. И теперь по сути давайте здесь подискустируем подробнее, да, собственно, почему, ну, почему это работает вообще? То есть почему, каким это таким образом, да, добавлением вот этого терма, да, в наше уравнение формулы влияет на то, что модель становится собственно, ну, лучше работает, не уэффициится. Почему? Может быть, ну, может быть, давайте вместе подискустируем, подумаем, потому что это важно, да, нам подискусировать. Это тема. Можно? Да, конечно. Например, наверное, датасет, он сам по себе может быть ограниченный. Ну, и чтобы вот предотвратить loss penalty может как-то модифицировать этот тест, ой, training датасет и добавить больше данных, больше точек. И чтобы, на которых может учиться модель, например, таким образом усилить. Хорошая попытка, интересно, интересно, как бы, на понятие. Ну, не совсем. Ещё может кто-то. Ну, хорошая, за старания плюсик можно дать. Классно. Ну, мне кажется, там при обучении там же есть веса. Вот некоторые весы могут содержать очень большие значения. Когда мы добавляем вот эти параметры регулизации, оно оштрафует вот эти большие весы. Очень, очень, очень рядом, да. В принципе, наверное, ваш игрой я чуть по-другому изучал, но я понял. Кажется, кажется, об этом тоже говорим сейчас. Да, очень рядом. Да, на самом деле, Айкен, да, это говорил? Айкен очень рядом сказал, очень близко на самом деле сказал. Смотрите, не забывайте, что мы сейчас говорим только о функции потери, да? Наша госфанкшн, да? Наша функция потери. То есть, помните, да, что что мы всегда стремимся, да, чтобы наша функция потери стремился к минимальному значению. Вобстотно, поэтому, да, это вокруг этого всего крутится. Смотрите, теперь выходит так, что по 50 мы обучаемся, да, вот мы первую трансу прогнали, да, посчитали коэффициенты, высчитали среднюю критичную ошибку, что там стало значение там 1100. И теперь еще к этому, да, мы добавляем сумму, да, наших весов дополнительно. То есть, мы искусственно завысили нашу еще функцию потери, да? А, действительно, наших весов, да? То есть, мы несли наши же веса в систему, да, которые мы пытаемся контролировать, понижать, да, или увеличить унит. Мы их умножили еще на какую-то лямду и тем самым мы еще увеличили функцию потери. Это о чем говорит? Наша степь, ну, наша вот эта кост-фанкция понимает, что она же оптимизируется каждой итерацией, она пытается оптимум найти. Она понимает, что ну, в моем большом значении, да, в моих больших потерях, функции потери, да, меня сильно поталкивают на это мои веса, то есть, мои w, да? Они сильно меня это поталкивают, в общем, если функция растет, то это понятно, что она из-за w сильно растет, потому что мы добавили w еще сверху. И тем самым она понимает, что нужно сильнее нашей w штрафовать. То есть, раз w сильно растет и меня это загоняет в большую там ошибку, то давайте там будем, ну, на то натуральный розум она понимает, что нужно теперь w понижать. То есть, штрафовать больше нашу, ну, как всегда, оптимизацию штрафовать больше на нашу w. Вот. Вот. Тем самым, смотрите, а, как сказать, ну, то есть, w наши выросли и тем самым мы пытаемся понизить, да? Вот. То есть, смотрите, получается, если у нас, понимаете, если у нас w очень большие, да, там, не знаю, там, типа w1 равно 100, да, там, w2 равно 300, да? Обычно, да, вот эти веса, да, у хороших моделей, вообще, это такое, не знаю, это такое, больше из опыта говорю, что у хороших моделей, нормальных моделей, которые адекватно работают в каких-то рабочих состояниях, у них вот эта размерность весов, они в районе нуля всегда. 0.0, там, 1.1, минус 0.5 и так подобное. То есть, у них значения очень такие, ну, обнормализованные, нормальные. Если вы видите, не дай бог, в модели какие-то очень странные, большие, какие-то разбросы страшные, это говорит о том, что модель, ну, очень плохо обучена. Обычно, как бы, смотрите, если у нас веса очень большие, да, вот такого бы не туда, то у нас loss, да, то есть тоже растет сверх. Ну, логично, да, поэтому. Обычно, видите, чем больше вес, тем сильнее штрафуется, да, модель. Чем больше веса, тем меньше штрафуется. Тут ошибка нам уменьшится. Естественно, мы пытаемся на нашу модель, ну, как бы, веса сделать более адекватными. Вот. В принципе, то же самое и в такой же интерпретации, просто в L2 регулируем. То есть, тут только логика, просто теперь у нашей веса еще квадрат возведенный. Ну, это, как бы, еще больше усилия deffect. То есть, в данном случае здесь все идентично, то же самое, но кто может подсказать вообще, в чем отличие, ну, вообще, какой функционал отличия здесь помимо квадрата? Ну, вернее, квадрат, да, функционал, ну, в смысле, а почему, что здесь квадрат вносит в эту систему? Он еще строже. Кто сказал? Девушка с дэску за вас. Да, я говорю, он еще строже, да, панишит. Ага, поподробнее, раз вы сможете? Ну, если цифра возведенная в квадрат, она же еще выше, то есть, у нас вот этот общий кост или loss function, она еще выше становится, чем просто сумма, ну, чем L1. И таким образом она, как будто, сама себя еще больше строже, ну, вот эти penalty de наказывает и испытает сама себя еще, снизить саму себя, еще лучше пытается переобучиться. Так, в чем вопрос? В чате сейчас. Absurdity values. Акжани, всем поняла, что ты имела это? Магдана в микрофоне, нет? В целом, да, можно, да? В целом, верно. Вы датафайн сбрали, да, ранее, не брали? Нет, нет, не брала. А ну, да. Ну да, в целом, все верно, потому что вот этот квадрат, все верно, я сказал, еще опятовать, переведу еще раз. Ну, функция квадрата, она сама по себе растет быстрее по y, чем по x, да? Соответственно, она, когда наша функция растет, линейно скажем, x, нашу w, то их ошибка будет расти в квадрате гораздо быстрее. То есть, чем больше ошибка, чем сильнее у нее с penalty, чем меньше ошибка в w, чем меньше penalty будет. Соответственно, вот этот регулизатор l2, он штрафует гораздо сильнее, если у вас больше веса в размерности. Если у вас, раз веса меньше становится, то она их меньше штрафует. Такая, как бы, функция очень гладкая получается, то есть, она такая адаптивная, да? Но это, как знаете, это как МСЕ, да, сама функция сама по себе. Вот. А пусть у тебя есть функция МСЕ, есть функция МАЕ, да, Min Absolute Error, и есть Min Square Error. Допустим, если у вас много выбросов данных, то МАЕ в целом, это, наверное, будет все равно, она не так будет чувствительна выбросом, а МСЕ, наверное, выброса сильно почувствует на себе, потому что она в квадрат воздействует. То есть, попал, допустим, какой-то случайный шов-выброс в данные, допустим, предсказываем, а он фактически там все там в районе 10 скучкуются данные, да, Why Predict, Why True в лейбле, а выброс его, да, вот этого, на котором вы предсказали 5, он на то, что он как-то шум появился, он там 10 тысяч. Вообще далеко. То есть, если вы выложите радицу квадратную 10 тысяч и 5, это будет сколько? Это будет 9 тысяч 995 квадрат, это вот огромная величина. Соответственно, у нас будет какой-то скачок страшный в погрешности и в общем пересчете весов. Ну, вот, то есть, здесь, видите, да, и такая логика обычная. Вот. Можно тоже вопрос с Биберас? Не знаю, меня-то я когда-то, ну, изучал, меня всегда волновал вопрос один. При градиенном спуске мы же спускаемся вот, ну, то есть, давайте представим, типа, L1, L2, ну, типа, на графике это же просто пора было от, ну, двигается, типа, выше или ниже, правильно понимаю? Вот есть, типа, сет параметров и вот, ну, как бы, выше или ниже в целом для всего. Вы на муте есть что? А, да-да-да, типа, вот так, да, вот? Ну, да-да. И, типа, просто вверх поднимаемся, условно. Ясно, ну, там. То есть, что я хочу сказать? Мы же, типа, градиентно не меняем слоп нашей параболы, мы ее просто по Y двигаем вверх, и я никогда не понимал, типа, почему почему она работает, точнее, как она оптимизируется? Если у нас параболы одинаковые, ну, все так же остается. Или я неправ еще? Патрон светится. Ну, наверное, да, окей. Я примерно понял. Сейчас повторю вопрос. Правильно ли я сейчас сказал? Так, ну, тогда мне придется растереть все это, тогда нужно будет все это грамотно объяснить сейчас. Так, хорош вопрос, по идее. Подключал только что. Давайте для этого вопроса мы сейчас выделим. Прям пустое... Нужно будет место для рисования. Давайте, окей, это был у нас... сейчас это был... это был Димитрий, да? Дима. Все, все, все. Дима задал хороший вопрос, я повторюсь сейчас его. Получается, Дима спросил в контексте гранированного спуска, да? Как эта штуковина работает? Угу. Окей. Давайте начнем с гранированного спуска. По сути, мы как-то его почему-то уже не прошли, но почему-то я вообще по дефолту, как будто бы это немного не отсюда, это из-за частей неохранительной организации, но все равно объясню. Смотрите. Мы всегда стремимся, да? А, нет, нет, нет, нет. Смотрите. Помните, да, наша функция mse? Она в своей природе квадратная, да? Вот. Ну там, в y-х, да еще. То есть помните, здесь за алгебрами понимаем, что квадратные функции, они параболойты, да, в своей натуре. Теосно вот я нарисовал параболойт, да, обычный, в зависимости w, в веса 1, от l, да, в самой функции. Ну, предположим, он так выглядит. Хотя не факт. И что мы делаем? Мы всегда пытаемся, да, градиентно пускать, но когда мы начинаем, у нас чего-то, у нас какой-то параметр, да, стоит вот здесь. Простите, пожалуйста, так, а все сейчас понимают, пожалуйста, ответьте, все сейчас понимают, где мы сейчас, и кто-то потерялся. Просто разброс знаний на самом деле сильный. Нет, слышно, все нормально. Да, понятно. Всем все понятно. Пожалуйста, точно это, может быть, пару людей не всем понимают, это окей, я могу быстро объяснить. Только честно. Окей, окей, отлично. Смотрите, мы стартуем всегда с какой-то точки, да, во функции потери. Потому что у нас всегда какие-то есть точки, ну, точки в весах. Будут они рандомные, будут они нулевые, будут они хоть какие. Это не важно, да, ни с чего начать. Естественно, после каждого градиентного спуска, да, если мы, конечно, идем с градиентным спуском, а не там этим, да, close form solution, да, когда мы напрямую приравниваем нулю градиент и учитываем, да, уже точное значение. Если у нас функция какая-то, ну, отрезанная, предположим, что мы используем градиентную спуску. Форму градиентной спуска помните, да? Все помнят? Окей, давайте, вау, напишу здесь. Смотрите, очень плохой почерк. Смотрите, θ, да? Равно θ, θ новая, θ старая. Отнять learning rate. Наш learning rate, я забыл греческую букву. Как называется эта штука. Умноженная на частную производную нашей функции потери относительно того или иного параметра модели. Ну, вот, вау, вот, вот, то есть, стаминировать, да, на то или иного параметра модели. Ну, вместо θ, об partir Вот, обычно, я на что частную производную потому что это в общем виде. Потому что параметров öвает много, да, чем одна переменная. якобин треугольник если у вас совсем много параметрии на матрице вот соответственно что происходит здесь вот мы начали с чего-то да вот нашу нашу loss функцию на какой-то в да мы посчитали градиент да градиент а шесть тонн сюда да указывает положим спустились пересчитали пересчитали пересчитали пересчитали пересчитали пересчитали да все мы здесь и типа здесь ну все стоп стоп трейнинг все вот наша функция обучения при оптимальном параметре w но вы дабы у нас нашли его вот смотрите кто теперь здесь есть то что вот дмитрий спросил касательно касательно этой регулизации да собственно что как она типа влеет отвечая на вопрос на прямом образум сразу же она влияет на то что в наш градиент становится больше внутри каждого параметра сразу градиент а побольше и мы либо быстрее спускаем с либо просто приводим к нулю он получается увеличиваться магнитуде но еще а смотри получается а видишь ты помнишь в этом так плохо плохо не удалось ваше смотри вы здесь в ты все с математикой помнишь да помнишь что такое частное производное частное производное это получается у нас когда мы считаем то есть а сейчас ванна смотрите у нас есть функции да вот вот фикс да равно и скоро да ее производная да это будет 2x это помните да все да смотрите а давайте еще закаром так у нас есть fx он теперь выглядит так там не знаю 5 y да ваграте давайте так мне нужно написать fx да от fx что будет будет 2x пресс 5 y да и множенная на на все на один на все на один да а если возьмем до fx это будет что у нас это будет то же самое что это 5 y да еще еще будет здесь y еще будет 5 да давайте много физики дайдес это значит что мы когда про моделируем да что-то понимаете вы можете моделировать то же самое да какой-то сложная термальное уравнение да обернули то же самое много параметрии на да где где допустим вас переменная так это есть сложная переменная моделирование там тепловых частей движение движение вообще как тепло да пространство двигается словно грата может в этом в этом сложности в этом сложном дифференциальном уравнении да можно сидеть не знаю до 20 до там до сотни различных переменных каждый из которых какой-то определенной форме влияет на общую функцию да там не знаю тепло не тепло частицей самотема частицы с которой там там состоит объекта там всякие коэффициенты сжат воздух и там подобное тесно чтобы вам понимать насколько да меняется и как вообще меняется там словно какая-то целевая функция да ну обычно мы всегда градиент да считаем мы считаем градиент чего-то градиент что-то производный берем от каждой относительно целевое значение градиент отцени на каждого этого параметра интересно здесь тоже самое да у нас есть функция x плюс 5 y в квадрате это целевая функция и мы должны теперь понимать ага давай посчитаем градиент как он выглядит функции для каждого параметра то есть иными словами как изменение до маленькой изменения в каждом маленьком параметре и к за дифференциали икс или там y да это маленький дифференциал до маленькой изменения да функции как вот это изменение эффектит влияет на нашу основную переменную то есть на f да соответственно то же самое и здесь мы говорим это частная прозона ддд этот как вам вот этот дел на дд в дайд и получается говорим находим как изменение по уайт по даббл по даббл ю и туда или вперед изменение динамики влияет сильно на нашу л нашу функцию соответственно мы получили по параметре иногда для каждого из параметров мы производу считаем таких параметров может просто быть сотен сотен на сотни да матрица ну да я веру понял вот ответственно возвращающего не до конца ответили на вопрос если мы идем сюда опять то есть два случаев у нас теперь функция да считать производное сюда да считаться будет согласны с этим да да да да да то что градиент здесь будет здесь будет потом мы просумеривали это и градиент увеличился потому что здесь его не туда появилось но соответственно если он увеличился то у нас здесь получается а да то есть у нас получается есть магнитуда из градиента вырос то есть у нас как будто шаг да выделился тоже то есть вот этом он же нажав<|tl|> на лен крейт но почту вилеса то есть мы как бы грубо говоря начинает сильнее идти туда в сторону изменения пошли в обыкновение этого вектора это же вектора но он туда свои он тоже величился и тем самым чем он больше да чем он магнитута выросла да он больше пойдет туда типа его его рахнута надо сильнее да это это это как его функция да а я ссория я что-то подумал просто ну типа что мы дерева ите в берем вот только тимас я я что-то забыл ну еще даже не думал о том что мы еще этот от тотал кста берем дерева ите в по дидаболу да да да да да то есть условно он например если вот мы на параболе там сколько было раз два три четыре итерации да или пять итерации то за счет вот этой вот этой и л1 или л2 не важно за счет того что новую параметры и увеличился шаг он уже надо идет быстрее типа за три итерации да вот за три таких но это как бы да но это не совсем то есть это это не это не как сказать это не это не целевое применение было это не целево принять чтобы ускорить это все это не про ускорение это больше про то что нужно штрафовать сильнее именно определенные веса да ну не определенные а именно веса которые большие да то есть у нас был бы большой спец мы бы сильнее в тюрьму либо то есть если висит то в данном случае у нас как маленькая функция потери то она как бы она не быстрее бы сходилось если большая тут быстрее бдап и лап то есть здесь больше того чтобы дать нашим нашей градентной функции нашей функции потери информацию что нужно пенализить наши веса больше да каким-то определенным параметр то есть это больше про эту историю то есть мы даем вводную информацию для нашей функции функции потери что бы пожалуйста там давай в траф и больше на дабл ее получается это пока не про скорость ну как косминфектов а вот там скорость конечно есть но я не думаю как бы что это целевая так можно спросить в каком случае применять вот вас в каком случае л2 лично вопрос я как раз к этому что да вопрос что-то что это еще вопрос был да перебивает да да да как раз комментарий к этому вопросу каких случаях этот легализация л1 л2 то есть и как оно влияет просто мы раньше проходили что что вот когда мы фьючер селекша делаем л1 он для того чтобы и релевант фьючера ну брать она типа используется а л2 чтобы все фьючера оставить но минимизировать эффект тех которые ну меньше влияет и ну как это влияет ну то есть на самом деле это легко понять но из формулы вот тяжело то что допустим л1 с помощью л1 можно избавиться от некоторых фьючеров чтобы их вейты прям ровно до нуля спустить пенализирую это было объяснение или пол вопрос это был вопрос я просто не знаю как он спускает до нуля на форму не понял хорошо понял а пострадавец и вот до этого парен тоже такой вопрос был да да у него в каких случаях используем л1 отлично хороший вопрос но поэтому я диграмму донарисовал сюда представить хороший вопрос все верно вот кто последний говорил имя не видел часто извините л1 да обычная л1 используется как помимо того что он еще регулизатора да он еще является таким аля фьючер селектор то есть он помогает еще там за селектор фьючер почему потому что он обращает какие то дабл юз да в нули полные нули сейчас объясню почему а л2 он тоже регулирует но он не факт что приведет ваши фичи в ноль некоторые потому что следующая история начинается смотрите так мне это не нравится картинка покрасивее нашел когда я увидел ст persymity все виды да да сам рэ offensive Fly direction es это р1 это Давайте так. Все понимают интуитивно, почему L2 это круг, а L1 это ромбик. Только честно, пожалуйста. Здесь честно, будьте. Уровнение серкала через квадратное же. L1 это линейное, поэтому L2 квадратичное. Да, хорошо. Ну да, хорошо. Л1, а L2 это абсолют value. И за это ромбик формируется. Хорошо, отлично. Да, да, да. Ну да, это получается просто из математики. Хорошо. Если возьмем, да, как бы да, из квадрата, или W1, W2, это будет 1, да? Ну, R2 будет, да? Ну, это 1, потому что мы почти нормализовали, да? Ну, без разницы, окей. Ну, это уровень шара, да? Не шара, круга. А L2, да, это потому что у них зависимость. Например, у каждого из них она линейная, да? Потому что, ну, просто W. Ну, еще, как я уже сказала, она имеет модуль абсолютного значения. Вот поэтому и рамбообразный у него вид имеется. Окей, если мы здесь на синхронизации, это окей. Смотрите, как я говорил, L1 это наш ромбик, L2 это наш круг. Смотрите, получается графикал немного запутан, но в целом мы можем понять. Смотрите, представьте, что мы сверху смотрим, да? То есть это 3D, да? Ну, это, как я уже сказал, это в экран торчит, это типа, ну, какого-то L, да? Потеря, функция потери. Основно говоря, она лежит на такой-такой контрплот. На плоскости лежат W1, W2, это наши веса. Ну, мы будем стремиться все время. Соответственно, как бы, как сказать, как бы объяснить, точнее, чтобы понятно было. Мы когда оптимизируем задачу, мы вот с этой точки, да, вот так бегаем, спускаемся, да, и пытаемся вот сюда вниз, да, прийти, здесь то же самое, да, вот так, вот так, вот так. Ну, не обязательно с этой, да, с этой, с этой, с этой, с этой, с этой, без разницы. Это для наглядности просто показали, чтобы близко не стоит. Смотрите внимательно. По своей, да, по своей, вообще, природе, да, ну, здесь посмотреть, да, давайте L1 посмотрим. Ввиду того, что геометрическая фигура у нас является здесь, мы видим, да, что наше пересчечение, да, где наши веса могут пересчесться с этой кривой, они равны, равны, получается, на W0. W1 равна 0. Как-то странно сказал. То есть, как бы, шансов того, что у нас попадет сюда, куда-то, да, она только в двух точках присутствует. Вот здесь и вот здесь. То есть, видите, у нее нет пересчения там, где и то и то значение имеет какое-то промежуток значения. То есть, и для W, да, вот здесь, и для этого, да, вот где-то здесь. То есть, единственное, где она может пересчесться с этой функцией, это вот так вот здесь, а вот здесь или вот здесь, да, в стану говоря. А вот здесь, как раз таки, да, почему она не обращается в то никогда? Ну, там, может, редко прийти. Потому что, видите, у него вот эта вот поверхность, да, она шире выстоит. Поэтому мы можем попасть и сюда, и сюда, и сюда, и сюда, да. Поэтому это значение, да, она может быть и приблизится, да, сюда вот близко, да, но она типа по W1, да, не будет 0 никогда. Ну, по идее, может стать тоже, если очень-очень близко, то 0 станет. Не знаю, понятно ли обе с тем. Не всем, просто говоря, вот можно вот этот... То есть, правильно понимаешь, что для W1, вот у нас же, где ось такого овала, в принципе, вот касательно не пройдет через вот точки посередине W1 и W2 на ромбике, да, ну, там, вот где в центре ромба, там условно. Точнее, по краю между двумя точками, вот, где W2 0, где W1 0, мы не коснемся ее просто этим овалом. Овал, условно говоря, с кругом может хоть где-то столкнуться. Ну, где, да, вот здесь, как в перечень, есть регион, где он может перечечься с ним. Ну, это одна из просто обид, это, это, в самом деле, ну, в самом деле мне не всем нравится объяснение, потому что это больше геометрическое объяснение. Я просто читал, где-то там более математическое было, через формулу они показали, что там в целом, в среднем, в среднем, через математику вывели, так что в среднем, у тебя очень часто обращаются они вдоль, на L1. Здесь тоже какой-то шанс есть попасть вот в этот регион, а здесь, в общем, в общем, в общем, либо это, либо вот это, и все, типа, тебе нет другого. А здесь вот можно просто тут, на графике, с экрана, на нас, это же lost function, то есть, в данном? Да, да, да, это, да, да, да, типа, внутрь. Ага, вопрос? Кто был? Ирия? Алло, меня слышно, да? Да, слышно, нет вопросов. А, окей, хорошо. Так, сейчас, это что-то было сейчас? У нас feature slider, это же Loss or Reach? L1. А можно тоже еще там, мини вопрос, я не знаю, стоит ли время тратить уже. Ну, выглядит так, что, типа, L1 борется с мультиколинярностью. Вот, типа из Romba. Можете ли, ну, это так мысль chances нет. Я у Чатика спрошу. Я сейчас 진м 중요한 вопрос, тринадцатая, Chatering. Что indifferent, Yarna, с какой 진м, 진짜 или что? я сейчас не скажу может быть и да а почему смысл я почему так почему так решил из чего здесь додумаю может быть ну я так подумал о том что на смысл что что тебе привело к этому ну типа дай у нас есть линейная комбинация вот параметров и используя л1 мы приведем вот эти параметры условно некоторые к нулю и если два параметра ну я так подумал что вот есть два параметра поставить 1 во 2 чечня ну две фичи и вот как бы их реальные там пересечение с функция ошибки ну типа минимально будет именно в точке где один параметр мы вновь уберем о второй типа ну какое это значение типа из-за того что ромбик не пересечаться между ними ну вот а ваточнее между ромбиками а про каленерацию про мультикальнарац не всем полагает здесь где именно за сапплитами ну то есть тебе что теперь свечи да одна x1 да другая 3х да типа она линейная линейная трансформация между ними на то в три раза больше да типа да и дальше что с ним понял ну я что-то совал вот этого подумал что типа условно имею два параметра просто ну она да да да давайте дальше поделим я учатся прошу я не знаю окей хорошо так в целом да уберем окей в целом да не спонятно вопрос нет может почитайте до дома еще про регулизацию то что очень мощная тема но теперь пойдем до вот это пустинка этот раз мы проходили про адаптер бустинг хотел бы более подробнее на него остановиться помните это бустинг алгоритмы которые как бы и исполнивают вы кернер и да внутри себя для того чтобы сделать более сильные модели у них sequential learning process то есть они постоянно обучаются то есть одно дерево дальше за ним дичь другой дерево 3 дерево 4 дерево и так далее то есть это не парили да как рандом по вас работает вот и адаптер бустинг он работает так что ну а допустим так что она дает она взвешивает определенные какие-то сэмплы на которых она хуже работала чем выше да до этого веса на самом которых она показала себя лучше да и хорошо дело то есть она их не дает так много веса им обучение тем самым она но адаптивно выбирает более сложные случаи для себя во время учения это так в целом в целом тут такой псевдокод описан пожалуйста и вы изучите почитайте очень ну что мы еще к чему вернемся во время примера в целом еще написал такой интересный этот но каково в полчарт а да услугу гритнед мы начинаем старта да интервизируем какие-то веса равные да вернее веса для наших самплов ну обычную униформ делаем так чтобы для каждого веса для каждого примера нашим сэмпле был одинаковый вес допустим 111 везде там один директ на стол стоп примеров там получит одно сотый веса для каждого примера обучаем первым первого век лорнера на взвешенной дате да считаем ошибку как лорна вы считаем получит пересчитываем вес век лорнера далее апдейтим ну там правила правила рлин стоппинг критерии да нет если нет то опять заходим уже как его обучать второй век лорнер на уже пересвешенный данных да потому что мы здесь мы здесь апдейтим веса и так далее и тротина и тротина дилом получается мы каждый и это рацию в этом лупе мы пересвешиваем наши веса нашим примером и тем самым когда с таким учетом обучаемся мы обучаем нашу наши деревья внутри век лорнеры с учетом с учетом получет и веса этих примеров вот такой как бы по версии но больше ноутбука с вами будем тратить время вот теперь давайте посмотрим проговорим про гранит бусинг, да, повторим то же самое так, че там при комментарии чат я в частике спросил, реально помогает L1 смыть скалинирность и объяснение только более адекватное, чем я смог какой вопрос возникает? то есть это в принципе бустинг использует несколько моделей для обучения и выбирает на основе предиктов нескольких моделей ну, то есть определенный предикт, да? ну, сори за это автология я правильно понял? еще раз, не всем понял вопроса, простой, пожалуйста вот то, что вы сейчас объясняли про бустинг вы говорили, что это как то есть в бустинге есть несколько моделей для обучения то есть это decision tree, еще какой-то там модели, да, машинного обучения и на основе этого он обучает то есть предиктов этих нескольких моделей выдают свой предикт, да? так, если я понял корректно, да, получается в целом у нас получается каскад из моделей, да, слабых обучаемых моделей, типа там деревья, решений мы их каскад, то есть в ряд строим последовательно, то есть каждый дерево один построили, да, первое дерево обучили его, посчитали его ошибку и эту ошибку дальше берем и учитываем во время обучения второго дерева то есть тем самым, как бы у нас второе дерево будет учитывать ошибки передущего дерева в своем учении а, все? и третье дерево будет да, также учитывать ошибки второго дерева, и так далее про поколение, да это вы имеете в виду про light gbm, который используется, да? да, совершенно верно, light gbm, hgboost, catboost это просто фреймворки для границ бустинга потому что я сейчас здесь объясняю, это ванильные модели это просто основа, да? то есть посмотрите, на самом деле, hgboost это тоже самый границ бустинг просто там у них свой оптимизированный фреймворк есть, да и там чуть-чуть есть вариации того, как они деревья сплитят ну, чуть-чуть есть, то есть hgboost он чуть по-другому устроит бустинг catboost он тоже чуть по-другому устроит, light gbm он тоже по-другому чуть-чуть, там вариации небольшие в основном там больше, кому как удобнее имплемитации кода иметь и у catboost как-то еще категорийные признаки под капотом существуют поэтому категория catboost называется light gbm, касс, очень оптимизированный, быстрый потому что на плюсах написано вроде бы а hgboost такой, кажется, он наиболее функциональный понял, спасибо это просто фреймворки, на самом деле в esclerd тоже есть границ бустинг, но они не такие оптимизированные долгие слегка давай, двигаемся дальше границ бустинг подошли, все то же самое, что раньше объяснял core-концепт, то, чтобы оптимизировать дифференцированную функцию потери об этом поговорим сейчас additivemodel, то есть мы получается каскадно обучаем модели и вот, в принципе, все в докод сейчас объясним подробнее в коде уже так, здесь тоже самое, такой легкий, очень простейший диаграмма то есть мы инициализируем модель каким-то residual, residual это получается разница между нашим предиктом и нашим трудозначением и разом инициализируем наш прогноз модели, как будто бы, каскадное значение 0, допустим, считаем pseudo residual остатки, разница ошибок заводим сюда обучаем base learner на основе наших pederisitals то есть, мы берем первую модель и обучаем ее разницей между 0 и разницей между нашим этим целым значением обучились, посчитали step size мультиплеер, сейчас объясню, что это такое, лямбда и обдати модель далее, итерация итеративная, получается, вычитываем раз за разом residual на нашей модели и тем самым обучаем каждый новый модель на наших residual такое поверхственности к объяснению но, в целом, я думаю, это не так важно здесь сегодня так, давайте теперь сейчас перейдем плавно уже срисов ноутбук и и и и и и и и так, смотрите два rada boost два rada grd датасет вот смотрите, что я здесь сделал вообще, я бы хотел бы, чтобы, советую, рекомендую каждому предпосмотреть этого ноутбука потом предснимание на реализацию from scratch и и и и и и и и и и и и и и и и и и и и и и и и и и и вот так же есть метод fit, который уже обучает, фитит наши модели на фото приходит параметры, обучение, набор обучения входных данных, набор наших таргетов наш tolerance, получается наша терпимость к изменению в ошибке то есть получается у нас модель перестает ну считает уже нет изменений, если изменения в дельте меньше, чем вот эта tolerance то есть 10 степени минус 4 то есть 1000, меньше 1000 мы уже не чувствуем во время обучения так же maxiteration, это для деревьев скорее всего не для деревьев, это для stopping критерии дополнительный стейт, то есть не больше 100 пробегов хватает для обучения окей, здесь просто инстализируем веса ой, не веса, а наша размерность, первое количество сэмплов, количество фичей, их shape далее создаем пока просто наш w ну как сказать, это вес, обучаем параметр который будет получается размерность samples с значением 1 делина samples ну то есть получается это, если у нас 100 стэмплов, это вес будет 1 делина 100 то есть у нас будет матрица w да, или спектр w размерность 100 на 1 первая ось это будет вес сигнала то есть 0.1, если при стэмплов равно 100 сейчас посмотрим, я сейчас покажу вам это просто инстализация так же, смотрите, так как это еще до лупа я говорю, previous error равно infinity то есть просто задаю так, чтобы передующая ошибка была бесконечно большая, дабы не перекрыть ее и соответственно, я начинаю while loop загоняю в сам обучение модели дальше это интиматоры количество итерации, это меньше, чем максимальный квизитист из стиматоров, то заходим создаю модель, я просто унаследую из этой библиотеки decision tree classifier классификатор древа решений я указываю, что он не больше 5 это глубина дерева и обучаю модель загоняю x на y и загоняю sample weight благо я могу передать в decision tree дополнительный вес по которому визвесить нужно как это сказать я посмотрю, как они так говорят вот, сейчас нот есть нот ну, получается, мы дополнительно в этом нашем даплете, который мы здесь инцидировали, мы дополнительно еще взвешиваем примеры в нашем деле, во время учения потому что, помните, мы говорили, что графика с формулой adaboost в том, что adaboost работает так, что он пытается во время обучения простейших квит-клонов он обучается, помимо того, чтобы просто их обучить он еще пытается взвесить наши сэмплы у нас на каждой сэмпл у него есть w какой-то вес и после на каждой вот этот сэмпл мы пытаемся найти этот вес определенный но мы ему и в время обучения здесь его передадем вот, обучились хорошо, фит сделали далее делим predict то есть, predict на x, то же самое потом prediction получаем и вчитаем error error считается несоответственно true и false здесь будет буллин внутри будет буллин помноженное на наш вес здесь понятно, что происходит если подробнее или не надо если честно, вообще что-то непонятно то есть, мы матрицу весов мы набираем либо на 0, либо на 1 то есть, получается, у нас если попадание есть если у нас 1,1 1,0,1,0 попадается 1,1, то это true 1 равен 1, true умножается на вес, вес сохраняется если prediction не равен 0, то есть 0 false prediction не равен y, это будет false false 0, y и y 0 умножается на w, соответственно w стоит 0 не w, а ошибка то есть, ошибка считается, мы суммируем потом это все то есть, мы здесь, по сути, что делаем? этот error, это равна сумма сумма всех наших весов на которых мы получается как сказать ну, промахнулись на которых мы не смогли опасть ну, на которых мы ошиблись поэтому это error логично ну, то есть, получается допустим, y да? давайте так обшу y, наш оригинальный, правильный true label равен 1 мой prediction равен 0 то есть, ошибся естественно, я говорю, да? ну, отсюда будет следующая, да? будет 1, верно? true true это 1 а 1 важно 1, но 1 важно w равно w, да? здесь будет 10 вес, то есть, вес будет суммировать мы будем суммировать все веса, на которых мы ошиблись то есть, в этот error заcontributed только те сэмплы, на которых мы ошиблись то есть, понимаете, да? теперь прослеживается логика адаптивного густинга тем самым, что помните, я говорил, что она конфигурирует веса адаптирует только на тех, на которых она ошибается то есть, видите, в этот error в этот contribution этого error веса, на которых мы здесь, да? все поняли, да то есть, это легкие примеры, да, условно, игра а на плохих, на сложных, она true 1, равно w заходит вот можешь, пожалуйста, объяснить вот model fit, то есть, модель создает классификацию дерева, то есть создает дерево с максимальным значением 5, да? да, это так Расслуда говорит да, да, да да, смотрите, все верно, но не всем, смотрите на этой строчке я создаю объект дерева то есть, я создаю модель, это объект дерева который у нас следуется строиться на основе класса decision3 это уже класс импортируемый на скайлер 3 видите отсюда? я говорю, что максимальная 5, это не факш 5 не больше 5, оно может быть и 2 быть а модель fit у меня вопрос да, а модель fit, получается, да, это вот обычно, помните, мы этот раз обучали это просто обучение класса, обучение модели то есть, это просто заводжим модел обучись на вот x, набор обучающих, набор фичей и таргетов, с учетом еще в дополнительной матрице w можно еще вопрос я вот смотрю и все равно не могу понять если у меня y равен 1 а продикцион равен 0 0 равен 1 будет же false же почему он true равен? где же стоит неравно? а, это неравно, да? это логическая неравно то, что неравно становится 1, то равно становится 0 вот как бы это то, что мы здесь вообще объясняем, это просто это то, что было в той формуле, в лекции просто в виде формулы, сложнее считать это вот то же самое и тем самым, видите, мы наш рост считаем на основе только сложных примеров далее здесь просто stopring критерии, то есть основочные критерии до того, как нам двигаться дальше или не двигаться получается, смотрите, первое stop условие второе, это итерация добро то есть мы мы достигли максимально количественной итерации и еще, что итерация текущая, она равна или больше максимальной итерации то ломаться, то есть завершить цикл перейти к учению если это не срабатывает, как guardrail, то мы дальше идем сюда смотрим, что если предыдущая ошибка которая равна, в данном случае, инфинити так как это еще первый запуск отнять от ошибки текущие, которые посчитали и вот это равнится ошибок между предыдущей и текущей итерацией меньше, чем tolerance, который раньше был, наш отчучительность обучения то тоже ломаться ниже этого значения мы не рассматриваем как изменение потому что мы не можем до бесконечности идти если это оба не работают, то обучаемся дальше переписываем previous error с infinity на текущий error извиняюсь, мне интересно было всегда, когда в кодах пишут почему мы отнимаем previous error от eror что это нам дает и почему именно должна быть меньше tolerance так, это кто говорит сейчас? это соленжан хороший пример, хороший вопрос вопрос был такой, почему мы здесь в этом куске ломаемся, да? да, почему мы не можем сделать так чтобы оно было больше tolerance почему именно, чтобы оно должно быть меньше да, остальное все просто потому что оно меньше tolerance то есть мы берем разницу предыдущие ошибки от текущей ошибки разницу взяли, взяли абсолютное значение чтобы было плюсовое значение и говорим, если оно меньше текущего tolerance который мы задали, то ломайся это значит, другими словами, что если мы на 20, 25 и трансе у нас ошибки стали очень маленькими в этой ошибке 0.401 и 0.40011 то есть там получается 0.501 разница ошибка, абсолютно очень маленькое значение естественно, мы говорим, что это значение меньше чем 10-4 и мы говорим, что изменения в ошибках наших итераций в реам в учении, да? настолько малы, что они меньше tolerance поэтому мы пришли к выводу, что нет смысла обучаться у тебя уже нет изменения в обучении сильного раз у тебя раненый в ошибках, перешел за tolerance это получается tolerance tolerance это значение после которого не обязательно уже обучение продолжать потому что мы пришли уже к такому несгораемому остатку бесконечному уже нет смысла обучаться далейшее обучение не имеет смысла потому что модель, которая будет делать ошибку 10-16 и 10-20 она будет так же работать если дальше уходить еще вот вопрос был мне вот интересно было вы когда говорили про overfitting ладно, если с underfitting все понятно модель будет максимально плохо себя показывать если в задачах, к примеру и обычный фитинг, обычное обучение для каких-то задач я понимаю, что это абсурдно но может ли overfitting иногда выступать в качестве лучшей модели, чем обычное обучение для меня вопрос очень хороший очень хороший вопрос ты говоришь, что бывают ли такие случаи, когда overfitting в текущих обучениях по целью чтобы она выявляла какие-то ошибки что угодно под копирку тогда это уже будет не машинное обучение я понял, когда у нас всегда повторяются те же примеры задачи но это не отличается от обычных ручных правил по сути, смотрите машинное обучение это я не говорю про нейронные сети это другая механика мы говорим про дерево решений дерево решения это огромный набор правил больше и меньше это и фэлсы но куда более изощреннее правила имеется если вы хотите говорить про overfitting и где это нормально тогда мы можем исследовать что ваши задачи требуют обычных правил при условии того, что ваши сэмплы всегда будут одинаковыми тогда вам нужно 100% наточность если вы гарантируете, что ваши сэмплы всегда будут одной и той же природы тогда это будет overfitting но такое никогда не бывает потому что у вас никогда не будет в реальной задачи что ваши последующие примеры это очень важно если не ошибаюсь то в нейронных сетях и в диплерниге это не по методике машинного обучения это работает если не ошибаюсь то в нейронных сетях то тоже decision tree если не ошибаюсь то в random forest используют ли они эти функции машинного обучения в обучении нейронных сетей используют ли нейронные сети какие функции еще раз? функции машинного обучения инструменты машинного обучения люди разделяют или группируют это все называется машинное обучение та же самая логическая регрессия если вы поймете это своего рода нейронная сеть а дослойная там вопрос имелось в виду используем ли мы как работает нейронная сеть использует ли она то, что мы сейчас проходим конечно все эти концепты пригодятся в понимании того, что мы будем делать дальше вопрос добавлю чтобы с хорошим инженером машинного обучения неважно занимаетесь ли вы если вы не знаете о нейронной сети или о нейронной сети вам нужно знать хорошую базу если у вас не будет понимания элементарно что это такое или как считают это градиенты спуск, мс, спуск это же основа не говоря о нейронной сети нейронная сеть это общее название есть сложные архитектуры трансформеры гпт, антропики дали, миджорни у них пара входа будет для вас выше если вы не понимаете что такое нет такой формулы обойти, резать здесь не надо, здесь не надо такого не бывает единственная формула в всех это делать все до глубины как можно больше нет никаких шорткатов поэтому если у вас есть сомнения и где-то закладывается пожалуйста, убейте их доверьтесь процессу можно идти дальше? да смотрите может тоже будем учиться нейронной сетям рандом форест как вы думаете, какой урок? сейчас идет седьмой какой еще раз вопрос был? когда будем изучать нейронную сеть? начнете примерно в ноябре 18-го числа хорошо спасибо там будет проект, деплой все кажется нейронной сети хотят изучать это интересно фото, видео ставить это интересно это куда более нагляднее алгоритм все-таки нужно понимаете чтобы на самом деле не на обувательском уровне понимать как работают нейронные сети нужно вставить копипаста и запустить а реально понимать что под капотом происходит лучше понимать с этих основ что мы сейчас проходим я не совру я сам года 3-4 только с нейростями работаю с классическими ML но это все равно мне помогает гораздо глубже концепты нейросетей понимать классика ML как такового сейчас уже теряет свою актуальность на рынке для использования большая часть людей использует нейросети нет, такого нет как я говорил ранее в этом году я не имею в виду не в плане чемпионата я общался с одним инженером и он говорит что ML сейчас как таковый теряет актуальность нейросети это задача зависит да давайте поговорим о банке банки используют в проекционных вещах скрифтный скоринг но в основном доминирующим моделей и модельного ряда это не хайп это хорошие модели которые 50% в ряд но они не универсальные они не могут решить много задач которые классический ML на раз-два щелкает поэтому сказать что классический ML Uber это глубокое заблуждение есть много задач где нейросети вообще не справятся где легко справятся гранденты бустят хорошо, спасибо большое опять такие учу чазы давайте вернемся сюда вы же этот код отправите? не код, а сам да, я уже отправил альфа да после обучения после проверки мы делаем альфа равно learning rate кто помнит эту функцию? бинарная кроссентропия да, бинарная кроссентропия лог лог надувай это еще пересчитываем наш альфа мы умножаем наш learning rate на нашу сейчас программу в этом видео это в патреоне вы можете посмотреть на моем канале что я делаю что я делаю вот обдатали альфу нашу далее еще делаем перевзвешивание наших W мы даже обдадим W вот этот подвох Это не log loss, это не crossentropy. Это несколько другая функция, которая просто для альфы считается. Я бы что-то сам запутался сейчас с игрой. Это не то. Это немного другая функция. Посмотрите на 5-й слайд, это под пунктом C. Альфа T равно пополам, на natural на грифм и фракция какая-то есть. Вот. Далее смотрите вот здесь. Это просто альфы посчитали. Далее мы берем W. И W равно старое W умножить на значение. То есть в этой строке мы назначаем нашим W новое значение. Опять-таки только там, где мы не смогли хорошо продвигать. На VT-10 предусловие true стоит. Потом далее мы нормализуем наше W. Ну и все. И в целом в таком fashion мы обучаем наш Adaboosting. Поэтому на следующий раз рекомендую ознакомиться с алгоритмом. Понять от корки до корки, как это работает. Такой имплементации легкий. Вот здесь держат просто наши предикты. Высчитывается это с топ-критерией. Принты сделают. Ставим скейлер. Мы инцидентировали. Ой, а вы про предикт можете сказать, что является ответом? Что он делает на предикте? Это классификация. Я понял, как он высчитывает вероятность. Получается сумма, альфа. Получается вот. Мы загоняем такую нехитрую функцию. Мы поняли, что это же каскад моделей. Мы не запендим. Мы запендили модели. Альфы запендили модельные листики. А, и мы для каждой модели делаем предикты и суммируем их. Умножим на эту точку. Перед этим мы еще переносим на альфу. Да, да. На альфу, на предикт. И потом мы суммируем это все. И после того, как это все просуммерилось, мы делаем sign. Знаковую функцию. Плюс, минус, если помните, математика. И все. Минус один, либо один. И все. Еще вопрос такой. Все знают, что такое стандарт скейлер. Работали ли со скалированных данных? Ну, я да. Ну, то есть, грубо говоря, да, это просто скалирование для того, чтобы отнормализовать наши данные. В данном случае это у нас минвак скейлер. Вот, допустим, если посмотрите, почитайте. Он переобразует, да, как я понял? Да, да, да. Это своего рода нормализация. Ну, вернее, стандартизация скейлера. Чтобы наши, да, вот, вот, стандартные фитебы ремонтируем измены скалирования туджинфалец. То есть, грубо говоря, мы делаем наши данные таким образом, чтобы они стали нормальными. А именно, не то что нормальными, а еще и чтобы у них была средняя, да, мин. Эта популяция на нуле, а стандарт отклонения равно 1. Это была стандартизация прям. А это получается, по сути, Z-score, да? Да, да, все верно, Z-score. По сути, Z-sublitz, да. Ну, смотрите, у нас есть нормализация, да, того, когда мы просто приводим в нормальное распределение. А есть еще стандартизация, да? Раненцию знаете, да, вы? То есть, нормализация, стандартизация, это часто случается в нормализации, когда при том, что у нас мю равно нулю, а stdv, 100% отклонений, да, равно 1. То есть, грубо говоря, у вас бывает же нормальное распределение, что вы делаете, да, 100% и стандарт, да, и 3 равных. При этом это нормальное распределение будет, но оно не стандартизировано, да? Вот, поэтому здесь ну, это про это, по сути. То есть, мы когда заводим его, мы все фичи, да, все наши оси по колонкам заводим его и внутри себя стандартизируем Z-score. Дальше сплитеем просто, обучаем, фитием, предикты получаем, скорые, морые там, все дела короче, ну и запускаем. Вот, посмотрите, вот уже видите, просто с трех итераций она стопнулась, потому что, видимо, она ударилась в предел толеранца, да? Видимо, она, видите, я им говорил, что давай, обучись на трести итерациях, да? Ну, а она, видите, уже на третий уже остановилась, потому что она ударилась в stopping критерии, да, вот сюда. Видимо, по этому значению. То есть, у нас уже ошибка, да, равница между ошибками двух периодов, она была нижней нашей пороговой ошибки, толеранца. Поэтому снимилась, то есть дальше не смысл учиться, потому что, ну, бесплатно. И видим вот, метрики наши, да, неплохие вполне, очень отличные, и вот наш фич импотенц. В данном случае фич импотенц это, на самом деле, просто наши веса, наши параметры. Вот. Впереди, да, дальше, помните, я говорил, у нас по PSI Lab у нас идет Model Evaluation, оценка на модели. Мы будем поговорить, просто, видите, мы как-то так сложились, что вообще, как бы мы как будто сперва прошли модели, потом мы начинаем метрики смотреть, хотя мы метрики смотрели, по факту, уже чуть раньше, во время моделей. Ну, здесь подробнее поговорим про что такое Accuracy, Precision, Recall F1 и различные еще вариации. Там очень много деталей есть. Так, здесь вопросы есть по A-boost? Ну, помимо, может, только вот это, может быть, здесь какие-то вопросы? Помимо, типа здесь есть вопрос, но, если не сдавать, я могу сам посмотреть. А, ну, в смысле, имеется в виду, да, может быть, здесь какие-то вопросы, давайте посмотрим вместе. Один вопрос, здесь sample weight равно W, как штука? Типа, мы вот W это как раз наши веса, ну, типа наши параметры модели, матрица параметров. А... Не-не-не, не совсем, здесь ошибочка. Вот это W, да? Это не веса модели. Ну, это веса примеров. А, веса, да, веса. Это веса примеров, да, это не модельные веса. Это другие. Как она fit заходит? То есть, как она adjusts, получается, сам decision tree во время fit? Типа, чем fit без вот этой W будет отличаться от вот этой, ну, с тем, что W? Хороший вопрос. Так, это я хочу воспользоваться тем, что написано в документации. Да, смотрите, получается, это заходит как параметры, x, y, здесь понятно все. Здесь заходит sample weight. То есть мы заходим сюда, как какой-то sample weight, от нас, которые мы, говорим, используем, вот эти веса, для того, чтобы весить, да, примеры. Соответственно, что происходит под капотом, в самом деле, просто строится такое же дерево, как и до этого, да, мы строили. Но при этом вес каждого, ну, не то чтобы вес, а, как сказать, вот эта W, да? Типа значимость ошибки, да, вот я так думал. Типа, условно, если у нас у модели маленький вес, то и ошибки в построенном tree, ну, которые вы достали, типа output и ошибки вот этой модели, они будут, типа, незначительны, да, для вот этого. Так, не совсем, не совсем, потому что, не, не совсем. Получается, здесь что происходит? Ввиду того, что вот этот sample weight W, он размерности в точно совпадающих с количеством samples data set, да? То есть у нас на каждой пример, обучающий, есть какой-то вес. Вначале он uniform, да, распределенный полностью, равный. Но потом мы его тюнем. Соответственно, вот этот вес, он перемножается, взвешивает все наши примеры, все значения наших примеров. И то есть они уже, когда дерево строится, они уже взвешены становятся. Ну, если прост, вот еще раз, если простыми словами, типа у нас есть какие-то хорошие примеры, есть какие-то плохие примеры. Легкие, легкие и сложные примеры, да. И чем больше получается, ну, типа, чем сложнее пример, тем больше веса, ну, то есть тем больше decision tree будет именно под него китаться. Да, по сути, да. То есть по сути, мы же как бы итеративно обучили, да, и у нас вот это веса немного меняется, да, распределение весов. Легкие поднижаются, типа с ними все окей, давай акцент отдавать. Соответственно, да, она будет, как бы во время обучения просчета градиентов, она будет давать акценты на те примеры, то есть она будет градиент, вот этот, оптимизировать, да, под больше те примеры, которые касачили больше, да. Вау, это прикольно. Да, я все понял сейчас. Вот это прям классно объяснили. А я не знаю, что так можно сделать. Да, она понимает, как бы, где был косяк больше, да, ну, потому что вес увеличился в этих примерах. И она понимает математически, что в этом пространстве нужно идти туда, где косяка больше было. Прикольно. Вес больше был. Я думаю, только надо boost, градиент не так работает, да? Градиент немного по-другому, да, там чуть хитрый здесь. Вопрос за HAC? А может вопрос? Это формула, α равняется self learning rate, NP log 1-error, вот. Это формула что была, я не могу? Так, еще раз, еще раз, еще раз, как? Вот формула, вот, NP log 1-error, вделенный error, здесь NP log, len, α, по равно self learning rate, умножаем на NP log 1-error. Вот этот логриф мы... Формула, альфы. Вот это, да? Вот это. Да, а что за формула, я не могу запутать в этом? Это же не баня для треосердинга. Это не она, да, я бы говорился, потом сказал, что это не она. Это получается формуле на шестпятом фаде, пункте. Можно вопрос, вот мы до этого обсуждали регулирующие, вот эти L1, L2, они здесь представлены? Здесь нет. Это больше экологическое, не, не игре, символ. Смотрите, почему здесь нет, потому что, смотрите, именно в бусингах и в бейкингах, да, ансамбль, у них, смотрите, регулирующие, бейкинг, ее принципы, смотрите, регулирующие в бейкинга, ее, в принципе, не существует. Почему? Потому что, помните, да, что там же был рандом форус, да, как пример, регулирующие, пример бейкинга. В соответствии, смотрите, если вы помните, да, рандом форус, это много, много древ-решений, которые обучаются паразельно, и которые, каждый из которых деревья, получает только, ну, рандомно, на некоторые фичи, не все фичи одновременно. В соответствии, кстати, таким образом, видите, в данном случае у нас есть уже, ну, как сказать, естественным путем мы внедряем туда регулирующие. То есть мы, условно говоря, уже как-то делаем так, чтобы модель, ну, опаркинили, делали же сложности какие-то, дискарж, да, ее модель. Потому что, видите, мы каким-то даем, эти фичи, каким-то не даем эти фичи, каким-то даем эти, а тем не даем. Ну, и все медуем. Это уже по своей постепени здесь регулизация. Бустинги как таково, L1, Ridge, Lasso, такого здесь нет. Там есть регулизация, но немного по-другому называется. Здесь мы пока их покрываем. Но у них есть регулизации, только там они немного в другой форме выглядят. Там, типа, идет как, оставляющие критерии, регулировка глубины дерева, такого рода. Прунирование, но это немного вне этого курса обсуждаться. То есть такого, что, как в МСЕ, добавить λ плюс сумму w в квадрате, такого здесь нет вообще. Это здесь можно полегче быть. То есть L1 и L2, они только в линейных регрессиях и в фонте. Линейных регрессиях, да-да-да. Ну, может быть, там еще поймаете, видите, еще раз скажу, что там этот курс очень такой, все равно хоть бы он такой на инженера, но на самом деле он очень поверхностный. То есть, вообще, можно вот линейные регрессии, можно вообще было вести лекцию по ней месяца два-три. По одной только линейной регрессии. Там очень много глубин, да, еще. То есть, я к тому, что вы тоже не получили ложную информацию того, что, типа, там только L1 для линейной, на самом деле, там еще очень много видов регрессий. Там изотоническая, да, регрессия, k-квадрат, да, регрессия. Там, ну, там их много вариантов есть. То есть, то, что мы здесь проходим, мы проходим больше ориентированно на такую пысую практику. А много ли, много ли из этих регрессий из этого всего используется реальная жизнь на практике? Чего я просто был? А все ли из этого используется на практике? Не все, но оно в другом другу зависит. Смотрите, в общем, в линейной регрессии, наверное, редко здесь появится. Спецфизиазм, да, потому что, ну, постепенно все просто, это просто линия, да, ровная. Но надо какие-то легкие прогнозы, да, понимание вообще, как рынок там движется. Вот. Но в целом, не забудьте, есть изотонические регрессии, да, а вот они используются, допустим, на фондовых рынках, вот они заодно используются. Но, опять-таки, они в основе используют в линейной регрессии под капотом. То есть, если вы не понимаете, как работает обычная модель, вряд ли вы поймете изотоник регрессий, как работает. То есть, вообще, сам курс полностью, AI, он очень сильно скомкан, да? То есть, есть какие-то определенные доп-материалы, которые нам нужно изучать. Будут ли эти доп-материалы по ходу курса? Да, конечно. Ну, то есть, вообще, мы это закладываем в домашних зданиях. То есть, домашние здания, это не так, что вы пришли в лекции, посмотрели, и потому что в лекции есть, вы так просто перенесли, и все. В основном, там будет очень много моментов таких, что, есть что-то, что не было в лекциях, допустим, что требует вашего дополнительного изучения этой области. Ну, я не говорю, что это будет прям вообще из другого пространства, чем что-то там вселенной, это будет что-то рядом, но требуется дополнительный ресурс с вашей стороны. Потому что, не забывайте, да, что типа 30% это идет от лекции, да, материал учителя, а 70% успеха, да, это идет от вас. То есть, в любом деле, вы должны делать больше контрабюшен, чем в это дело. Поэтому, не обходите столько лекций, это очень мало, на самом деле. Ну, возможно, при практических занятиях, при домашних занятиях, мы будем с какими-то моментами сталкиваться и изучать, да, вот эти моменты, которые не проходили на лекции, да? Да, ну, там не будет такого супер, вообще, типа, первослышу, такого точно не будет, но в целом, чуть-чуть какие-то моменты, нюансы, да, это посчитано так, что вы должны еще сами изучать в целом стадии, да? Вот. Ага, еще какие-то вопросы дальше. Пойдем? Да, в итоге функцию вы не объяснили. А, ну, альфа. Альфа, да, это, в самом деле, просто это из формулы, да, лекции она стоит. Под пунктом цеки. Под пунктом ц, кажется. Пятый слайд, а шестой слайд, там букву ц стоит. Это вот про нее есть. А, все, я вижу, да. Да, да, да. Ну, тогда пойдем дальше, у нас тебе еще нужно градительный бутик посмотреть. Смотрите, здесь мы побыстрее пойдем. Здесь то же самое, что и там, просто импортируем библиотеки и также, смотрите, я здесь создаю градительный бутик, из формы сградча, почти. Используется регрессор до 3. Извините, а почему у вас Минскуа-Эйтерер зачеркнут? Это как так? Да, это, да, это, просто депрекация пакетов происходит. То есть, видите, функция Минскуа-Эйтерер, большинство игр импортируются использованием пайланса. Просто у меня стоит пакет пайланс, как такой эктеншн для ВСКОДа, для Питона. И там, видно, прописано, что вот эта функция, она, видимо, скоро депрекетнется, и поэтому по-другому депрекется. А, понял. Типа, кажется, он это имеет, посмотрите. Кажется, он так называется будет потом. Да, видите, на мессенде рукается. Он называется ТВМС-ЕПРОСТ. Это, ну, это пак, пейланс, механика. Это, короче, при импорте библиотеки самые, да? Да, да, да, это, в смысле, это, если я просто обдабнейтну Экстендерн, кажется, такой не будет, или откачу назад, ошибка еще есть. Вот, смотрите, вот класс, сградетный, бустинг, регрессор, то есть задача регрессии не решается. Вот, она не сильно отличается, но все же, да, внимание не стоит прийти, почему регрессор? Потому что задача, ну, регрессионная, там, фэч Калифорния, хаусинг, это квартиры в Калифорнии. Все то же самое, интуизирую данные, это как от ремуты, да, входящие, входящие для статей индицидации модели, объекта модели. Далее, фид, все то же самое, толеранс, с хй, итерации и так подобное. Вот, смотрите, вот, бустинг немного поинтереснее. А да, бустинг как бы, он такой, честно, на самом деле, не прям на себе нашимел, но бустинг действительно поинтереснее будет. Смотрите, мы создаем в начале какой-то f, наши начальные предикты. NP0, значит, просто создать ноль его массив из NMP array, размерность Y. То есть, у нас 100 примеров, значит, в ноль NMP array. Это наши начальные предикты. Предположим, что это просто то, что мы в начале нашей модели, скажем, выдают. Начальная стадия наша. Далее, мы создаем ошибку, привезем с ней, это наш, ну, infinity, да, большая ошибка, бесконечная большая ошибка. Просто как начальная цилизация. А здесь возвращается итерация, да, сам луп итерации, обучение. Говорим, для каждой итерации в количестве итерации, то есть, может быть, дело следующее, да. То есть, считая резидуалы, да, резидуалы, помните, получается, это резидуал, это наша ошибка. Ну, называя остатки, ну, я как бы называю еще и ошибками. То есть, то, что мы предиктнули, вернее, то, что мы предиктнули, да, от того, что было по факту. В данном случае, F это наш начальный, помните, до нули, это типа мы предиктим, как-то наш предикт. Резидуал, это просто разница. Обычно разница. Это типа не MSA, это просто разница. Далее, говорим, создаем 3 от decision3.regressor, да, от SQLer, с такой-то глубиной максимальной, которая передает параметры. И фитим. Фитим X, смотрите внимательно, не к Y, да, а к резидуалам. Поняли, что произошло здесь? В общем, с на... Ну, ну, это, это, это, это, это, это, в общем, с на... Ну, на ошибках только. Да, в общем, видите, что мы делали? Мы теперь не на целевую примерно, да, финальную, а мы берем и фитимся, как target, уже разницу ошибки, которая была от Y, ну, в данном случае, окей, так как F это 0 начально, да, это, в общем, равно Y, в данном случае, но это только первая итерация, да, вот, послушайте, ну, вот, вот, вот, все соль, мы зафитили, дерево зафитили, идем дальше, говорю, update, update — это наши уже как бы предикты, ну, слегка странно, но просто берите показы вверх. Update равно 3 predict, то есть, здесь уже предиктим наши веса, наши результаты, первые модели, вот, мы первые модели обучили, говорю, а теперь давай предикт, делай предикт. Делаем, теперь, наши, вот, наши, наши оригинальные предикты, да, вот F, которые мы положили, большое F, это будет равно предыдущим, то есть, нуля, плюс learning rate помножено на наш update. То есть, тем самым мы к нулю, к нашим оригинальным предиктам, добавили learning rate умножено на наш, ну, выход модели, первые модели. И вот наш F теперь. То есть, ну, здесь немного видите, а вот модели хитрее, в сравнении с предыдущим, это этапы F. Как бы такой, чуть хитрый update модели происходит, аутбукерные. Все, здесь добавляем модели в append, append list, добавили, все, теперь считаем, аккуратно ошибку. Вот, считаем, аккуратно ошибку, уже видите, от Y и уже от F. F это наш финальный, наш предикт. Посчитали, все тоже с топливной кратерии, если больше не меньше, там, иктерацию закрываем, если больше меньше, там, там, разница от top 12, и видим, что все MSI равно MSI. И все. По сути, если видеть, градитный бустинг, ванильный, классический и оригинальный, имеется ввиду. Диски, вопросы есть? Вопросов нет? Все понятно, вроде бы. ОК. Ну, на самом деле, да, на самом деле, формула, у него куда сложнее, у этого бустинга, он выглядит куда страшнее, на самом деле, ничего не приятнее. в самом деле ничего, не в алгоритме, всё покуда проще. Поэтому я тебе это рекомендую, пожалуйста, изучите, что здесь написано. Это очень важно для понимания. Я понимаю, что вы никогда не будете с нами прописывать эту функцию в жизни, этот класс гранит бусинга, но в целом вы должны понимать, что происходит за вашими двумя линиями кода под капотом. Принцип работы, то есть. Конечно, потому что, смотрите, вы не сможете что-то улучшить, да? Допустим, вы пришли в какую-то компанию работать, да? Вам скажут, вот лучшая модель. Она там уже работает на пиковых нагрузках, она уже колбасит всю базу, и она уже хорошо работает, она деньги приносит на компанию. Но вы не сможете что-то в чём-то улучшить, если вы не понимаете, как это работает. Понимаете? Поэтому, я бы даже всё знал, как это работает. В целом всё, предиктулю. Да, да, да. У меня теперь по предикту мини-вопрос. Разве у нас F конечное не хранит, а вот все модели? Ну, при инференции мы всё равно заново предиктуем по всем моделям, да? Да, да, да. Не берём конечную просто? Нет, нет. Потому что, смотрите, это хороший вопрос в самом деле. Мы не храним это F? Потому что это зависит от примера же. Какой пример? На вход вышел такой же будет. Поэтому мы не храним. Мы только храним модели и киевшли. Ой, что я? Да, это же виклёрын. Да, да. И Лёлик, ну... И ледник рейд, да? Ну, а... Вот, по сути, только это. Здесь все просто, в самом деле. Алгоритм чуть-чуть хитренький вот здесь, но он очень сильный. Если вы еще почитаете математику за ним, в смысле, почему это так работает? Видите, мы сейчас просто по факту пришли, делаем вот так, отмениваем так, умножаем это и бери это. Как бы это, в самом деле, видите, это уже... Мы именовали вот это все грамотное математическое обоснование и пришли сразу к концу. Как бы это окей, это работает, так можно это делать, но, в целом, если вы хотите, на самом деле, понять, почему это так, в смысле, кто это увидел, почему догадался это так сделать, и почему это очень работает, то, конечно, читайте, идите там, всереду в литературу, где, собственно, вообще объясняется, почему он называется градиент градиентно-бузненький. Но я так спойлер сделаю. Градиентно-бузненький значит то, что градиент, по сути, этой функции, любой дифференцируемой функции, которая используется, как loss функция в этом алгоритме, является этот градиент и равен, по сути, разницей этих значений. То есть residuals – это есть градиент. Просто здесь это немного явно не видно, но, на самом деле, это так. То есть, поэтому называется градиентно-бустинг. Вот. Поэтому, если вы хотите, то вы можете, если вы хотите, это просто грибов, то вы можете, если вы хотите, это просто градиентно-бузненький. Поэтому, по сути, это очень хорошо. Но, если вы хотите, то вы можете, и видим, что наши предикты бьются с тем, что по факту происходит. Но у нас же большая ошибка. Еще раз. А, ну хотя, сейчас, я думаю, это маппи. Не, ничего, ничего. Да, это MSE. Ну, то есть, она такая. Я на мая посмотрел. А, нет. Угу. Давайте быстрее классификатора посмотрим. Тоже самое, только классификация. В главном деле, здесь все то же самое, что и в предыдущем образе. Только здесь момент такой, смотрите, что... как его? Сейчас, сейчас скажу вам. А, ну здесь споется еще сикбойда. Как бы, как этот... Ну, сейчас, где это было? А, ну вот, вот, вот. То есть, здесь просто... Удейте, это уже наша так его... Здесь стоит наш лог лоз. Вот, всеми любимые. Видите, написано здесь лог лоз. То есть, здесь, в самом деле, просто используется лог лоз дополнительно, да? И еще здесь дополнительно используется как вариант... Так, нет, это все то же самое. Ну да, только вот это используется. Смотрите, перед тем, как... Ну, MSC считается через... Не MSC, да? Лоз, наверное, считается не через MSC, а через лог лоз. И все. И все, все то же самое. Вот, а предикт, софпредикт. Ну, и здесь дав предикт, и здесь используется вещь предикт проба. То есть, это, по сути, просто ну, предсказать, чтобы сделать так, чтобы функция как вероятно, была, как раз, выше 0,5. Ну, и порока, отрезание. И все. А так все то же самое. Ну, видим там метрики. А потом поговорим про этот ROK-AUK. Эту метрику. Это очень полезная метрика. Поэтому пока можете прочитать, что это такое, как такое ревью. Что такое REC-repredig-characteristic curve. Вот, вообще, как ее не депрессирит, понимать. Вот, есть витчи, и в целом все. Так. В целом, ну, пока как бы так. Я думаю, мы прямо уже здесь вовремя. Давайте найдем вопросов. Вот. К вопросу давайте. Или вообще, по теме, по... Не по теме тоже. В общем-то, постараемся объяснить что-нибудь. Ну, я вот, что хотел сказать. По сути, мини-фидбэк. Точнее, даже не фидбэк. Вот, чуть объяснение тем, у кого не было опто. Я на самом деле, ну, давно уже в ДСе. И мне очень нравится то, что мы делаем модели в втором скрэтч. Я бы хотел, чтобы нейронки тоже мы так изучали. И вот ребятам мысль хотел вынести, что вот то, что реборис говорит, типа, в какой-то момент упрётесь. Ну, вот, типа, я упёрся в мид, и все, да, типа, выше я не могу прыгнуть. И вот как раз я столкнулся с той ситуацией, что, то, что я нормально не учил базу, вот сейчас живу и учу базу, хотя, как бы, уже пару лет работаю в ДСе. Все равно эти вещи, вот я пришел, типа, заново переизучать и суперполезно нахожу для себя. Потому что, как раз, что-то улучшить, это уже сильно сложно становится. Или особенно с... То, что ну, типа, все говорят про нейронки, чтобы их реально понимать, ну, нормально и не просто фидпредикт и там пару слоёв добавить, там, готовые с кероса или с торча... ну, типа, с библиотек, типа, нужно прям понимать, что за этим стоит, какая математика. Вот. Я, например, с этим столкнулся, и поэтому я тут. Да, на самом деле спасибо, да, Дима, спасибо за откровение, прям очень... Да, на самом деле, я тоже добавлю. В целом, это моя работа, поэтому я здесь, помогать вам. Смотрите. Полностью присоединяюсь с кладом. Смотрите. Вы на самом деле не сможете далеко пойти, реально, по карьерной, не только по карьерной, имеется в виду по знаниям, да, в таймэре, если вы на самом деле не будете понимать основы. Понимаете, вы не сможете там реально понять то же самое GPT, как работает, GPT-2, там, всякие ламы, неламы, трансформеры, там, берты, ниберты, всякие трансформеры, визуал-трансформеры, все точнее все эти, как работают, по-настоящему. Да, не про заниматься самобадом, да, там, я там, МЛ-щик, не портнул библиотеку, там, в ВИПДТ посмотрел, Этер, все, типа, выбрали, но я там, МЛ знаю, нет. Я имею в виду понимание, на самом деле, как работают слои как, что такое градиенты, что такое веса в модели, как это что такое backpropagation, что такое функция активации, как они все между собой разнятся, отличаются. Как бы, если вы хотели на этом уровне быть, то я говорю, основа этой математики, основа этих формул, помните, да, на первых предыдущих лет, я помню, что было чуть-чуть, по чувствам, показалось, что было какое-то негодование в адрес формул. Я понимаю, что да, формул это немного про, может быть, не всегда про индустрию, но, поверьте, МЛ, сама по себе, эта дисциплина новая, она стоит на пересечении индустрии, как применение практики, где зарабатывают в IT деньги, но также и пересечение на чуть-чуть науки. То есть МЛ это, на самом деле, про науку с математикой, которая работает, чтобы зарабатывать деньги для бизнеса. Поэтому, если вы, это нормально, то что МЛщик, да, дата саинтиста, МЛщика, дата инженер, он зачастую в своей карьере будет встречаться с математикой и наукой вообще, это нормально, потому что именно и наука делает деньги, на самом деле, в бизнесе. Вот. Вы говорите про математику, а как насчет статистики? Да, это тоже, когда я говорю на математику, это все это имеет в виду. То есть статистика тоже нужна по-любому. То есть там вероятность конечно, конечно, конечно, все это. Вообще, да, как бы я, когда собеседую людей, там на позиции, там, в принципе, всегда оцениваю, да, вот, ну, по-моему, питона, да, питона, на самом деле, я не всегда считаю, что самопрограммирование очень важно, потому что программирует, в принципе, уже можно через GPT модели, очень часто спасываю акцент на математику просто. Что такое производное, допустим, даже самое. Потому что, когда человек обладает инвестициями, я считаю, он, на самом деле, может найти какие-то нетривиальные решения к задачам. Вот. А ML это про это, про математику, про статистику и чуть-чуть кода. Вот. Поэтому, да, путь здесь нелегкий, но он стоит то, на самом деле. Вот. Поэтому, да, прислушайтесь, пожалуйста, советом, на самом деле, чтобы, да, выиграть себе же время в будущем. Пытайтесь прям досконально понимать основ. Не пытайтесь перешагивать, прыгать. Потому что придете в какой-то момент, поймете, что нужно было по-другому делать. Это гарантия 100%. Вот. Ну, а если какая-нибудь личная от вас рекомендация по литературе именно углублена в изучении математики? То есть база, в принципе, есть, ну, наверное, у многих здешних, а вот чтобы, может быть, даже с базы начиная, какая-то определенная библиотека именно в плане математики той же самой, именно в плане мэллинга. Ага. Ну, на самом деле, я же указал, помните? В конце каждой лекции я указываю вот это, да, свой этот. Сейчас. Да. Видно, да, сейчас? А, я думал, это просто какой-то определенной теме вот, вот то, что вы советуете. А, не-не-не. Это обобщенное, да? Да, это, на самом деле, мне кажется, не соврать, может быть. Ну, это очень оптимистичные книги в плане, мне кажется, жизни не хватит их пройти. Вот. То есть это, я не говорю, что это пособие, типа, как читаете, как с первой страницы до конца, это не такого формата пособия, вы не сможете их прочитать так. Это пособие, типа, я не знаю какую-то тему, пойду эту тему гуглить там, читать там. А вот так вот с первой страницы по трехсоттысячную, пятьсот третью страницу до конца, эти пособия, мне кажется, мало кто читает в мире. Это очень сложно. Вот. Не знаю, ну, как бы... Сейчас, я даже покажу вам, кажется. То есть я тоже учусь, я тоже читаю, не то, что я написал, это то, что я сам читаю. Вот. Сейчас покажу вам. На примере, что я имею в виду. Ну, допустим, вы видите, да, сейчас книгу? Бишеб. Да. Да, слегка. Ну, вот эта вот книга, допустим, она... Я ее всем рекомендую. Вообще сумасшедшая книга, в плане понимания. Да. Ну, вот здесь просто темы, да? Допустим, я хочу понять, да, что такое, там, не знаю. Не знаю, там, типа... Ну, вот, первая сети, да? Все, я, типа, бегу без 25 страниц, да? Типа... Вернее, так, давайте так напишем. Да. Ну, и начинаем читать, да? Читаю примеры, вот это все изучение, да, то есть это... Это слегка мутр, кажется, но это лучшего пока ничего не придумали. То есть если была какая-то панацея на то, чтобы это все можно было быстрее усваивать, их 3, там, их 5 по скорости, то есть это все, да, их 3, там, их 5 по скорости, я бы сам, наверное, использовал подход. Но лучше этого пока ничего не понимают. Я не хочу сейчас вас допугать, то есть я пытаюсь там, типа, показаться вот таким, вот такой, вот задирать порог, в смысле, да, всего, сложности. Это не так. Я сам, в смысле, изучаю все это каждый день. То есть это не так, что, типа, я там добрался к такой-то пике, да, в карьере своей. Ну, опять-таки, для кого-то пик, для кого-то мой пик – это разогрев, да. Соответственно, я хочу сказать, что я тоже все самое до сих пор это изучаю. То есть, понимаете, глубина, в которую можно погружаться этой темой, смежной темой этих, этих, этих топиков, они, по сути, ну, почти бесконечно. Там всегда что-то можно еще найти. Поэтому, вот, читайте, начинать чего-то начинать. То есть, кажется, чего-то начинать, я думаю. Поэтому не бойтесь к тому. Без этого никак, в смысле. Если вы хотите, как бы, какую-то разницу иметь в этом деле, на работе, да, текущей своей, на будущей работе, иметь какую-то разницу, да, делать разницу, реально, в понимании процессов изменения бизнеса, там, допустим, ну, прийти в организацию, да, реально перестроить процесс, построить какую-то модель, которая, реально, будет экономить, там, в тысячу раз деньги и увеличивать, увеличивать продажи, да, в десять раз. И при этом, делая это там, реально, математикой, инструментами машинного обучения, это реально даст вам огромную валью. Как сотрудник, как кадр, и вообще, ну, как, как, как, как профессионала дело. Но это невозможно просто, так просто взять фидпредикты, как вот Дима сказал так, что. Это реально такой прикол в камините, да, это, что фидпредикты это все, что там люди умеют сейчас. Не будьте фидпредиктами, да? Это не было, это не было. Вот. Так, урок будет, да, с какими? На самом деле, все, мы закончили, то есть, два с три, с тридцать, уже, то есть, мы закончили. Какие-то вопросы, может, последние? Вопросов у меня нет. Дальше, в целом, мы пойдем, знаете, я еще хотел так сказать, дальше мы пойдем в этот, уже начнем кластеризацию, начнем уже кластеринг, понижение, понижение размерности темы, как силуэты, всякие скоры, потом методы плеча, там, и оценка модели, модулы влажь, начнем. У меня вопрос. Ускоряться будут. Ускоряться будут. У меня вопрос. Ускоряться будут. Да? По поводу, ну, быстро такой организационный вопрос, по домашке, когда у нас будет домашка и какой будет лайн? Просто, чтоб понимать календарь. Да-да-да, домашка вообще, как я понял, там рекламин такой, что одна домашка на одну неделю. То есть, он говорит, видите, мы прошли сейчас, вот, вы до этого проходили дата инжинирик, да? Угу, угу. Там был, там, да, Сыш. Они уже закрыли, да. Да, вот вы вот недавно закрыли, только домашка, говорится, вот вчера, дотягся сегодня. Угу. Соответственно, вот вы сейчас закрыли, да, вот ML. Соответственно, я, наверное, сегодня, завтра вам вышлю, не в смысле там TA, вышлю вам домашку по вот этому самым декциям. И вы, скорее всего, будете тоже неделю это делать. Ну, кажется, вы неделю доделали ее. Да, кажется. Ну вот. То есть, это примерно так и будет. Ну, не парьтесь, в самом деле там, я думаю, несложно будет, в самом деле. Угу. Все понятно. Спасибо. А получается, следующий блок, эээ... Под блок, да. Это Model Evolution, да. Тоже будете вывести, да? Да, да, да. Я буду вести его. А следующий, после следующего блока, это будет Жене. Deploy, да? Да, Deploy, Product Development. Потом, да, потом вы после Model Evolution меня умеете, кажется, где-то в январе. Севи, компьютервижение? Да, я больше по севи части. Ага, ага. Вот я буду вам все показывать. Все понятно, спасибо. Вот. Да, еще хотела бы вопрос спросить, кто остался, у нас, в принципе, 31 человек. Какую-то обратную связь, может быть, где-то надо добавить, где-то надо остановить, где надо убрать, и вообще, как идет flow? Что скажем, можем улучшить или можем, наоборот, убрать, да, чтобы время терять? Чтобы корректироваться тоже по вас. Не знаете, я вот хотел бы имена, что вот каждый вот этот не прям все детально, но вот хотелось бы немного попотропнее, то есть, например, вот в дальнейшем вы выписываете какую-то определенную функцию или метод, и в дальнейшем этот метод, как будет использоваться, например, в каких отраслях будет использоваться, например. Вот это хотел бы узнать. Мне кажется, тут проблема не в том, что не хотят, не могут, да, просто времени бы нам не хватает по часам. Или более ремотонозу, чтобы воспрятие было легким. Ага, так это получается, так это был, сейчас это был Расул, да? Расукарц? Да, да, да. Получается, если я понял вас корректно, вы сейчас говорите про касательно того, что... Я понимаю, что у меня метод функции там, хотелось бы немного попотропней и немного глубиной. А именно определение, да, математическое подробнее? Или в смысле применение? Да, именно математические функции.